{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36836ad4-35bb-4d04-9118-170b1a3f37d1",
   "metadata": {},
   "source": [
    "### Task 0 Before your go\n",
    "\n",
    "> 1. Rename Assignment-02-###.ipynb where ### is your student ID.\n",
    "> 2. The deadline of Assignment-02 is 23:59pm, 04-21-2024\n",
    "> 3. In this assignment, you will use word embeddings to explore our Wikipedia dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caec95d8-a23b-4116-8d00-197a049cfd4e",
   "metadata": {},
   "source": [
    "### Task 1 Train word embeddings using SGNS \n",
    "> Use our enwiki-train.json as training data. You can use the [Gensim tool](https://radimrehurek.com/gensim/models/word2vec.html). But it is recommended to implement by yourself. You should explain how hyper-parameters such as dimensionality of embeddings, window size, the parameter of negative sampling strategy, and initial learning rate have been chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "baf0db53-3382-4534-b93e-e2a03796d525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import some necessary libraries\n",
    "from typing import List, Dict, Callable\n",
    "from collections import defaultdict\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim import utils\n",
    "import json\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b61425c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the train and test data from the json file\n",
    "\n",
    "# NOTE: The function is inherited from my solution of assignment 1\n",
    "def load_json(file_path: str) -> List:\n",
    "    \"\"\"\n",
    "    Fetch the data from `.json` file and concat them into a list.\n",
    "\n",
    "    Input:\n",
    "    - file_path: The relative file path of the `.json` file\n",
    "\n",
    "    Returns:\n",
    "    - join_data_list: A list containing the data, with the format of [{'title':<>, 'label':<>, 'text':<>}, {}, ...]\n",
    "    \"\"\"\n",
    "    join_data_list = []\n",
    "    with open(file_path, \"r\") as json_file:\n",
    "        for line in json_file:\n",
    "            line = line.strip()\n",
    "            # guaranteen the line is not empty\n",
    "            if line: \n",
    "                join_data_list.append(json.loads(line))\n",
    "    return join_data_list\n",
    "\n",
    "train_file_path, test_file_path = \"enwiki-train.json\", \"enwiki-test.json\"\n",
    "train_data_list, test_data_list = map(load_json, [train_file_path, test_file_path])\n",
    "\n",
    "class Corpus:\n",
    "    def __iter__(self):\n",
    "        for line in train_data_list:\n",
    "            yield utils.simple_preprocess(line[\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "876121ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyWord2Vec:\n",
    "    def __init__(self, text: List[dict], dimensionality: int=100, window_size: int=5, negative_samples: int=5, lr: float=0.001) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - text: The training data\n",
    "        - dimensionality: The dimension of the word embeddings\n",
    "        - window_size: The size of the context window\n",
    "        - negative_samples: The number of negative samples\n",
    "        - lr: Learning rate of the algorithm\n",
    "        \"\"\"\n",
    "        self.dim = dimensionality\n",
    "        self.window = window_size\n",
    "        self.neg = negative_samples\n",
    "        self.lr = lr\n",
    "        self.__vocab = set()\n",
    "        self.__word_frq = defaultdict(int)\n",
    "        self.__word2idx = {}\n",
    "        self.__idx2word = {}\n",
    "        self.__embedding = None\n",
    "        self.__context_words = []\n",
    "        self.__context_targets = []\n",
    "        self.__build(text)\n",
    "        \n",
    "\n",
    "    def __preprocess(self, text: List[str]) -> None:\n",
    "        \"\"\"\n",
    "        Calculate the vocabulary and the frequency of each word in the training data, while maintaining the (idx, word) map.\n",
    "        \"\"\"\n",
    "        for sample in text:\n",
    "            words = sample[\"text\"].split()\n",
    "            for word in words:\n",
    "                self.__vocab.add(word)\n",
    "                self.__word_frq[word] += 1\n",
    "        for idx, word in enumerate(self.__vocab):\n",
    "            self.__word2idx[word] = idx\n",
    "            self.__idx2word[idx] = word\n",
    "\n",
    "    def __generate_training_data(self, text: List[str]) -> None:\n",
    "        \"\"\"\n",
    "        Generate training data from each window and save them in `self.__context_words` and `self.__context_targets`\n",
    "        \"\"\"\n",
    "        for sample in text:\n",
    "            words = sample[\"text\"].split()\n",
    "            for i, curr_word in enumerate(words):\n",
    "                # the \"window\" around the current world\n",
    "                for j in range(max(0, i - self.window), min(i + self.window + 1, len(words))):\n",
    "                    if i != j:\n",
    "                        self.__context_words.append(self.__word2idx[curr_word])\n",
    "                        self.__context_targets.append(self.__word2idx[words[j]])\n",
    "\n",
    "    def __initialize_embedding(self) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the embedding matrix with random values\n",
    "        \"\"\"\n",
    "        self.__embedding = np.random.uniform(-0.5 / self.dim, 0.5 / self.dim, size=(len(self.__vocab), self.dim))\n",
    "    \n",
    "    def __build(self, text: List[map]) -> None:\n",
    "        \"\"\"\n",
    "        Compute and store the relevant information of the training data in the class\n",
    "        \"\"\"\n",
    "        self.__preprocess(text)\n",
    "        self.__generate_training_data(text)\n",
    "        self.__initialize_embedding()\n",
    "\n",
    "    def train(self, epochs: int=5) -> None: \n",
    "        for epoch in range(epochs):\n",
    "            # learning rate decay\n",
    "            learning_rate = self.lr * (1 - epoch / epochs)\n",
    "\n",
    "            print(\"Training Epoch: %d\" % (epoch + 1))\n",
    "\n",
    "            for context_word, target_word in zip(self.__context_words, self.__context_targets):\n",
    "                context_vector = self.__embedding[context_word]\n",
    "                target_vector = self.__embedding[target_word]\n",
    "\n",
    "                # positive sample update\n",
    "                score = np.dot(target_vector, context_vector)\n",
    "                exp_score = math.exp(score)\n",
    "                grad_context = (exp_score / (1 + exp_score) - 1) * target_vector\n",
    "                grad_target = (exp_score / (1 + exp_score) - 1) * context_vector\n",
    "                self.__embedding[context_word] -= learning_rate * grad_context\n",
    "                self.__embedding[target_word] -= learning_rate * grad_target\n",
    "\n",
    "                # negative sample update\n",
    "                for _ in range(self.neg):\n",
    "                    negative_word = random.randint(0, len(self.__vocab) - 1)\n",
    "                    if negative_word != target_word:\n",
    "                        negative_vector = self.__embedding[negative_word]\n",
    "                        score = np.dot(negative_vector, context_vector)\n",
    "                        exp_score = math.exp(score)\n",
    "                        grad_context = exp_score / (1 + exp_score) * negative_vector\n",
    "                        grad_target = exp_score / (1 + exp_score) * context_vector\n",
    "                        self.__embedding[context_word] -= learning_rate * grad_context\n",
    "                        self.__embedding[target_word] -= learning_rate * grad_target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "286cfc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = Corpus()\n",
    "model = Word2Vec(\n",
    "    sentences=sentence, vector_size=100, alpha=0.025, window=5, min_count=5, sample=0.001, \n",
    "    seed=1, workers=3, min_alpha=0.0001, sg=1, negative=5, ns_exponent=0.75, epochs=5, \n",
    "    sorted_vocab=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeadc4b2-2c39-46f1-a9b9-28089393c24f",
   "metadata": {},
   "source": [
    "### Task 2 Find similar/dissimilar word pairs\n",
    "\n",
    "> Randomly generate 100, 1000, and 10000-word pairs from the vocabularies. For each set, print 5 closest word pairs and 5 furthest word pairs (you can use cosine-similarity to measure two words). Explain your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d81f7d38-0bf0-4e55-aedc-b0f8f24195f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For   100 random pairs from the vocabularies:\n",
      "The 5  closest word pairs:\n",
      "Word pairs: (      herodotus,       sorcerers) --> Similarity: 0.862411\n",
      "Word pairs: (        bruxing,        plumbers) --> Similarity: 0.843897\n",
      "Word pairs: (         shaded,           booby) --> Similarity: 0.818646\n",
      "Word pairs: (        deflect,      comforting) --> Similarity: 0.815063\n",
      "Word pairs: (           sown,     prohibitive) --> Similarity: 0.810534\n",
      "The 5 furthest word pairs:\n",
      "Word pairs: (  manifestation,        homeless) --> Similarity: 0.218703\n",
      "Word pairs: (          harry,           kuala) --> Similarity: 0.197687\n",
      "Word pairs: (     violations,         yevgeny) --> Similarity: 0.184321\n",
      "Word pairs: (        trapped,          arabia) --> Similarity: 0.147944\n",
      "Word pairs: (     separately,       communism) --> Similarity: 0.145576\n",
      "----------------------------------------------------------------------\n",
      "For  1000 random pairs from the vocabularies:\n",
      "The 5  closest word pairs:\n",
      "Word pairs: (        itching,         dryness) --> Similarity: 0.911543\n",
      "Word pairs: (        serrano,    englishwoman) --> Similarity: 0.901493\n",
      "Word pairs: (           chua,        lionized) --> Similarity: 0.894430\n",
      "Word pairs: (          glaze,           tripe) --> Similarity: 0.885291\n",
      "Word pairs: (           acme,       methodism) --> Similarity: 0.877236\n",
      "The 5 furthest word pairs:\n",
      "Word pairs: (        teenage,           pages) --> Similarity: 0.025713\n",
      "Word pairs: (         bought,        numbness) --> Similarity: 0.017405\n",
      "Word pairs: (         lucien,     regulations) --> Similarity: 0.012484\n",
      "Word pairs: (       category,          godwin) --> Similarity: 0.011847\n",
      "Word pairs: (        exceeds,            karl) --> Similarity: 0.001551\n",
      "----------------------------------------------------------------------\n",
      "For 10000 random pairs from the vocabularies:\n",
      "The 5  closest word pairs:\n",
      "Word pairs: (          chaat,          capers) --> Similarity: 0.954522\n",
      "Word pairs: (    autocorrect,           addon) --> Similarity: 0.941782\n",
      "Word pairs: (           scot,          leyden) --> Similarity: 0.922606\n",
      "Word pairs: (        crumbly,          yakhni) --> Similarity: 0.918996\n",
      "Word pairs: (      succulent,          chalky) --> Similarity: 0.915554\n",
      "The 5 furthest word pairs:\n",
      "Word pairs: (        manotoc,       cluttered) --> Similarity: -0.016643\n",
      "Word pairs: (     implicated,           smart) --> Similarity: -0.022397\n",
      "Word pairs: (           vary,         schmidt) --> Similarity: -0.032612\n",
      "Word pairs: (hospitalization,              și) --> Similarity: -0.041692\n",
      "Word pairs: (      longridge,      projectile) --> Similarity: -0.084916\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def generate_random_paris(samples: int):\n",
    "    \"\"\"\n",
    "    Generate random indices without replacement, then pairs the indices to get word pairs\n",
    "    \"\"\"\n",
    "    indices = random.sample(range(len(model.wv)), 2 * samples)\n",
    "    indices1, indices2 = indices[:samples], indices[samples:]\n",
    "    return [model.wv.index_to_key[i] for i in indices1], [model.wv.index_to_key[i] for i in indices2]\n",
    "\n",
    "def find_closest_furthest(num: int=5, words1: List[str]=None, words2: List[str]=None) -> None:\n",
    "    \"\"\"\n",
    "    Find the cloest/furthest word pairs using `model.wv.similarity`.\n",
    "\n",
    "    Here a heap queue is used to reduce time complexity to $O(n\\log k)$, where k denotes the `num`\n",
    "    \"\"\"\n",
    "    heap = []\n",
    "    for i in range(len(words1)):\n",
    "        # compute the similarity and push it into the heap\n",
    "        heapq.heappush(heap, (model.wv.similarity(words1[i], words2[i]), words1[i], words2[i]))\n",
    "    return heapq.nlargest(num, heap), heapq.nsmallest(num, heap)[::-1]\n",
    "\n",
    "def print_word_pairs(results: List[tuple], flag: str) -> None:\n",
    "    \"\"\"\n",
    "    Print the result in formatted string\n",
    "    \"\"\"\n",
    "    print(\"The 5 {:>8} word pairs:\".format(flag))\n",
    "    for result in results:\n",
    "       print(\"Word pairs: ({:>15}, {:>15}) --> Similarity: {:>8.6f}\".format(result[1], result[2], result[0]))\n",
    "\n",
    "\n",
    "random.seed(408)\n",
    "pairs = [100, 1000, 10000]\n",
    "for pair in pairs:\n",
    "    print(\"For {:>5} random pairs from the vocabularies:\".format(pair))\n",
    "    cloest, furthest = find_closest_furthest(5, *generate_random_paris(pair))\n",
    "    print_word_pairs(cloest, \"closest\")\n",
    "    print_word_pairs(furthest, \"furthest\")\n",
    "    print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af756bde-2960-4c56-b037-0bfd2cf7e47e",
   "metadata": {},
   "source": [
    "### Task 3 Present a document as an embedding\n",
    "\n",
    "> For each document, you have several choices to generate document embedding: 1. Use the average of embeddings of all words in each document; 2. Use the first paragraph’s words and take an average on these embeddings; 3. Use the doc2vec algorithm to present each document. Do the above for both training and testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "279547d8-e4e7-4bfa-9be9-6d535b86bbc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the training dataset:\n",
      "Title: Citizen_Kane\n",
      "Embedding: [-0.2960049  -0.01423333  0.14721517  0.12139755 -0.17462535 -0.31337783\n",
      "  0.18368073  0.34699985 -0.17591807 -0.19838323  0.02308599 -0.2998711\n",
      "  0.01666955  0.13167198 -0.06327387 -0.08708493  0.14249018  0.01380194\n",
      " -0.21497476 -0.36701983 -0.06805244  0.14445285  0.08747505  0.04904256\n",
      "  0.14309205  0.23049948 -0.23031184  0.10671046 -0.19837528 -0.1316407\n",
      " -0.1189249   0.16955267 -0.01272503 -0.04033708  0.02200293  0.13950647\n",
      " -0.17399263 -0.2843225   0.06627771 -0.166714    0.02661788 -0.25058183\n",
      " -0.40715316 -0.18351702  0.20407248 -0.10050888 -0.21705762  0.04414554\n",
      "  0.09181677  0.04567541  0.03726172 -0.05376059 -0.31678638 -0.10022476\n",
      "  0.01748321  0.04129237 -0.06482305  0.07935403  0.04977779  0.10498507\n",
      "  0.22192508  0.15182945 -0.00224699 -0.06766742  0.07389529  0.03351407\n",
      " -0.22441798  0.21850444 -0.24207619  0.22126253  0.15975542  0.24951118\n",
      "  0.06868735 -0.03211427  0.0378467   0.16712835  0.04484984  0.11626846\n",
      "  0.11699506 -0.06100503 -0.1731261  -0.24548528  0.0984984  -0.00590739\n",
      " -0.0057951   0.16834931  0.01010739  0.1258305  -0.06657883  0.21638276\n",
      "  0.26900733  0.11062074  0.11661544 -0.24936055  0.0902198   0.11163478\n",
      " -0.14589168 -0.23836777  0.04219155 -0.00266276]\n",
      "----------------------------------------------------------------------\n",
      "Title: It_(2017_film)\n",
      "Embedding: [-2.95731932e-01 -4.52664234e-02  1.36496022e-01  1.29603729e-01\n",
      " -1.98250562e-01 -2.66261190e-01  1.41174048e-01  3.75714391e-01\n",
      " -1.76323846e-01 -1.86429232e-01  4.57113981e-02 -3.02159876e-01\n",
      " -7.26874406e-03  1.83630139e-01 -7.40433633e-02 -7.99311027e-02\n",
      "  1.42906010e-01  2.68904008e-02 -2.11828738e-01 -3.92653644e-01\n",
      " -6.53587356e-02  1.21414766e-01  8.49858224e-02  6.06464967e-02\n",
      "  1.36692435e-01  2.26464912e-01 -2.06046879e-01  6.09398670e-02\n",
      " -1.84027746e-01 -1.61627948e-01 -1.22797862e-01  1.82273164e-01\n",
      "  6.70894049e-03 -4.58757393e-02 -2.41617411e-02  1.03129372e-01\n",
      " -1.93177521e-01 -3.03203851e-01  7.81063586e-02 -1.96969703e-01\n",
      "  4.62587401e-02 -2.73154587e-01 -3.75647426e-01 -2.01020166e-01\n",
      "  1.99768856e-01 -8.30263942e-02 -2.07066149e-01  3.84864248e-02\n",
      "  1.20787024e-01  7.32577890e-02  2.58721411e-02 -6.67333007e-02\n",
      " -3.25388342e-01 -1.08601280e-01  2.65422780e-02  5.89916036e-02\n",
      " -7.21774921e-02  8.68679062e-02  4.91261669e-02  1.02912545e-01\n",
      "  2.61581033e-01  1.58329293e-01 -2.02105206e-04 -4.93070595e-02\n",
      "  4.79649678e-02  3.82461883e-02 -2.27749467e-01  2.50603706e-01\n",
      " -2.56641507e-01  2.19278812e-01  1.48013383e-01  2.50205368e-01\n",
      "  6.22323118e-02 -3.37597318e-02  9.93119329e-02  1.64962128e-01\n",
      "  7.99283981e-02  1.10968687e-01  1.01180620e-01 -5.89583218e-02\n",
      " -1.71596870e-01 -2.13699982e-01  9.86425355e-02  3.02902013e-02\n",
      "  1.27726309e-02  1.48055956e-01  8.15419294e-03  1.03591412e-01\n",
      " -7.31618032e-02  2.31931776e-01  2.41102710e-01  1.27681807e-01\n",
      "  8.95378515e-02 -2.77112216e-01  5.53820170e-02  1.08518124e-01\n",
      " -1.48930132e-01 -2.45657980e-01  7.70181194e-02  7.68374233e-03]\n",
      "----------------------------------------------------------------------\n",
      "Title: Star_Wars_(film)\n",
      "Embedding: [-0.32138616 -0.00616291  0.14141694  0.13087772 -0.14377946 -0.2764655\n",
      "  0.14316598  0.378755   -0.17835613 -0.25010598  0.01981841 -0.3140662\n",
      "  0.0027611   0.13509645 -0.06177996 -0.10153932  0.16323839  0.01347988\n",
      " -0.21417528 -0.37517962 -0.02667199  0.15524356  0.10186664  0.03610171\n",
      "  0.13846935  0.23424888 -0.19601674  0.0658121  -0.20667459 -0.14955391\n",
      " -0.1354598   0.16931543 -0.02725095 -0.05313079  0.02192406  0.11629929\n",
      " -0.19553486 -0.28700578  0.06055827 -0.18137634  0.04947186 -0.23790686\n",
      " -0.4090495  -0.19474535  0.18796836 -0.07645879 -0.20273343  0.026587\n",
      "  0.1102316   0.05848782  0.06199409 -0.07874337 -0.29515103 -0.10481113\n",
      "  0.03150193  0.04016899 -0.04759806  0.0538801   0.03481751  0.10411701\n",
      "  0.22971609  0.15439059  0.01960197 -0.07485794  0.0143529   0.01792865\n",
      " -0.22106105  0.23354955 -0.24041946  0.21172936  0.1464255   0.2409055\n",
      "  0.0697533  -0.0378116   0.04966925  0.14876096  0.04052582  0.09805814\n",
      "  0.11144119 -0.06752006 -0.17354374 -0.24852638  0.12366874  0.01004842\n",
      "  0.02327093  0.16098885  0.00816601  0.10396638 -0.07422841  0.21091947\n",
      "  0.25851318  0.09000319  0.12368035 -0.24500456  0.07793628  0.09010138\n",
      " -0.16947652 -0.24288546  0.05742608 -0.022102  ]\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "For the  testing dataset:\n",
      "Title: Monty_Python's_Life_of_Brian\n",
      "Embedding: [-2.93183744e-01 -4.78273481e-02  1.77307934e-01  1.26199722e-01\n",
      " -1.90671101e-01 -2.78181106e-01  1.60060376e-01  3.83491009e-01\n",
      " -1.53484181e-01 -1.94951624e-01  2.54598260e-02 -2.91900396e-01\n",
      " -2.41979286e-02  1.85131028e-01 -1.04277059e-01 -7.56761432e-02\n",
      "  1.55854836e-01  4.59429957e-02 -2.45426521e-01 -4.09734935e-01\n",
      " -6.43344745e-02  1.07447743e-01  9.68378335e-02  6.08703755e-02\n",
      "  1.60953417e-01  2.24633664e-01 -2.17116460e-01  8.27599466e-02\n",
      " -2.16245249e-01 -1.24755941e-01 -1.42634124e-01  1.55524954e-01\n",
      "  1.44800246e-02 -5.11190444e-02  8.68990924e-03  1.01991609e-01\n",
      " -1.65401667e-01 -3.21886182e-01  8.03807378e-02 -2.03489378e-01\n",
      "  8.63647610e-02 -2.68672079e-01 -3.49577636e-01 -1.82579219e-01\n",
      "  2.14118093e-01 -1.03036910e-01 -1.96827412e-01  3.17288339e-02\n",
      "  1.23457700e-01  9.24777538e-02  1.41363358e-02 -8.23942050e-02\n",
      " -3.32757235e-01 -1.17338933e-01  2.25941446e-02  6.20288067e-02\n",
      " -8.36742148e-02  8.29970241e-02  6.44630343e-02  7.42747709e-02\n",
      "  2.41213813e-01  1.26106083e-01 -1.65543724e-02 -4.45752628e-02\n",
      "  7.86938816e-02 -1.19704011e-04 -2.26894453e-01  2.28597492e-01\n",
      " -2.51915187e-01  2.06455275e-01  1.40309229e-01  2.67007709e-01\n",
      "  6.24162033e-02 -2.92971097e-02  5.12234718e-02  1.48795605e-01\n",
      "  7.68955424e-02  1.01913035e-01  1.21841125e-01 -2.53674574e-02\n",
      " -1.93908349e-01 -2.24861369e-01  1.13308035e-01  3.13502774e-02\n",
      "  1.79860555e-02  1.39894411e-01  1.38263907e-02  1.03545271e-01\n",
      " -4.45781462e-02  2.47981474e-01  2.39173904e-01  1.14209034e-01\n",
      "  1.12659425e-01 -2.73660183e-01  6.87758252e-02  1.67473927e-01\n",
      " -1.60150409e-01 -2.67609388e-01  7.32047856e-02  9.40943789e-03]\n",
      "----------------------------------------------------------------------\n",
      "Title: Superman_(1978_film)\n",
      "Embedding: [-0.3060346  -0.0061886   0.12954813  0.13226682 -0.15473305 -0.28299162\n",
      "  0.15164268  0.37537137 -0.18525818 -0.22704604  0.02132085 -0.312403\n",
      " -0.0046503   0.14244169 -0.06437117 -0.09963937  0.13540219  0.01510372\n",
      " -0.21754074 -0.3726205  -0.0438495   0.1329619   0.07500134  0.04501323\n",
      "  0.11211383  0.2211469  -0.20659736  0.07569841 -0.19326621 -0.15054148\n",
      " -0.13637367  0.16614538 -0.0215284  -0.03096098  0.00849946  0.11350957\n",
      " -0.19525936 -0.277429    0.06274489 -0.18366313  0.04441428 -0.25039732\n",
      " -0.40261996 -0.18529952  0.19099721 -0.08099777 -0.21346433  0.04009855\n",
      "  0.11840949  0.07478184  0.05387662 -0.05908035 -0.30484328 -0.10465696\n",
      "  0.01851847  0.05328193 -0.0509699   0.06623745  0.04300617  0.09853172\n",
      "  0.23134933  0.16021773  0.00189716 -0.06628191  0.03480472  0.03379355\n",
      " -0.2327046   0.24287285 -0.23821929  0.20924762  0.16507252  0.23565172\n",
      "  0.05686936 -0.03822218  0.05808493  0.1618437   0.049702    0.09444372\n",
      "  0.09770658 -0.0783456  -0.17285837 -0.23528764  0.11096493  0.02000082\n",
      "  0.02490525  0.14875135  0.01332339  0.10863789 -0.06416831  0.20864595\n",
      "  0.25970402  0.10893734  0.11148334 -0.26936     0.06346657  0.1021897\n",
      " -0.1552536  -0.23750366  0.05023949 -0.01934007]\n",
      "----------------------------------------------------------------------\n",
      "Title: Boys_Don't_Cry_(1999_film)\n",
      "Embedding: [-0.2598688  -0.04593632  0.16274777  0.1358269  -0.18941663 -0.29721892\n",
      "  0.179752    0.3846637  -0.17281869 -0.2050874   0.01839592 -0.29012266\n",
      " -0.01290701  0.16983846 -0.09093293 -0.06605322  0.16089694  0.0401838\n",
      " -0.22591259 -0.36906102 -0.0801823   0.13179456  0.08243724  0.02269925\n",
      "  0.14960591  0.22919348 -0.19896328  0.09169303 -0.20803706 -0.14038041\n",
      " -0.13188452  0.18161125  0.02448278 -0.05940153  0.00530003  0.09685286\n",
      " -0.1920178  -0.30879536  0.09458397 -0.1891418   0.05612837 -0.2730657\n",
      " -0.37046278 -0.19509803  0.22115359 -0.09677067 -0.20386583  0.05865807\n",
      "  0.11068017  0.11576355  0.03082449 -0.0569521  -0.32086357 -0.10015807\n",
      "  0.0273855   0.04060133 -0.10469946  0.0655599   0.06174644  0.07223582\n",
      "  0.25467366  0.14778489 -0.01895648 -0.04797861  0.04534839  0.02041843\n",
      " -0.2437463   0.2241398  -0.2608458   0.20564702  0.13579366  0.2716726\n",
      "  0.0812144  -0.03612581  0.0607876   0.1429095   0.08351713  0.1020845\n",
      "  0.12230504 -0.03528596 -0.2044895  -0.23651572  0.09095936  0.02299538\n",
      " -0.02726199  0.11864185 -0.02410209  0.08494978 -0.05100348  0.25939608\n",
      "  0.2213815   0.12488212  0.10277191 -0.24419779  0.09906349  0.1358687\n",
      " -0.1662696  -0.24701834  0.08254659 -0.00834009]\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "####################################################################################\n",
    "###        1. Use the average of embeddings of all words in each document        ###\n",
    "####################################################################################\n",
    "def print_document_embeddings(embeddings: Dict, display: int=3) -> None:\n",
    "    \"\"\"\n",
    "    Print the first `display` embeddings, default value is 3\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    for embedding, title in embeddings:\n",
    "        if count >= display:\n",
    "            break\n",
    "        count += 1\n",
    "        print(\"Title: {}\\nEmbedding: {}\".format(title, embedding))\n",
    "        print(\"-\" * 70)\n",
    "\n",
    "def print_training_testing(method: Callable):\n",
    "    \"\"\"\n",
    "    Print the embeddings for training and testing dataset\n",
    "    \"\"\"\n",
    "    print(\"For the training dataset:\")\n",
    "    print_document_embeddings(method())\n",
    "    print(\"-\" * 70)\n",
    "    print(\"For the  testing dataset:\")\n",
    "    print_document_embeddings(method(test_data_list))\n",
    "\n",
    "def average_all_words(data_list: List[Dict]=train_data_list, mode: str=\"title\") -> List[tuple]:\n",
    "    doc_embeddings = []\n",
    "    if mode not in [\"title\", \"label\"]:\n",
    "        raise ValueError(\"Please input a valid mode: ['title', 'label']\")\n",
    "    for line in data_list:\n",
    "        line_text = utils.simple_preprocess(line[\"text\"]) # preprocess the text as in class `Corpus`\n",
    "        # doc title/label and doc embedding computed by averaging all words embeddings\n",
    "        doc_info = line[mode]\n",
    "        valid_word_embeddings = [model.wv[word] for word in line_text if word in model.wv]\n",
    "        try:\n",
    "            doc_embedding = sum(valid_word_embeddings) / len(valid_word_embeddings)\n",
    "        except:\n",
    "            doc_embedding = np.zeros(model.vector_size)\n",
    "        # store the information in a dictionary\n",
    "        doc_embeddings.append((doc_embedding, doc_info))\n",
    "\n",
    "    return doc_embeddings\n",
    "\n",
    "print_training_testing(average_all_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1ae2cadc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the training dataset:\n",
      "Title: Citizen_Kane\n",
      "Embedding: [-0.27791345  0.03093471  0.25582522  0.10712704 -0.14603665 -0.39557108\n",
      "  0.21222307  0.33513114 -0.27648464 -0.21422291 -0.04528995 -0.29881924\n",
      "  0.027381    0.04872361 -0.08348218 -0.14888479  0.14539245 -0.05736789\n",
      " -0.12656057 -0.33338633 -0.1022215   0.22359993  0.07359979 -0.02213276\n",
      "  0.12973529  0.27343455 -0.28661847  0.0712615  -0.20643789 -0.12265062\n",
      " -0.09869216  0.1762715  -0.04076541 -0.00666856  0.05650603  0.15743554\n",
      " -0.21651553 -0.27838734  0.02791196 -0.12518217  0.00946915 -0.2921295\n",
      " -0.46128526 -0.2091034   0.21861024 -0.04594442 -0.17977004  0.07432243\n",
      "  0.00299822  0.04505717  0.09264806  0.01829637 -0.264414   -0.07577453\n",
      "  0.02178697 -0.02288042 -0.06831323  0.09111134  0.03942756  0.14259186\n",
      "  0.28405476  0.20258082  0.0321538  -0.17838289  0.09601185 -0.05103689\n",
      " -0.22543323  0.2711815  -0.2431821   0.28218347  0.14013624  0.22906084\n",
      "  0.08563164 -0.0424281  -0.00476583  0.08102161  0.00983071  0.07451709\n",
      "  0.13893789 -0.03200591 -0.15851076 -0.3239141   0.08599011 -0.07598387\n",
      "  0.00377604  0.23039366 -0.04114128  0.09849866  0.00115701  0.22975639\n",
      "  0.26572686  0.01821994  0.01312182 -0.26988846  0.07877075  0.04801346\n",
      " -0.14774263 -0.21177833  0.01725193 -0.0507729 ]\n",
      "----------------------------------------------------------------------\n",
      "Title: It_(2017_film)\n",
      "Embedding: [-0.30382752  0.00949348  0.18402222  0.12583822 -0.15640986 -0.30335078\n",
      "  0.22269727  0.4210854  -0.1985731  -0.24157968  0.06638671 -0.27020043\n",
      " -0.05323279  0.1172054  -0.11138473 -0.00521793  0.16911678  0.03165939\n",
      " -0.20535189 -0.32298344 -0.03239781  0.14732866  0.128194   -0.00402535\n",
      "  0.11496175  0.31056428 -0.24907665  0.07363646 -0.2230985  -0.15841435\n",
      " -0.1628767   0.11087784 -0.0334412  -0.05062202 -0.00179135  0.0764638\n",
      " -0.22869736 -0.28426218  0.05004999 -0.16561031  0.08612207 -0.2928643\n",
      " -0.40281987 -0.17026098  0.18397865 -0.12793317 -0.23994267  0.0604757\n",
      "  0.08275656  0.1158869  -0.02159754 -0.04251725 -0.3098416  -0.12713492\n",
      "  0.02285277  0.11467695 -0.06695411  0.05951071  0.03331789  0.15002121\n",
      "  0.26081973  0.21730217  0.03215421 -0.10255507  0.07920791 -0.02799113\n",
      " -0.21324109  0.24852557 -0.21393481  0.25008544  0.10402782  0.22438806\n",
      "  0.05026201 -0.04449293  0.02448577  0.08897486  0.09045751  0.05951104\n",
      "  0.11805879  0.01613821 -0.17746414 -0.2053994   0.14604968  0.01300118\n",
      "  0.04126076  0.17780127 -0.08589857  0.04149948 -0.03612146  0.23432827\n",
      "  0.28906164  0.02350358  0.08706368 -0.2732882   0.03246232  0.12074046\n",
      " -0.18725172 -0.21230724  0.10428357  0.00599805]\n",
      "----------------------------------------------------------------------\n",
      "Title: Star_Wars_(film)\n",
      "Embedding: [-0.35378626  0.05801983  0.23093407  0.14878249 -0.03285825 -0.33850816\n",
      "  0.25091344  0.40153062 -0.26853484 -0.2674043  -0.05130502 -0.39276442\n",
      "  0.02825448 -0.07880398 -0.01864235 -0.1260179   0.19038175 -0.01515524\n",
      " -0.18585835 -0.2859653   0.07416745  0.19675337  0.07305588 -0.04825011\n",
      "  0.02932598  0.2980375  -0.19970696 -0.06123476 -0.22182317 -0.11738469\n",
      " -0.07261504  0.05761918 -0.1201497   0.13471928 -0.01283645  0.1307945\n",
      " -0.24717462 -0.11735442  0.02733668 -0.13266987 -0.01588154 -0.2875916\n",
      " -0.4682408  -0.17530699  0.10389222  0.05559388 -0.19870685  0.11920263\n",
      "  0.05883037  0.06216843  0.18398602  0.05977768 -0.14420176 -0.06301109\n",
      "  0.02458962  0.09373066 -0.04960991 -0.01561266 -0.0374672   0.15047465\n",
      "  0.08639476  0.26574934  0.08748329 -0.2639868  -0.11651304  0.0974116\n",
      " -0.29891852  0.19481982 -0.18349706  0.17095587  0.08582228  0.15407063\n",
      "  0.1605415  -0.02658812 -0.06138991  0.1597756   0.02694589  0.08560124\n",
      "  0.16481693 -0.11762103 -0.1882199  -0.27428845  0.08934593 -0.04658258\n",
      "  0.00981463  0.23084274 -0.05927275 -0.00416034  0.04436799  0.05322018\n",
      "  0.15894769 -0.00909428  0.18254778 -0.1979731  -0.03094608  0.06982774\n",
      " -0.07916562 -0.11286665  0.05229554 -0.15639845]\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "For the  testing dataset:\n",
      "Title: Monty_Python's_Life_of_Brian\n",
      "Embedding: [-0.25831184  0.0526552   0.24659054  0.17348373 -0.11639549 -0.35574454\n",
      "  0.29391238  0.4243951  -0.22764327 -0.17703974 -0.0191918  -0.29806936\n",
      " -0.03908418  0.03383212 -0.09527439 -0.07406714  0.13040316  0.04141763\n",
      " -0.2037689  -0.36039042 -0.04224373  0.13835853  0.0036546   0.00510514\n",
      "  0.05055307  0.27944255 -0.29022717 -0.00568299 -0.18843253 -0.11478765\n",
      " -0.15487012  0.08250224 -0.03625749  0.05398312  0.02983708  0.04132105\n",
      " -0.1678635  -0.22280891  0.05287664 -0.1877694   0.06380574 -0.34170303\n",
      " -0.3992673  -0.18075866  0.19218454 -0.08504197 -0.23367889  0.13607614\n",
      "  0.05372404  0.11944411  0.02987958  0.0063829  -0.29412866 -0.09477068\n",
      " -0.00717492  0.06998096 -0.11214113  0.0426778   0.04453284  0.04975238\n",
      "  0.1998551   0.19354422  0.01515534 -0.15928097  0.04203857  0.08056711\n",
      " -0.28276005  0.2608259  -0.23352641  0.15912007  0.06225391  0.17193387\n",
      "  0.1572755  -0.03871161 -0.03979667  0.1695236   0.13670659  0.09458555\n",
      "  0.16284978 -0.02432337 -0.22211158 -0.23645572  0.05559612  0.00263881\n",
      " -0.03105829  0.17578535 -0.05444995  0.05064297  0.07130478  0.12377407\n",
      "  0.19447212  0.07915128  0.12876472 -0.22405349  0.039374    0.12592773\n",
      " -0.13592985 -0.15017253  0.04978108 -0.04691358]\n",
      "----------------------------------------------------------------------\n",
      "Title: Superman_(1978_film)\n",
      "Embedding: [-2.41630331e-01  2.06170976e-02  2.10200399e-01  1.64978161e-01\n",
      " -1.07915781e-01 -3.72544020e-01  2.77532935e-01  4.24134612e-01\n",
      " -2.13455155e-01 -1.70958221e-01 -2.34110504e-02 -3.17851961e-01\n",
      "  2.57840306e-02  4.23155166e-02 -1.63131338e-02 -5.22867404e-02\n",
      "  1.72483027e-01  9.63196508e-04 -2.24803686e-01 -3.42331529e-01\n",
      " -7.61262514e-03  1.39432758e-01  5.37882335e-02 -6.80379989e-03\n",
      "  5.23679964e-02  2.58628756e-01 -1.82804748e-01  5.96905723e-02\n",
      " -2.00213015e-01 -1.22310601e-01 -1.40306696e-01  1.19430989e-01\n",
      " -8.57482255e-02  5.16749509e-02  1.17640197e-02  7.80929402e-02\n",
      " -1.92133233e-01 -1.88571170e-01  2.34891977e-02 -1.49935573e-01\n",
      "  6.72236364e-03 -2.39834994e-01 -4.04547691e-01 -1.86589375e-01\n",
      "  1.73040435e-01 -1.73590612e-02 -2.29737312e-01  1.50286436e-01\n",
      "  9.65487435e-02  9.90922004e-02  9.44424123e-02  3.38753164e-02\n",
      " -2.45061085e-01 -1.06612362e-01 -2.87562981e-02  9.27068815e-02\n",
      " -7.01128542e-02  3.04828715e-02  1.26897627e-02  1.03186257e-01\n",
      "  1.42909780e-01  2.12879092e-01 -1.44168315e-03 -1.36567131e-01\n",
      " -5.44050848e-03  9.23347697e-02 -2.44960830e-01  2.04552546e-01\n",
      " -2.18190834e-01  1.26551151e-01  1.53821900e-01  1.72047675e-01\n",
      "  1.51801884e-01 -5.98254576e-02 -5.60206883e-02  1.86495915e-01\n",
      "  7.59915188e-02  6.67325184e-02  1.48679882e-01 -5.77407554e-02\n",
      " -2.45640457e-01 -2.33246714e-01  4.24199514e-02 -2.91985198e-04\n",
      " -2.95831561e-02  1.74188510e-01 -8.09616819e-02  2.02171616e-02\n",
      "  4.19009142e-02  1.12881176e-01  1.83372632e-01  8.28644261e-02\n",
      "  1.83745682e-01 -2.22979531e-01  6.90236241e-02  7.02417269e-02\n",
      " -1.27856240e-01 -1.43746600e-01  8.25640410e-02 -6.25410080e-02]\n",
      "----------------------------------------------------------------------\n",
      "Title: Boys_Don't_Cry_(1999_film)\n",
      "Embedding: [-0.22404188 -0.01867747  0.2523596   0.106583   -0.09673392 -0.31679544\n",
      "  0.26730242  0.39509436 -0.21187511 -0.1973781  -0.03849635 -0.348283\n",
      "  0.03472193  0.11420026 -0.07572306 -0.0235968   0.15511137  0.07698319\n",
      " -0.14510858 -0.36370647 -0.09266761  0.14807855  0.04964746 -0.01178589\n",
      "  0.16918612  0.24191698 -0.20306505  0.0272362  -0.1913907  -0.13976038\n",
      " -0.1655668   0.15402015 -0.00702796 -0.04972709  0.03356618  0.06484328\n",
      " -0.22809602 -0.26943293  0.10276478 -0.16124234  0.0624146  -0.32000753\n",
      " -0.3387249  -0.1962962   0.21120699 -0.12022158 -0.19484243  0.13141386\n",
      "  0.04347452  0.17521632  0.04801134  0.03663047 -0.29876506 -0.13839576\n",
      "  0.00666755  0.07110569 -0.14683105  0.05050461  0.00067874  0.04693154\n",
      "  0.21432963  0.14734049  0.04519179 -0.1610424   0.02001772  0.09853068\n",
      " -0.27565032  0.20888798 -0.33510154  0.20681404  0.06610738  0.22165425\n",
      "  0.11318707 -0.08498344  0.02287774  0.08048291  0.12583259  0.10159397\n",
      "  0.16383615  0.02211837 -0.2777995  -0.19517946  0.07103476 -0.00315872\n",
      " -0.03180396  0.10527239 -0.05142783 -0.01484825 -0.01204302  0.20785047\n",
      "  0.23681827  0.11346701  0.11942403 -0.21164109  0.14346273  0.1514761\n",
      " -0.14932801 -0.17285608  0.13426377 -0.0249292 ]\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "####################################################################################\n",
    "###  2. Use the first paragraph’s words and take an average on these embeddings  ###\n",
    "####################################################################################\n",
    "def find_first_paragraph(text: str):\n",
    "    return text.split(\"\\n\")[0]\n",
    "\n",
    "def average_first_para_words(data_list: List[Dict]=train_data_list, mode: str=\"title\") -> List[tuple]:\n",
    "    doc_embeddings = []\n",
    "    if mode not in [\"title\", \"label\"]:\n",
    "        raise ValueError(\"Please input a valid mode: ['title', 'label']\")\n",
    "    for line in data_list:\n",
    "        doc_info = line[mode]\n",
    "        valid_word_embeddings = [model.wv[word] for word in utils.simple_preprocess(find_first_paragraph(line[\"text\"])) if word in model.wv]\n",
    "        # since the number of words in the first paragraph is so small, it may occur `ZeroDivisonError` in the computation, here I use a \n",
    "        # try-except flow to handle this exception\n",
    "        try:\n",
    "            doc_embedding = sum(valid_word_embeddings) / len(valid_word_embeddings)\n",
    "        except:\n",
    "            doc_embedding = np.zeros(model.vector_size)\n",
    "        doc_embeddings.append((doc_embedding, doc_info))\n",
    "\n",
    "    return doc_embeddings\n",
    "\n",
    "print_training_testing(average_first_para_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b65aeca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the training dataset:\n",
      "Title: Citizen_Kane\n",
      "Embedding: [-0.91197944  1.0177789  -0.87328035 -2.8970726  -0.22374134 -1.6525056\n",
      " -0.09971668  2.0014598   1.2450341  -1.5300171   3.6276844   1.0656625\n",
      "  0.5074591  -3.4257743  -3.2505605  -2.8916092   2.9884274   4.4198318\n",
      "  0.44310835  1.0066502  -0.46507087 -2.0297766   0.33820763  1.1429517\n",
      " -1.2871732  -1.0288019   2.1324449  -3.1070325   3.8972461  -0.584727\n",
      " -0.8729446  -0.89607465  2.514835    2.4750972  -0.9747412   2.2987094\n",
      " -0.68717337 -3.8264184  -3.9325871   3.0186417  -0.10682293 -2.2484853\n",
      "  1.5026642  -6.136729    2.8145642  -3.7217798   2.4966438   0.64525604\n",
      "  0.4302792  -3.3386102   2.6108866   0.08630164 -0.8840496  -2.3507173\n",
      "  1.5288154   2.5773897   0.6166947   1.647873   -3.0836458  -1.5615094\n",
      " -2.6482725   1.2739259   2.9393506  -0.6705638  -0.39135727 -1.0394808\n",
      "  0.9505518  -3.24473    -0.87178475 -2.5605524  -4.557449    2.2995675\n",
      " -4.329714   -4.0590553  -3.206194    3.2260227   5.4857607  -2.2458603\n",
      "  1.4818414   4.8264704  -3.4413147   2.7079453  -3.979476    2.0074317\n",
      " -1.9893891   2.145051   -2.0285602   1.4970496  -0.32965574  5.408203\n",
      " -2.0462112   1.9091547  -3.0209694   0.7074156   2.3858755   1.4178252\n",
      "  2.3782368  -0.39400554 -1.427141   -1.0193422 ]\n",
      "----------------------------------------------------------------------\n",
      "Title: It_(2017_film)\n",
      "Embedding: [ 2.8081996  -2.340445    2.2275212   0.5069369  -6.0333023   2.2668402\n",
      " -0.7780963   0.91479886  1.0604876   2.9616363   3.3797402  -1.738484\n",
      "  0.2440735  -6.3372154   1.9181706  -3.7782025  -0.05383308  3.8191042\n",
      " -4.6015677   3.9623756   3.0868561  -2.2869873   1.3608853  -2.4767723\n",
      " -3.7981613  -2.0534582  -5.6249514  -1.4422605   1.9208074  -3.045682\n",
      " -3.3777177   1.9869912  -1.2438604   2.371259    0.67576206 -1.7777367\n",
      "  1.1727139  -6.0486484  -3.8434033   0.5149402   4.7332273  -0.17155449\n",
      "  4.387896   -5.9117804  -0.7770168  -2.914445    3.1459293  -0.19160293\n",
      " -0.24227975 -3.707514    0.6336367   0.16161463  2.9227433   0.02487064\n",
      " -1.5095292   1.5331556  -2.2470434   2.8330507  -0.011652    0.5300026\n",
      "  3.8757546   0.6447119  -0.1523297  -3.5635517   2.1460822  -0.10695105\n",
      " -3.2518048  -0.6935996   0.56662935  4.85012    -5.9676323  -1.0101976\n",
      " -1.1239446  -0.48053437 -2.4244173   2.478061   -0.00683187 -5.550066\n",
      "  0.32614285 -2.0507853   0.7842983   5.5086184  -1.0164237   0.9318601\n",
      " -3.150467    3.6889644   1.5180621  -2.8505936   0.5009331   2.1473055\n",
      " -2.0827496   2.4248552  -0.43641707  2.4766893   1.7334456   3.882272\n",
      " -0.17844757 -0.27888995 -2.7013721  -2.6565382 ]\n",
      "----------------------------------------------------------------------\n",
      "Title: Star_Wars_(film)\n",
      "Embedding: [ 0.63162476 -2.270414   -0.7372657  -2.9321766  -1.5350251  -1.4761659\n",
      " -0.8552243   0.8877876   0.525315    0.19442332  2.3416553  -1.6340775\n",
      "  1.7068788  -1.8457061   0.06810638 -1.2011995   0.26379094  2.207956\n",
      " -4.176504    3.1792002  -0.19664606 -1.8652352  -0.19115128  1.892879\n",
      " -1.1687453  -2.161996   -0.87480175 -1.3602239   0.9977563   0.68905175\n",
      " -1.7626398  -1.9474926  -1.7156199  -4.1512194   2.261246   -0.21765502\n",
      "  2.5054166  -4.3793054  -3.8260312  -1.0324316   0.43909445  1.9972105\n",
      "  1.0254112  -5.4265256  -1.5311744  -2.3116653   1.7292597   4.017262\n",
      "  0.21937004 -0.74105364  2.8614585  -1.2585764  -3.2613835  -1.2045995\n",
      "  0.97170043  4.623182   -0.2142757   0.2980449  -1.7236459   0.25899377\n",
      "  0.39954165  3.1955638   3.0079622  -5.2721467   0.1501142   0.68406516\n",
      " -0.14889446 -2.840976    0.3811852   1.226131   -4.4240203  -1.8608799\n",
      " -4.4131303  -2.284116   -1.1674192   2.1523383  -0.6434457  -6.0361414\n",
      "  3.689852    5.3499594  -0.03495538  1.4439805  -2.7790706  -0.02618023\n",
      " -4.6014333   1.4891434  -2.2507102  -0.8951452  -2.0343773  -0.554146\n",
      " -4.492081    2.0308545  -2.2578444  -2.9041045   2.3883667   1.5444973\n",
      "  2.1234846   2.1465213  -2.2542307   1.312463  ]\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "For the  testing dataset:\n",
      "Title: Monty_Python's_Life_of_Brian\n",
      "Embedding: [-0.89860815 -0.883758   -4.9695325   0.0554833  -1.590777    1.2490648\n",
      " -1.5230428   1.3057338   0.08720325  0.3772103   1.445082    0.02943848\n",
      "  0.03709232 -2.612443   -0.05262998  0.7501247   1.6980746   1.2885275\n",
      " -2.0012224   1.0887657  -0.96086645 -2.968004   -0.28776795  0.11519707\n",
      " -0.5939574  -0.8225818  -1.0645239  -1.6902452   0.9707223  -1.1598737\n",
      "  0.0493518  -1.9707675   0.59614605 -1.3672471   1.9142798  -1.3547887\n",
      " -3.2844586  -1.8852712  -2.6602323   0.820371    0.43255198 -0.38621098\n",
      "  0.5352609  -4.2616386  -0.4876825  -1.2955608   1.2327653  -0.7047037\n",
      " -0.9518653  -2.4864485   1.436311    0.9963832  -0.3571175   0.11616255\n",
      "  3.1777537   3.4444792  -0.5617688   1.2866756   0.1529003  -0.43129256\n",
      " -0.33209345  1.0018302   2.2956731  -2.1812906  -0.01969972  0.854905\n",
      "  1.4302266   1.6524286   0.06392208 -0.97125256 -1.1412313  -3.1397324\n",
      " -1.9551332  -0.84173536 -0.69935817  1.7476771   1.4206265  -2.2615802\n",
      "  1.3059253   1.4293318  -0.35832143  1.4816717  -3.1399236  -1.2411549\n",
      " -0.9395142  -0.58927196  0.04910982  0.602575    0.8457195   0.5474641\n",
      " -0.06072138  0.8211262  -1.3515978  -2.7739646   0.49716     0.529752\n",
      "  0.02042589  1.3393273  -1.4002942   1.142318  ]\n",
      "----------------------------------------------------------------------\n",
      "Title: Superman_(1978_film)\n",
      "Embedding: [ 0.9577962  -1.5021017  -0.61792666 -1.9665712  -2.4281058  -0.12959413\n",
      " -0.42010155  0.85812795  0.99996895 -0.17518066  2.9147313   0.396391\n",
      " -0.8892557  -2.0619433   1.520945   -1.7486342   1.4312768   2.4882452\n",
      " -1.3596016   3.7017257   1.387222   -1.0232527   0.05988383  0.41079435\n",
      " -2.5784738  -0.8036588  -1.1013018  -1.3683361   2.2768931  -1.9864582\n",
      " -1.7859517   0.01630008 -1.1160749  -0.15088592  1.4521625  -0.02874712\n",
      " -0.36864266 -4.656432   -3.4907453   1.2881558   2.1464643  -0.50667757\n",
      "  1.4300799  -4.356633   -0.258487   -3.379471    1.9209486   1.3178153\n",
      " -0.525162   -1.4816304   2.2025692   0.8057396   0.30119094 -0.3493252\n",
      "  0.68990767  3.4819775  -1.0790006   0.9910546  -1.6618993  -1.6482998\n",
      "  1.6404804   1.5304922   2.342246   -4.153594    1.8222586   0.09517507\n",
      "  1.3007009  -2.2033956   1.1640885   1.0343409  -3.2933614  -0.69127315\n",
      " -2.6703966  -1.7243172  -0.96308684  3.2911646   0.8632237  -3.3295445\n",
      "  2.1026962   3.114298    1.8338796   2.2466621  -2.299944    0.43485916\n",
      " -3.3194575   0.87116456 -0.6325598  -0.29217452 -0.8608608   0.34286657\n",
      " -1.8366901   1.1733016  -1.8060784  -0.70780784  0.88215286  1.3600734\n",
      "  1.1310763   1.6685679  -1.5747225   0.5213141 ]\n",
      "----------------------------------------------------------------------\n",
      "Title: Boys_Don't_Cry_(1999_film)\n",
      "Embedding: [-0.94355476 -1.9448522  -1.554126    1.3865299  -2.3767068  -0.43800077\n",
      " -1.0733397   1.7243305  -0.14933524 -1.0795094   3.5772889  -1.0939041\n",
      "  1.1791195  -1.9459666   1.8517385  -1.7105254   0.65327716  2.808311\n",
      " -1.4836042   3.907216    1.9279418  -0.7081744  -1.9532531  -2.4497178\n",
      " -3.438929    0.2773814  -0.48382452 -3.0198267  -0.22362046 -2.3886085\n",
      " -0.31538585  0.8653349  -0.13781458  1.2718465  -0.4012166  -0.72248465\n",
      " -2.3153176  -3.620161   -2.7189817   3.9989333  -0.31059557 -1.6473553\n",
      "  0.7251755  -3.6700568  -0.8494072  -1.925111    1.9031388  -0.8999885\n",
      " -0.01566174 -0.98644346  3.4837165   1.7791358   1.8180456   0.3799463\n",
      "  0.22068246  2.190319    0.1594286   2.1329308   0.12033839 -2.5347674\n",
      "  0.46380383  1.1411574   0.85198337 -1.6654271   0.23309438 -1.0840633\n",
      "  1.0745916  -0.6709608  -0.54996985  1.0227277  -2.9982097  -2.3018339\n",
      " -0.9422757  -2.4142072  -1.4784112   2.4930594  -1.1552275  -1.6732744\n",
      "  0.02955263 -0.20588772  0.06744149  5.6023474  -3.2864     -0.24734667\n",
      " -0.82050824  0.7815815  -2.011862   -0.7776346   0.8466296   0.49099743\n",
      " -0.5253126  -0.10248404 -3.8820143  -2.5247734   1.7717696   2.2433116\n",
      " -0.41494718  0.11875352 -1.6467686  -0.3481801 ]\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "####################################################################################\n",
    "###             3. Use the doc2vec algorithm to present each document            ###\n",
    "####################################################################################\n",
    "def build_doc2vec() -> Doc2Vec:\n",
    "    \"\"\"\n",
    "    Build the `Doc2Vec` model based on the `train_data_list`\n",
    "    \"\"\"\n",
    "    tagged_data = [TaggedDocument(words=doc.split(), tags=[str(i)]) for i, doc in enumerate([line[\"text\"] for line in train_data_list])]\n",
    "    model = Doc2Vec(vector_size=100, window=5, min_count=5, epochs=10)\n",
    "    model.build_vocab(tagged_data)\n",
    "    model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "    return model\n",
    "\n",
    "doc2vec_model = build_doc2vec()\n",
    "\n",
    "def doc2vec_model_embeddings(data_list: List[Dict]=train_data_list, mode: str=\"title\") -> List[tuple]:\n",
    "    embeddings = []\n",
    "    if mode not in [\"title\", \"label\"]:\n",
    "        raise ValueError(\"Please input a valid mode: ['title', 'label']\")\n",
    "    for line in data_list:\n",
    "        embeddings.append((doc2vec_model.infer_vector(line[\"text\"].split()), line[mode]))\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "print_training_testing(doc2vec_model_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091195ab-834c-4184-a8f2-490cd26b7ff4",
   "metadata": {},
   "source": [
    "### Task 4 Build classifier to test docs\n",
    "> Build softmax regression model to classifier testing documents based on these training doc embeddings. Does it getting better than Naive Bayes'? (You have 3 models.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "98c43b8e-603f-4352-982c-f90d85e9f790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:             Use the average of embeddings of all words in each document, accuracy: 0.930000\n",
      "Model: Use the first paragraph’s words and take an average on these embeddings, accuracy: 0.940000\n",
      "Model:                      Use the doc2vec algorithm to present each document, accuracy: 0.920000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class softmaxModel:\n",
    "    def __init__(self, data: List[Dict], method: Callable) -> None:\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        self.__model = None\n",
    "        self.__ytest = None\n",
    "        self.__ypred = None\n",
    "        self.__method = method               # the three different models in task 3\n",
    "        self.__train(method(data, \"label\"))  # use `method(data)` to get the embedding\n",
    "\n",
    "    def __train(self, embedding: List[tuple]):\n",
    "        \"\"\"\n",
    "        Train a softmax classifier based on the embedding in train data\n",
    "        \"\"\"\n",
    "        self.X, self.y = self.__decode(embedding)\n",
    "        self.__model = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\").fit(self.X, self.y)\n",
    "\n",
    "    def __decode(self, embedding: List[tuple]) -> tuple:\n",
    "        X, y = map(list, zip(*embedding))\n",
    "        X, y =  map(np.array, [X, y])\n",
    "        return X, y\n",
    "\n",
    "    def predict(self, test_data: List[Dict]) -> List:\n",
    "        \"\"\"\n",
    "        Return the pred value of y based on the trained model and given test data\n",
    "        \"\"\"\n",
    "        test_embedding = self.__method(test_data, \"label\")\n",
    "        X_test, y_test = self.__decode(test_embedding)\n",
    "        self.__ytest = y_test\n",
    "        self.__ypred = self.__model.predict(X_test)\n",
    "        return self.__ypred\n",
    "\n",
    "    def accuracy(self) -> float:\n",
    "        return accuracy_score(self.__ytest, self.__ypred)\n",
    "    \n",
    "methods = {\n",
    "    \"Use the average of embeddings of all words in each document\": average_all_words, \n",
    "    \"Use the first paragraph’s words and take an average on these embeddings\": average_first_para_words, \n",
    "    \"Use the doc2vec algorithm to present each document\": doc2vec_model_embeddings\n",
    "}\n",
    "\n",
    "for text, method in methods.items():\n",
    "    softmax_model = softmaxModel(train_data_list, method)\n",
    "    softmax_model.predict(test_data_list)\n",
    "    print(\"Model: {:>71}, accuracy: {:>.6f}\".format(text, softmax_model.accuracy()))\n",
    "    del softmax_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8206de2a-6d96-4016-b74e-4a08da2e1cab",
   "metadata": {},
   "source": [
    "### Task 5 Use t-SNE to project doc vectors\n",
    "\n",
    "> Use t-SNE to project training document embeddings into 2d and plot them out for each of the above choices. Each point should have a specific color (represent a particular cluster). You may need to try different parameters of t-SNE. One can find more details about t-SNE in this [excellent article](https://distill.pub/2016/misread-tsne/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907fe1b9-0650-438c-ad75-49e6632bf6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
