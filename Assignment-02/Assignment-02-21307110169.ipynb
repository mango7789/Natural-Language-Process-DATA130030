{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36836ad4-35bb-4d04-9118-170b1a3f37d1",
   "metadata": {},
   "source": [
    "### Task 0 Before your go\n",
    "\n",
    "> 1. Rename Assignment-02-###.ipynb where ### is your student ID.\n",
    "> 2. The deadline of Assignment-02 is 23:59pm, 04-21-2024\n",
    "> 3. In this assignment, you will use word embeddings to explore our Wikipedia dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caec95d8-a23b-4116-8d00-197a049cfd4e",
   "metadata": {},
   "source": [
    "### Task 1 Train word embeddings using SGNS \n",
    "> Use our enwiki-train.json as training data. You can use the [Gensim tool](https://radimrehurek.com/gensim/models/word2vec.html). But it is recommended to implement by yourself. You should explain how hyper-parameters such as dimensionality of embeddings, window size, the parameter of negative sampling strategy, and initial learning rate have been chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "baf0db53-3382-4534-b93e-e2a03796d525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import some necessary libraries\n",
    "from typing import List\n",
    "from collections import defaultdict\n",
    "from gensim.models import Word2Vec, Doc2Vec\n",
    "from gensim import utils\n",
    "import json\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b61425c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the train and test data from the json file\n",
    "\n",
    "# NOTE: The function is inherited from my solution of assignment 1\n",
    "def load_json(file_path: str) -> list:\n",
    "    \"\"\"\n",
    "    Fetch the data from `.json` file and concat them into a list.\n",
    "\n",
    "    Input:\n",
    "    - file_path: The relative file path of the `.json` file\n",
    "\n",
    "    Returns:\n",
    "    - join_data_list: A list containing the data, with the format of [{'title':<>, 'label':<>, 'text':<>}, {}, ...]\n",
    "    \"\"\"\n",
    "    join_data_list = []\n",
    "    with open(file_path, \"r\") as json_file:\n",
    "        for line in json_file:\n",
    "            line = line.strip()\n",
    "            # guaranteen the line is not empty\n",
    "            if line: \n",
    "                join_data_list.append(json.loads(line))\n",
    "    return join_data_list\n",
    "\n",
    "train_file_path, test_file_path = \"enwiki-train.json\", \"enwiki-test.json\"\n",
    "train_data_list, test_data_list = map(load_json, [train_file_path, test_file_path])\n",
    "\n",
    "class Corpus:\n",
    "    def __iter__(self):\n",
    "        for line in train_data_list:\n",
    "            yield utils.simple_preprocess(line[\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "876121ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyWord2Vec:\n",
    "    def __init__(self, text: List[map], dimensionality: int=100, window_size: int=5, negative_samples: int=5, lr: float=0.001) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - text: The training data\n",
    "        - dimensionality: The dimension of the word embeddings\n",
    "        - window_size: The size of the context window\n",
    "        - negative_samples: The number of negative samples\n",
    "        - lr: Learning rate of the algorithm\n",
    "        \"\"\"\n",
    "        self.dim = dimensionality\n",
    "        self.window = window_size\n",
    "        self.neg = negative_samples\n",
    "        self.lr = lr\n",
    "        self.__vocab = set()\n",
    "        self.__word_frq = defaultdict(int)\n",
    "        self.__word2idx = {}\n",
    "        self.__idx2word = {}\n",
    "        self.__embedding = None\n",
    "        self.__context_words = []\n",
    "        self.__context_targets = []\n",
    "        self.__build(text)\n",
    "        \n",
    "\n",
    "    def __preprocess(self, text: List[str]) -> None:\n",
    "        \"\"\"\n",
    "        Calculate the vocabulary and the frequency of each word in the training data, while maintaining the (idx, word) map.\n",
    "        \"\"\"\n",
    "        for sample in text:\n",
    "            words = sample[\"text\"].split()\n",
    "            for word in words:\n",
    "                self.__vocab.add(word)\n",
    "                self.__word_frq[word] += 1\n",
    "        for idx, word in enumerate(self.__vocab):\n",
    "            self.__word2idx[word] = idx\n",
    "            self.__idx2word[idx] = word\n",
    "\n",
    "    def __generate_training_data(self, text: List[str]) -> None:\n",
    "        \"\"\"\n",
    "        Generate training data from each window and save them in `self.__context_words` and `self.__context_targets`\n",
    "        \"\"\"\n",
    "        for sample in text:\n",
    "            words = sample[\"text\"].split()\n",
    "            for i, curr_word in enumerate(words):\n",
    "                # the \"window\" around the current world\n",
    "                for j in range(max(0, i - self.window), min(i + self.window + 1, len(words))):\n",
    "                    if i != j:\n",
    "                        self.__context_words.append(self.__word2idx[curr_word])\n",
    "                        self.__context_targets.append(self.__word2idx[words[j]])\n",
    "\n",
    "    def __initialize_embedding(self) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the embedding matrix with random values\n",
    "        \"\"\"\n",
    "        self.__embedding = np.random.uniform(-0.5 / self.dim, 0.5 / self.dim, size=(len(self.__vocab), self.dim))\n",
    "    \n",
    "    def __build(self, text: List[map]) -> None:\n",
    "        \"\"\"\n",
    "        Compute and store the relevant information of the training data in the class\n",
    "        \"\"\"\n",
    "        self.__preprocess(text)\n",
    "        self.__generate_training_data(text)\n",
    "        self.__initialize_embedding()\n",
    "\n",
    "    def train(self, epochs: int=5) -> None: \n",
    "        for epoch in range(epochs):\n",
    "            # learning rate decay\n",
    "            learning_rate = self.lr * (1 - epoch / epochs)\n",
    "\n",
    "            print(\"Training Epoch: %d\" % (epoch + 1))\n",
    "\n",
    "            for context_word, target_word in zip(self.__context_words, self.__context_targets):\n",
    "                context_vector = self.__embedding[context_word]\n",
    "                target_vector = self.__embedding[target_word]\n",
    "\n",
    "                # positive sample update\n",
    "                score = np.dot(target_vector, context_vector)\n",
    "                exp_score = math.exp(score)\n",
    "                grad_context = (exp_score / (1 + exp_score) - 1) * target_vector\n",
    "                grad_target = (exp_score / (1 + exp_score) - 1) * context_vector\n",
    "                self.__embedding[context_word] -= learning_rate * grad_context\n",
    "                self.__embedding[target_word] -= learning_rate * grad_target\n",
    "\n",
    "                # negative sample update\n",
    "                for _ in range(self.neg):\n",
    "                    negative_word = random.randint(0, len(self.__vocab) - 1)\n",
    "                    if negative_word != target_word:\n",
    "                        negative_vector = self.__embedding[negative_word]\n",
    "                        score = np.dot(negative_vector, context_vector)\n",
    "                        exp_score = math.exp(score)\n",
    "                        grad_context = exp_score / (1 + exp_score) * negative_vector\n",
    "                        grad_target = exp_score / (1 + exp_score) * context_vector\n",
    "                        self.__embedding[context_word] -= learning_rate * grad_context\n",
    "                        self.__embedding[target_word] -= learning_rate * grad_target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "286cfc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = Corpus()\n",
    "model = Word2Vec(sentences=sentence, vector_size=100, alpha=0.025, window=5, \n",
    "                               min_count=5, sample=0.001, seed=1, workers=3, min_alpha=0.0001, \n",
    "                               sg=1, negative=5, ns_exponent=0.75, epochs=5, sorted_vocab=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeadc4b2-2c39-46f1-a9b9-28089393c24f",
   "metadata": {},
   "source": [
    "### Task 2 Find similar/dissimilar word pairs\n",
    "\n",
    "> Randomly generate 100, 1000, and 10000-word pairs from the vocabularies. For each set, print 5 closest word pairs and 5 furthest word pairs (you can use cosine-similarity to measure two words). Explain your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d81f7d38-0bf0-4e55-aedc-b0f8f24195f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For   100 random pairs from the vocabularies:\n",
      "The 5  closest word pairs:\n",
      "Word pairs: (      herodotus,       sorcerers) --> Similarity: 0.888745\n",
      "Word pairs: (           sown,     prohibitive) --> Similarity: 0.849635\n",
      "Word pairs: (         shaded,           booby) --> Similarity: 0.839130\n",
      "Word pairs: (        deflect,      comforting) --> Similarity: 0.830611\n",
      "Word pairs: (        bruxing,        plumbers) --> Similarity: 0.827411\n",
      "The 5 furthest word pairs:\n",
      "Word pairs: (     violations,         yevgeny) --> Similarity: 0.248302\n",
      "Word pairs: (     preferring,  neolissochilus) --> Similarity: 0.226008\n",
      "Word pairs: (       superman,         outputs) --> Similarity: 0.219238\n",
      "Word pairs: (        trapped,          arabia) --> Similarity: 0.195078\n",
      "Word pairs: (     separately,       communism) --> Similarity: 0.124271\n",
      "----------------------------------------------------------------------\n",
      "For  1000 random pairs from the vocabularies:\n",
      "The 5  closest word pairs:\n",
      "Word pairs: (        itching,         dryness) --> Similarity: 0.907443\n",
      "Word pairs: (          glaze,           tripe) --> Similarity: 0.890766\n",
      "Word pairs: (       armature,       filaments) --> Similarity: 0.888100\n",
      "Word pairs: (         callum,        whitmore) --> Similarity: 0.879706\n",
      "Word pairs: (           chua,        lionized) --> Similarity: 0.879339\n",
      "The 5 furthest word pairs:\n",
      "Word pairs: (         lucien,     regulations) --> Similarity: 0.028043\n",
      "Word pairs: (   astronomical,          feared) --> Similarity: 0.027804\n",
      "Word pairs: (       category,          godwin) --> Similarity: 0.019664\n",
      "Word pairs: (        teenage,           pages) --> Similarity: 0.017133\n",
      "Word pairs: (           duff,           saccà) --> Similarity: -0.083931\n",
      "----------------------------------------------------------------------\n",
      "For 10000 random pairs from the vocabularies:\n",
      "The 5  closest word pairs:\n",
      "Word pairs: (          chaat,          capers) --> Similarity: 0.950709\n",
      "Word pairs: (    convertible,        callback) --> Similarity: 0.929111\n",
      "Word pairs: (       espinosa,         legrand) --> Similarity: 0.925043\n",
      "Word pairs: (        outflow,          efflux) --> Similarity: 0.924031\n",
      "Word pairs: (        crumbly,          yakhni) --> Similarity: 0.923268\n",
      "The 5 furthest word pairs:\n",
      "Word pairs: (         bypass,        romantic) --> Similarity: -0.022724\n",
      "Word pairs: (  vulnerability,         honneur) --> Similarity: -0.035284\n",
      "Word pairs: (        manotoc,       cluttered) --> Similarity: -0.048579\n",
      "Word pairs: (           vary,         schmidt) --> Similarity: -0.058368\n",
      "Word pairs: (      longridge,      projectile) --> Similarity: -0.134826\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def generate_random_paris(samples: int):\n",
    "    \"\"\"\n",
    "    Generate random indices without replacement, then pairs the indices to get word pairs\n",
    "    \"\"\"\n",
    "    indices = random.sample(range(len(model.wv)), 2 * samples)\n",
    "    indices1, indices2 = indices[:samples], indices[samples:]\n",
    "    return [model.wv.index_to_key[i] for i in indices1], [model.wv.index_to_key[i] for i in indices2]\n",
    "\n",
    "def find_closest_furthest(num: int=5, words1: List[str]=None, words2: List[str]=None) -> None:\n",
    "    \"\"\"\n",
    "    Find the cloest/furthest word pairs using `model.wv.similarity`.\n",
    "\n",
    "    Here a heap queue is used to reduce time complexity to $O(n\\log k)$, where k denotes the `num`\n",
    "    \"\"\"\n",
    "    heap = []\n",
    "    for i in range(len(words1)):\n",
    "        # compute the similarity and push it into the heap\n",
    "        heapq.heappush(heap, (model.wv.similarity(words1[i], words2[i]), words1[i], words2[i]))\n",
    "    return heapq.nlargest(num, heap), heapq.nsmallest(num, heap)[::-1]\n",
    "\n",
    "def print_word_pairs(results: List[tuple], flag: str) -> None:\n",
    "    \"\"\"\n",
    "    Print the result in formatted string\n",
    "    \"\"\"\n",
    "    print(\"The 5 {:>8} word pairs:\".format(flag))\n",
    "    for result in results:\n",
    "       print(\"Word pairs: ({:>15}, {:>15}) --> Similarity: {:>8.6f}\".format(result[1], result[2], result[0]))\n",
    "\n",
    "\n",
    "random.seed(408)\n",
    "pairs = [100, 1000, 10000]\n",
    "for pair in pairs:\n",
    "    print(\"For {:>5} random pairs from the vocabularies:\".format(pair))\n",
    "    cloest, furthest = find_closest_furthest(5, *generate_random_paris(pair))\n",
    "    print_word_pairs(cloest, \"closest\")\n",
    "    print_word_pairs(furthest, \"furthest\")\n",
    "    print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af756bde-2960-4c56-b037-0bfd2cf7e47e",
   "metadata": {},
   "source": [
    "### Task 3 Present a document as an embedding\n",
    "\n",
    "> For each document, you have several choices to generate document embedding: 1. Use the average of embeddings of all words in each document; 2. Use the first paragraph’s words and take an average on these embeddings; 3. Use the doc2vec algorithm to present each document. Do the above for both training and testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "279547d8-e4e7-4bfa-9be9-6d535b86bbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################\n",
    "###        1. Use the average of embeddings of all words in each document        ###\n",
    "####################################################################################\n",
    "doc_embeddings_1 = {}\n",
    "for line in train_data_list:\n",
    "    doc_title = line[\"title\"]\n",
    "    doc_embedding = sum(model.wv.n_similarity)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae2cadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################\n",
    "###  2. Use the first paragraph’s words and take an average on these embeddings  ###\n",
    "####################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65aeca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################\n",
    "###             3. Use the doc2vec algorithm to present each document            ###\n",
    "####################################################################################\n",
    "model = Doc2Vec()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091195ab-834c-4184-a8f2-490cd26b7ff4",
   "metadata": {},
   "source": [
    "### Task 4 Build classifier to test docs\n",
    "> Build softmax regression model to classifier testing documents based on these training doc embeddings. Does it getting better than Naive Bayes'? (You have 3 models.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "98c43b8e-603f-4352-982c-f90d85e9f790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8206de2a-6d96-4016-b74e-4a08da2e1cab",
   "metadata": {},
   "source": [
    "### Task 5 Use t-SNE to project doc vectors\n",
    "\n",
    "> Use t-SNE to project training document embeddings into 2d and plot them out for each of the above choices. Each point should have a specific color (represent a particular cluster). You may need to try different parameters of t-SNE. One can find more details about t-SNE in this [excellent article](https://distill.pub/2016/misread-tsne/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "907fe1b9-0650-438c-ad75-49e6632bf6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
