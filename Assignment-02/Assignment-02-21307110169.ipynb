{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36836ad4-35bb-4d04-9118-170b1a3f37d1",
   "metadata": {},
   "source": [
    "### Task 0 Before your go\n",
    "\n",
    "> 1. Rename Assignment-02-###.ipynb where ### is your student ID.\n",
    "> 2. The deadline of Assignment-02 is 23:59pm, 04-21-2024\n",
    "> 3. In this assignment, you will use word embeddings to explore our Wikipedia dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caec95d8-a23b-4116-8d00-197a049cfd4e",
   "metadata": {},
   "source": [
    "### Task 1 Train word embeddings using SGNS \n",
    "> Use our enwiki-train.json as training data. You can use the [Gensim tool](https://radimrehurek.com/gensim/models/word2vec.html). But it is recommended to implement by yourself. You should explain how hyper-parameters such as dimensionality of embeddings, window size, the parameter of negative sampling strategy, and initial learning rate have been chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "baf0db53-3382-4534-b93e-e2a03796d525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import some necessary libraries\n",
    "from typing import List\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b61425c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the train and test data from the json file\n",
    "\n",
    "# NOTE: The function is inherited from my solution of assignment 1\n",
    "def load_json(file_path: str) -> list:\n",
    "    \"\"\"\n",
    "    Fetch the data from `.json` file and concat them into a list.\n",
    "\n",
    "    Input:\n",
    "    - file_path: The relative file path of the `.json` file\n",
    "\n",
    "    Returns:\n",
    "    - join_data_list: A list containing the data, with the format of [{'title':<>, 'label':<>, 'text':<>}, {}, ...]\n",
    "    \"\"\"\n",
    "    join_data_list = []\n",
    "    with open(file_path, \"r\") as json_file:\n",
    "        for line in json_file:\n",
    "            line = line.strip()\n",
    "            # guaranteen the line is not empty\n",
    "            if line: \n",
    "                join_data_list.append(json.loads(line))\n",
    "    return join_data_list\n",
    "\n",
    "train_file_path, test_file_path = \"enwiki-train.json\", \"enwiki-test.json\"\n",
    "train_data_list, test_data_list = map(load_json, [train_file_path, test_file_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "876121ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec:\n",
    "    def __init__(self, text: List[map], dimensionality: int=100, window_size: int=5, negative_samples: int=5, lr: float=0.001) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - text: The training data\n",
    "        - dimensionality: The dimension of the word embeddings\n",
    "        - window_size: The size of the context window\n",
    "        - negative_samples: The number of negative samples\n",
    "        - lr: Learning rate of the algorithm\n",
    "        \"\"\"\n",
    "        self.dim = dimensionality\n",
    "        self.window = window_size\n",
    "        self.neg = negative_samples\n",
    "        self.lr = lr\n",
    "        self.__vocab = set()\n",
    "        self.__word_frq = defaultdict(int)\n",
    "        self.__word2idx = {}\n",
    "        self.__idx2word = {}\n",
    "        self.__embedding = None\n",
    "        self.__context_words = []\n",
    "        self.__context_targets = []\n",
    "        self.__build(text)\n",
    "        \n",
    "\n",
    "    def __preprocess(self, text: List[str]) -> None:\n",
    "        \"\"\"\n",
    "        Calculate the vocabulary and the frequency of each word in the training data, while maintaining the (idx, word) map.\n",
    "        \"\"\"\n",
    "        for sample in text:\n",
    "            words = sample[\"text\"].split()\n",
    "            for word in words:\n",
    "                self.__vocab.add(word)\n",
    "                self.__word_frq[word] += 1\n",
    "        for idx, word in enumerate(self.__vocab):\n",
    "            self.__word2idx[word] = idx\n",
    "            self.__idx2word[idx] = word\n",
    "\n",
    "    def __generate_training_data(self, text: List[str]) -> None:\n",
    "        \"\"\"\n",
    "        Generate training data from each window and save them in `self.__context_words` and `self.__context_targets`\n",
    "        \"\"\"\n",
    "        for sample in text:\n",
    "            words = sample[\"text\"].split()\n",
    "            for i, curr_word in enumerate(words):\n",
    "                # the \"window\" around the current world\n",
    "                for j in range(max(0, i - self.window), min(i + self.window + 1, len(words))):\n",
    "                    if i != j:\n",
    "                        self.__context_words.append(self.__word2idx[curr_word])\n",
    "                        self.__context_targets.append(self.__word2idx[words[j]])\n",
    "\n",
    "    def __initialize_embedding(self) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the embedding matrix with random values\n",
    "        \"\"\"\n",
    "        self.__embedding = np.random.uniform(-0.5 / self.dim, 0.5 / self.dim, size=(len(self.__vocab), self.dim))\n",
    "    \n",
    "    def __build(self, text: List[map]) -> None:\n",
    "        \"\"\"\n",
    "        Compute and store the relevant information of the training data in the class\n",
    "        \"\"\"\n",
    "        self.__preprocess(text)\n",
    "        self.__generate_training_data(text)\n",
    "        self.__initialize_embedding()\n",
    "\n",
    "    def train(self, epochs: int=10) -> None: \n",
    "        for epoch in range(epochs):\n",
    "            learning_rate = self.lr * (1 - epoch / epochs)\n",
    "\n",
    "            for context_word, target_word in zip(self.__context_words, self.__context_targets):\n",
    "                context_vector = self.__embedding[context_word]\n",
    "                target_vector = self.__embedding[target_word]\n",
    "\n",
    "                # positive sample update\n",
    "                score = np.dot(target_vector, context_vector)\n",
    "                exp_score = math.exp(score)\n",
    "                grad_context = (exp_score / (1 + exp_score) - 1) * target_vector\n",
    "                grad_target = (exp_score / (1 + exp_score) - 1) * context_vector\n",
    "                self.__embedding[context_word] -= learning_rate * grad_context\n",
    "                self.__embedding[target_word] -= learning_rate * grad_target\n",
    "\n",
    "                # negative sample update\n",
    "                for _ in range(self.neg):\n",
    "                    negative_word = random.randint(0, len(self.__vocab) - 1)\n",
    "                    if negative_word != target_word:\n",
    "                        negative_vector = self.__embedding[negative_word]\n",
    "                        score = np.dot(negative_vector, context_vector)\n",
    "                        exp_score = math.exp(score)\n",
    "                        grad_context = exp_score / (1 + exp_score) * negative_vector\n",
    "                        grad_target = exp_score / (1 + exp_score) * context_vector\n",
    "                        self.embeddings[context_word] -= learning_rate * grad_context\n",
    "                        self.embeddings[target_word] -= learning_rate * grad_target\n",
    "\n",
    "\n",
    "word2vec = Word2Vec(train_data_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeadc4b2-2c39-46f1-a9b9-28089393c24f",
   "metadata": {},
   "source": [
    "### Task 2 Find similar/dissimilar word pairs\n",
    "\n",
    "> Randomly generate 100, 1000, and 10000-word pairs from the vocabularies. For each set, print 5 closest word pairs and 5 furthest word pairs (you can use cosine-similarity to measure two words). Explain your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81f7d38-0bf0-4e55-aedc-b0f8f24195f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af756bde-2960-4c56-b037-0bfd2cf7e47e",
   "metadata": {},
   "source": [
    "### Task 3 Present a document as an embedding\n",
    "\n",
    "> For each document, you have several choices to generate document embedding: 1. Use the average of embeddings of all words in each document; 2. Use the first paragraphâ€™s words and take an average on these embeddings; 3. Use the doc2vec algorithm to present each document. Do the above for both training and testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279547d8-e4e7-4bfa-9be9-6d535b86bbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091195ab-834c-4184-a8f2-490cd26b7ff4",
   "metadata": {},
   "source": [
    "### Task 4 Build classifier to test docs\n",
    "> Build softmax regression model to classifier testing documents based on these training doc embeddings. Does it getting better than Naive Bayes'? (You have 3 models.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c43b8e-603f-4352-982c-f90d85e9f790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8206de2a-6d96-4016-b74e-4a08da2e1cab",
   "metadata": {},
   "source": [
    "### Task 5 Use t-SNE to project doc vectors\n",
    "\n",
    "> Use t-SNE to project training document embeddings into 2d and plot them out for each of the above choices. Each point should have a specific color (represent a particular cluster). You may need to try different parameters of t-SNE. One can find more details about t-SNE in this [excellent article](https://distill.pub/2016/misread-tsne/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907fe1b9-0650-438c-ad75-49e6632bf6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
