{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36836ad4-35bb-4d04-9118-170b1a3f37d1",
   "metadata": {},
   "source": [
    "### Task 0 Before your go\n",
    "\n",
    "> 1. Rename Assignment-02-###.ipynb where ### is your student ID.\n",
    "> 2. The deadline of Assignment-02 is 23:59pm, 04-21-2024\n",
    "> 3. In this assignment, you will use word embeddings to explore our Wikipedia dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caec95d8-a23b-4116-8d00-197a049cfd4e",
   "metadata": {},
   "source": [
    "### Task 1 Train word embeddings using SGNS \n",
    "> Use our enwiki-train.json as training data. You can use the [Gensim tool](https://radimrehurek.com/gensim/models/word2vec.html). But it is recommended to implement by yourself. You should explain how hyper-parameters such as dimensionality of embeddings, window size, the parameter of negative sampling strategy, and initial learning rate have been chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "baf0db53-3382-4534-b93e-e2a03796d525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import some necessary libraries\n",
    "from typing import List, Dict, Callable\n",
    "from collections import defaultdict\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim import utils\n",
    "import json\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3b61425c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the train and test data from the json file\n",
    "\n",
    "# NOTE: The function is inherited from my solution of assignment 1\n",
    "def load_json(file_path: str) -> List:\n",
    "    \"\"\"\n",
    "    Fetch the data from `.json` file and concat them into a list.\n",
    "\n",
    "    Input:\n",
    "    - file_path: The relative file path of the `.json` file\n",
    "\n",
    "    Returns:\n",
    "    - join_data_list: A list containing the data, with the format of [{'title':<>, 'label':<>, 'text':<>}, {}, ...]\n",
    "    \"\"\"\n",
    "    join_data_list = []\n",
    "    with open(file_path, \"r\") as json_file:\n",
    "        for line in json_file:\n",
    "            line = line.strip()\n",
    "            # guaranteen the line is not empty\n",
    "            if line: \n",
    "                join_data_list.append(json.loads(line))\n",
    "    return join_data_list\n",
    "\n",
    "train_file_path, test_file_path = \"enwiki-train.json\", \"enwiki-test.json\"\n",
    "train_data_list, test_data_list = map(load_json, [train_file_path, test_file_path])\n",
    "\n",
    "class Corpus:\n",
    "    def __iter__(self):\n",
    "        for line in train_data_list:\n",
    "            yield utils.simple_preprocess(line[\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "876121ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyWord2Vec:\n",
    "    def __init__(self, text: List[dict], dimensionality: int=100, window_size: int=5, negative_samples: int=5, lr: float=0.001) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - text: The training data\n",
    "        - dimensionality: The dimension of the word embeddings\n",
    "        - window_size: The size of the context window\n",
    "        - negative_samples: The number of negative samples\n",
    "        - lr: Learning rate of the algorithm\n",
    "        \"\"\"\n",
    "        self.dim = dimensionality\n",
    "        self.window = window_size\n",
    "        self.neg = negative_samples\n",
    "        self.lr = lr\n",
    "        self.__vocab = set()\n",
    "        self.__word_frq = defaultdict(int)\n",
    "        self.__word2idx = {}\n",
    "        self.__idx2word = {}\n",
    "        self.__embedding = None\n",
    "        self.__context_words = []\n",
    "        self.__context_targets = []\n",
    "        self.__build(text)\n",
    "        \n",
    "\n",
    "    def __preprocess(self, text: List[str]) -> None:\n",
    "        \"\"\"\n",
    "        Calculate the vocabulary and the frequency of each word in the training data, while maintaining the (idx, word) map.\n",
    "        \"\"\"\n",
    "        for sample in text:\n",
    "            words = sample[\"text\"].split()\n",
    "            for word in words:\n",
    "                self.__vocab.add(word)\n",
    "                self.__word_frq[word] += 1\n",
    "        for idx, word in enumerate(self.__vocab):\n",
    "            self.__word2idx[word] = idx\n",
    "            self.__idx2word[idx] = word\n",
    "\n",
    "    def __generate_training_data(self, text: List[str]) -> None:\n",
    "        \"\"\"\n",
    "        Generate training data from each window and save them in `self.__context_words` and `self.__context_targets`\n",
    "        \"\"\"\n",
    "        for sample in text:\n",
    "            words = sample[\"text\"].split()\n",
    "            for i, curr_word in enumerate(words):\n",
    "                # the \"window\" around the current world\n",
    "                for j in range(max(0, i - self.window), min(i + self.window + 1, len(words))):\n",
    "                    if i != j:\n",
    "                        self.__context_words.append(self.__word2idx[curr_word])\n",
    "                        self.__context_targets.append(self.__word2idx[words[j]])\n",
    "\n",
    "    def __initialize_embedding(self) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the embedding matrix with random values\n",
    "        \"\"\"\n",
    "        self.__embedding = np.random.uniform(-0.5 / self.dim, 0.5 / self.dim, size=(len(self.__vocab), self.dim))\n",
    "    \n",
    "    def __build(self, text: List[map]) -> None:\n",
    "        \"\"\"\n",
    "        Compute and store the relevant information of the training data in the class\n",
    "        \"\"\"\n",
    "        self.__preprocess(text)\n",
    "        self.__generate_training_data(text)\n",
    "        self.__initialize_embedding()\n",
    "\n",
    "    def train(self, epochs: int=5) -> None: \n",
    "        for epoch in range(epochs):\n",
    "            # learning rate decay\n",
    "            learning_rate = self.lr * (1 - epoch / epochs)\n",
    "\n",
    "            print(\"Training Epoch: %d\" % (epoch + 1))\n",
    "\n",
    "            for context_word, target_word in zip(self.__context_words, self.__context_targets):\n",
    "                context_vector = self.__embedding[context_word]\n",
    "                target_vector = self.__embedding[target_word]\n",
    "\n",
    "                # positive sample update\n",
    "                score = np.dot(target_vector, context_vector)\n",
    "                exp_score = math.exp(score)\n",
    "                grad_context = (exp_score / (1 + exp_score) - 1) * target_vector\n",
    "                grad_target = (exp_score / (1 + exp_score) - 1) * context_vector\n",
    "                self.__embedding[context_word] -= learning_rate * grad_context\n",
    "                self.__embedding[target_word] -= learning_rate * grad_target\n",
    "\n",
    "                # negative sample update\n",
    "                for _ in range(self.neg):\n",
    "                    negative_word = random.randint(0, len(self.__vocab) - 1)\n",
    "                    if negative_word != target_word:\n",
    "                        negative_vector = self.__embedding[negative_word]\n",
    "                        score = np.dot(negative_vector, context_vector)\n",
    "                        exp_score = math.exp(score)\n",
    "                        grad_context = exp_score / (1 + exp_score) * negative_vector\n",
    "                        grad_target = exp_score / (1 + exp_score) * context_vector\n",
    "                        self.__embedding[context_word] -= learning_rate * grad_context\n",
    "                        self.__embedding[target_word] -= learning_rate * grad_target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "286cfc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = Corpus()\n",
    "model = Word2Vec(\n",
    "    sentences=sentence, vector_size=100, alpha=0.025, window=5, min_count=5, sample=0.001, \n",
    "    seed=1, workers=3, min_alpha=0.0001, sg=1, negative=5, ns_exponent=0.75, epochs=5, \n",
    "    sorted_vocab=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeadc4b2-2c39-46f1-a9b9-28089393c24f",
   "metadata": {},
   "source": [
    "### Task 2 Find similar/dissimilar word pairs\n",
    "\n",
    "> Randomly generate 100, 1000, and 10000-word pairs from the vocabularies. For each set, print 5 closest word pairs and 5 furthest word pairs (you can use cosine-similarity to measure two words). Explain your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d81f7d38-0bf0-4e55-aedc-b0f8f24195f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For   100 random pairs from the vocabularies:\n",
      "The 5  closest word pairs:\n",
      "Word pairs: (      herodotus,       sorcerers) --> Similarity: 0.886167\n",
      "Word pairs: (           sown,     prohibitive) --> Similarity: 0.854565\n",
      "Word pairs: (        bruxing,        plumbers) --> Similarity: 0.850914\n",
      "Word pairs: (stereotypically,             een) --> Similarity: 0.850355\n",
      "Word pairs: (         shaded,           booby) --> Similarity: 0.811930\n",
      "The 5 furthest word pairs:\n",
      "Word pairs: (     violations,         yevgeny) --> Similarity: 0.238296\n",
      "Word pairs: (     preferring,  neolissochilus) --> Similarity: 0.230178\n",
      "Word pairs: (       superman,         outputs) --> Similarity: 0.230071\n",
      "Word pairs: (     separately,       communism) --> Similarity: 0.178022\n",
      "Word pairs: (        trapped,          arabia) --> Similarity: 0.151655\n",
      "----------------------------------------------------------------------\n",
      "For  1000 random pairs from the vocabularies:\n",
      "The 5  closest word pairs:\n",
      "Word pairs: (        serrano,    englishwoman) --> Similarity: 0.905918\n",
      "Word pairs: (           chua,        lionized) --> Similarity: 0.893601\n",
      "Word pairs: (        itching,         dryness) --> Similarity: 0.890620\n",
      "Word pairs: (          knobs,      seborrheic) --> Similarity: 0.870778\n",
      "Word pairs: (          glaze,           tripe) --> Similarity: 0.866924\n",
      "The 5 furthest word pairs:\n",
      "Word pairs: (         lucien,     regulations) --> Similarity: 0.038586\n",
      "Word pairs: (   astronomical,          feared) --> Similarity: 0.036978\n",
      "Word pairs: (        exceeds,            karl) --> Similarity: 0.028098\n",
      "Word pairs: (           duff,           saccà) --> Similarity: 0.027427\n",
      "Word pairs: (        teenage,           pages) --> Similarity: -0.028433\n",
      "----------------------------------------------------------------------\n",
      "For 10000 random pairs from the vocabularies:\n",
      "The 5  closest word pairs:\n",
      "Word pairs: (          chaat,          capers) --> Similarity: 0.949915\n",
      "Word pairs: (        crumbly,          yakhni) --> Similarity: 0.933560\n",
      "Word pairs: (    autocorrect,           addon) --> Similarity: 0.931763\n",
      "Word pairs: (        outflow,          efflux) --> Similarity: 0.921996\n",
      "Word pairs: (         aulich,              ky) --> Similarity: 0.912328\n",
      "The 5 furthest word pairs:\n",
      "Word pairs: (  vulnerability,         honneur) --> Similarity: -0.036571\n",
      "Word pairs: (    lymphocytes,          sunday) --> Similarity: -0.038521\n",
      "Word pairs: (   accompanying,        portillo) --> Similarity: -0.048933\n",
      "Word pairs: (      longridge,      projectile) --> Similarity: -0.093817\n",
      "Word pairs: (          heavy,        matsuoka) --> Similarity: -0.103985\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def generate_random_paris(samples: int):\n",
    "    \"\"\"\n",
    "    Generate random indices without replacement, then pairs the indices to get word pairs\n",
    "    \"\"\"\n",
    "    indices = random.sample(range(len(model.wv)), 2 * samples)\n",
    "    indices1, indices2 = indices[:samples], indices[samples:]\n",
    "    return [model.wv.index_to_key[i] for i in indices1], [model.wv.index_to_key[i] for i in indices2]\n",
    "\n",
    "def find_closest_furthest(num: int=5, words1: List[str]=None, words2: List[str]=None) -> None:\n",
    "    \"\"\"\n",
    "    Find the cloest/furthest word pairs using `model.wv.similarity`.\n",
    "\n",
    "    Here a heap queue is used to reduce time complexity to $O(n\\log k)$, where k denotes the `num`\n",
    "    \"\"\"\n",
    "    heap = []\n",
    "    for i in range(len(words1)):\n",
    "        # compute the similarity and push it into the heap\n",
    "        heapq.heappush(heap, (model.wv.similarity(words1[i], words2[i]), words1[i], words2[i]))\n",
    "    return heapq.nlargest(num, heap), heapq.nsmallest(num, heap)[::-1]\n",
    "\n",
    "def print_word_pairs(results: List[tuple], flag: str) -> None:\n",
    "    \"\"\"\n",
    "    Print the result in formatted string\n",
    "    \"\"\"\n",
    "    print(\"The 5 {:>8} word pairs:\".format(flag))\n",
    "    for result in results:\n",
    "       print(\"Word pairs: ({:>15}, {:>15}) --> Similarity: {:>8.6f}\".format(result[1], result[2], result[0]))\n",
    "\n",
    "\n",
    "random.seed(408)\n",
    "pairs = [100, 1000, 10000]\n",
    "for pair in pairs:\n",
    "    print(\"For {:>5} random pairs from the vocabularies:\".format(pair))\n",
    "    cloest, furthest = find_closest_furthest(5, *generate_random_paris(pair))\n",
    "    print_word_pairs(cloest, \"closest\")\n",
    "    print_word_pairs(furthest, \"furthest\")\n",
    "    print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af756bde-2960-4c56-b037-0bfd2cf7e47e",
   "metadata": {},
   "source": [
    "### Task 3 Present a document as an embedding\n",
    "\n",
    "> For each document, you have several choices to generate document embedding: 1. Use the average of embeddings of all words in each document; 2. Use the first paragraph’s words and take an average on these embeddings; 3. Use the doc2vec algorithm to present each document. Do the above for both training and testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "279547d8-e4e7-4bfa-9be9-6d535b86bbc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the training dataset:\n",
      "Title: Citizen_Kane\n",
      "Embedding: [-2.50620186e-01 -1.24460168e-01  1.07301518e-01  1.06681667e-01\n",
      " -2.21395597e-01 -3.52198243e-01  2.12455407e-01  3.53895932e-01\n",
      " -2.15869695e-01 -3.49741787e-01 -5.18604964e-02 -3.69868010e-01\n",
      "  3.14556248e-02  1.32968366e-01  3.22128274e-02 -1.76448002e-01\n",
      "  9.24646705e-02  8.66957288e-03 -8.88008401e-02 -3.57388258e-01\n",
      "  3.24919894e-02  2.21586511e-01  1.33043960e-01  1.34189054e-01\n",
      "  3.74840684e-02  1.87552407e-01 -2.69486457e-01  1.18333451e-01\n",
      " -1.14250392e-01 -1.12703137e-01 -2.77006570e-02  1.69034123e-01\n",
      " -9.03501064e-02 -4.79165353e-02  1.85195042e-03  2.02847227e-01\n",
      " -8.63823816e-02 -1.64019570e-01 -4.84447666e-02 -2.38638595e-01\n",
      " -2.54364070e-02 -2.61652976e-01 -3.04982334e-01 -1.23209968e-01\n",
      "  3.49789172e-01 -1.77557100e-04 -1.61893055e-01 -2.29572989e-02\n",
      "  7.82075897e-02 -7.45770931e-02  8.86614993e-02  2.33614668e-02\n",
      " -1.36879414e-01 -1.10366493e-01 -7.30106160e-02  1.19938485e-01\n",
      " -1.05985805e-01  8.51468965e-02  4.01250720e-02  1.30574077e-01\n",
      "  7.31197968e-02  1.28074691e-01  1.61692419e-03 -3.05825248e-02\n",
      "  4.58277352e-02  1.73044160e-01 -1.48848146e-01  3.53718877e-01\n",
      " -2.52270073e-01  2.37881333e-01  2.23707333e-01  3.07884812e-01\n",
      "  5.72596602e-02  4.83580492e-02  1.40797958e-01  2.66747087e-01\n",
      " -2.58719977e-02  9.42574814e-02  4.65423539e-02 -3.13752182e-02\n",
      " -9.04982463e-02 -1.46478936e-01  6.23904206e-02  1.02201246e-01\n",
      "  6.54438362e-02  6.96541071e-02 -1.46653755e-02  1.83539223e-02\n",
      "  8.44090134e-02  1.81269914e-01  8.92863274e-02  3.75561975e-02\n",
      "  1.75370544e-01 -3.05094570e-01  7.39397034e-02  4.51905504e-02\n",
      " -3.50340852e-03 -1.96438849e-01 -7.95993879e-02 -6.28853366e-02]\n",
      "----------------------------------------------------------------------\n",
      "Title: It_(2017_film)\n",
      "Embedding: [-0.24488217 -0.14957182  0.10772355  0.10846584 -0.24100474 -0.28769928\n",
      "  0.15984868  0.35164633 -0.22720915 -0.34741142 -0.06238467 -0.400932\n",
      "  0.02767061  0.160857    0.03252565 -0.20964128  0.09204626  0.01644065\n",
      " -0.07097974 -0.36858937  0.04102909  0.19199277  0.12186282  0.14965798\n",
      "  0.03488598  0.18605854 -0.26905784  0.10239656 -0.11303696 -0.12819478\n",
      " -0.01651674  0.17326924 -0.04339592 -0.06479227 -0.03096296  0.20127152\n",
      " -0.09499256 -0.1923069  -0.03705379 -0.26255032 -0.00041707 -0.26174524\n",
      " -0.27088854 -0.15038152  0.3701973   0.02276402 -0.14338358 -0.03296463\n",
      "  0.12553239 -0.05907996  0.06040789 -0.00934097 -0.13442679 -0.13523053\n",
      " -0.06988022  0.1365552  -0.09571983  0.08912787  0.03813821  0.15655115\n",
      "  0.06767659  0.14847338  0.02718132 -0.02701472  0.04880348  0.18568407\n",
      " -0.11675454  0.35654163 -0.24904513  0.26040855  0.21664655  0.3223226\n",
      "  0.06846297  0.03945282  0.18753722  0.26518866 -0.00067151  0.09921504\n",
      "  0.00623818 -0.03357329 -0.08310021 -0.10795262  0.04677015  0.14065021\n",
      "  0.05002255  0.04835185 -0.02104823 -0.01401299  0.09930477  0.20113514\n",
      "  0.07671671  0.0777377   0.14150704 -0.30546728  0.06060779  0.05532911\n",
      "  0.00054054 -0.18710797 -0.04479293 -0.05496506]\n",
      "----------------------------------------------------------------------\n",
      "Title: Star_Wars_(film)\n",
      "Embedding: [-0.29629478 -0.10353303  0.10967188  0.11527455 -0.18964735 -0.31567353\n",
      "  0.18129216  0.38453916 -0.23495828 -0.3840241  -0.07874676 -0.3840457\n",
      "  0.03935242  0.12747394  0.03203851 -0.22164801  0.11599029  0.00312872\n",
      " -0.07644044 -0.35057244  0.06403739  0.2077101   0.13548791  0.13812532\n",
      "  0.04774379  0.19502713 -0.2311912   0.10256311 -0.09734133 -0.12529807\n",
      " -0.05052993  0.16634534 -0.09343065 -0.03780145  0.01455524  0.21443154\n",
      " -0.09078431 -0.17301275 -0.04256228 -0.2396657  -0.00230465 -0.23128873\n",
      " -0.31713235 -0.12574247  0.3413232   0.00848199 -0.1565521  -0.0175163\n",
      "  0.10539526 -0.05997576  0.09707244 -0.00917523 -0.11268347 -0.0917214\n",
      " -0.05781149  0.11771287 -0.10053962  0.06293344  0.02866244  0.15250635\n",
      "  0.06734153  0.14899556  0.01719511 -0.01968183  0.02357819  0.16561995\n",
      " -0.14349176  0.35921592 -0.25195804  0.23948671  0.22693768  0.29166773\n",
      "  0.07194691  0.03716615  0.16090529  0.2366305  -0.0353337   0.09405118\n",
      "  0.01633828 -0.05546128 -0.08909149 -0.13016799  0.07384976  0.11562992\n",
      "  0.06281415  0.04048325 -0.0216449   0.00532491  0.0701567   0.17514229\n",
      "  0.08942176  0.04894411  0.16681947 -0.28205642  0.05079853  0.03250108\n",
      " -0.02876553 -0.20556562 -0.05527452 -0.06253268]\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "For the testing dataset:\n",
      "Title: Monty_Python's_Life_of_Brian\n",
      "Embedding: [-0.24520043 -0.19736952  0.12214972  0.09791048 -0.24552232 -0.29546508\n",
      "  0.19574825  0.38896683 -0.2218934  -0.36842543 -0.07254709 -0.34876353\n",
      "  0.01584628  0.18065158  0.02149899 -0.1915446   0.11597787  0.02101017\n",
      " -0.07267883 -0.37235522  0.05805107  0.1786446   0.14710207  0.17291468\n",
      "  0.02011308  0.19620383 -0.2436768   0.08358806 -0.11473269 -0.10067022\n",
      " -0.02120594  0.14678794 -0.04590682 -0.09271377 -0.00354019  0.19629808\n",
      " -0.06982736 -0.2079555  -0.0550481  -0.24685429  0.00636975 -0.27642512\n",
      " -0.25293276 -0.12491144  0.403026    0.01524521 -0.1488797  -0.01472915\n",
      "  0.10683564 -0.03036067  0.05913468 -0.00619659 -0.13753434 -0.12263241\n",
      " -0.07385048  0.142236   -0.11155904  0.0895727   0.04314721  0.139796\n",
      "  0.07177047  0.11778601  0.0189902  -0.00869207  0.04086816  0.1702747\n",
      " -0.11807182  0.36830366 -0.25764185  0.2629766   0.22242944  0.34429508\n",
      "  0.04211231  0.03714689  0.16659908  0.2622547  -0.01122016  0.1007498\n",
      "  0.02415737  0.00188249 -0.10147561 -0.12370478  0.06285101  0.13587704\n",
      "  0.07215933  0.03568869 -0.02078906 -0.00441359  0.1427739   0.19055028\n",
      "  0.05434983  0.03378057  0.16768055 -0.31482086  0.0710347   0.07787363\n",
      " -0.00816517 -0.20377296 -0.03846433 -0.05172408]\n",
      "----------------------------------------------------------------------\n",
      "Title: Superman_(1978_film)\n",
      "Embedding: [-2.64403343e-01 -1.03022657e-01  9.56152081e-02  1.25098586e-01\n",
      " -1.93937495e-01 -3.23246628e-01  1.77002594e-01  3.68500710e-01\n",
      " -2.28219017e-01 -3.72611105e-01 -7.33584911e-02 -3.86887163e-01\n",
      "  2.52907537e-02  1.23281591e-01  2.87577119e-02 -2.14504018e-01\n",
      "  9.07893181e-02 -1.56557420e-03 -8.07580799e-02 -3.55344862e-01\n",
      "  5.23822121e-02  2.03372568e-01  1.14459157e-01  1.34134308e-01\n",
      "  1.97038502e-02  1.83854565e-01 -2.51143068e-01  1.09441228e-01\n",
      " -1.03625916e-01 -1.30273432e-01 -3.73459160e-02  1.64834648e-01\n",
      " -7.34531209e-02 -3.95684503e-02 -2.76581733e-03  1.97965682e-01\n",
      " -9.52523202e-02 -1.74100712e-01 -4.37747799e-02 -2.45991468e-01\n",
      " -5.93644613e-03 -2.43869707e-01 -3.01629484e-01 -1.24146156e-01\n",
      "  3.43984991e-01  3.78704676e-03 -1.56223521e-01 -2.04483382e-02\n",
      "  1.18798748e-01 -4.43104692e-02  9.03835073e-02  4.92666895e-03\n",
      " -1.28938049e-01 -1.21163607e-01 -6.49530143e-02  1.23665586e-01\n",
      " -8.08680207e-02  7.40665495e-02  3.17177027e-02  1.55285120e-01\n",
      "  7.37348422e-02  1.52456403e-01  2.45672297e-02 -2.51954608e-02\n",
      "  2.72500440e-02  1.77120417e-01 -1.29949510e-01  3.53860021e-01\n",
      " -2.38530532e-01  2.39864454e-01  2.32388288e-01  3.04493695e-01\n",
      "  6.19263165e-02  3.24226432e-02  1.63714230e-01  2.59212196e-01\n",
      " -1.29982261e-02  9.66473371e-02  1.78797971e-02 -5.55607304e-02\n",
      " -9.40369964e-02 -1.18811876e-01  7.06345290e-02  1.21465430e-01\n",
      "  6.72392398e-02  5.04671931e-02 -1.84937790e-02 -6.15264580e-05\n",
      "  9.26451460e-02  1.73555434e-01  9.12383944e-02  6.39553592e-02\n",
      "  1.60804391e-01 -2.88992852e-01  5.83970807e-02  4.71384861e-02\n",
      " -2.59393360e-02 -1.99841768e-01 -6.18111417e-02 -5.73476702e-02]\n",
      "----------------------------------------------------------------------\n",
      "Title: Boys_Don't_Cry_(1999_film)\n",
      "Embedding: [-0.21852072 -0.19765694  0.12652454  0.09595376 -0.23702918 -0.30869743\n",
      "  0.2094519   0.36969435 -0.23711422 -0.36549476 -0.08052707 -0.36804852\n",
      "  0.01391289  0.15485603  0.03375941 -0.18174541  0.12056762  0.01729872\n",
      " -0.05490885 -0.3482098   0.03432876  0.21573474  0.12853968  0.13136265\n",
      "  0.03104366  0.16982041 -0.23156375  0.10670111 -0.13148719 -0.1390707\n",
      " -0.00424646  0.15982679 -0.05440209 -0.0770073   0.00350015  0.1950703\n",
      " -0.0883608  -0.17672655 -0.0485613  -0.26478454  0.00189011 -0.28185785\n",
      " -0.27405596 -0.13134494  0.39768207  0.00520402 -0.14630298  0.01142405\n",
      "  0.10646513 -0.04386384  0.06118486 -0.00359831 -0.12471887 -0.11731367\n",
      " -0.07516231  0.1373277  -0.14252368  0.05388051  0.04694361  0.128038\n",
      "  0.08312859  0.13082896  0.01199426 -0.00620656  0.02700054  0.17872907\n",
      " -0.13173792  0.3602941  -0.26693884  0.25043735  0.2215921   0.3409923\n",
      "  0.08689603  0.03689761  0.16093363  0.25673702  0.01025784  0.08738416\n",
      "  0.02599301 -0.01543003 -0.10602226 -0.11659151  0.03910267  0.12048167\n",
      "  0.04703739  0.00371554 -0.04979695 -0.01245553  0.12725368  0.21335992\n",
      "  0.06122717  0.05117129  0.15248026 -0.29576147  0.07731586  0.0675649\n",
      " -0.00604962 -0.18758373 -0.04036024 -0.06743155]\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "####################################################################################\n",
    "###        1. Use the average of embeddings of all words in each document        ###\n",
    "####################################################################################\n",
    "def print_document_embeddings(embeddings: Dict, display: int=3) -> None:\n",
    "    \"\"\"\n",
    "    Print the first `display` embeddings, default value is 10\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    for title, embedding in embeddings.items():\n",
    "        if count >= display:\n",
    "            break\n",
    "        count += 1\n",
    "        print(\"Title: {}\\nEmbedding: {}\".format(title, embedding))\n",
    "        print(\"-\" * 70)\n",
    "\n",
    "def average_all_words(data_list: List[Dict]=train_data_list, mode: str=\"title\") -> Dict:\n",
    "    doc_embeddings = {}\n",
    "    if mode not in [\"title\", \"label\"]:\n",
    "        raise ValueError(\"Please input a valid mode: ['title', 'label']\")\n",
    "    for line in data_list:\n",
    "        line_text = utils.simple_preprocess(line[\"text\"]) # preprocess the text as in class `Corpus`\n",
    "        # doc title/label and doc embedding computed by averaging all words embeddings\n",
    "        doc_info = line[mode]\n",
    "        valid_word_embeddings = [model.wv[word] for word in line_text if word in model.wv]\n",
    "        doc_embedding = sum(valid_word_embeddings) / len(valid_word_embeddings)\n",
    "        # store the information in a dictionary\n",
    "        doc_embeddings[doc_info] = doc_embedding\n",
    "\n",
    "    return doc_embeddings\n",
    "\n",
    "print(\"For the training dataset:\")\n",
    "print_document_embeddings(average_all_words())\n",
    "print(\"-\" * 70)\n",
    "print(\"For the  testing dataset:\")\n",
    "print_document_embeddings(average_all_words(test_data_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1ae2cadc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Citizen_Kane:\n",
      "[-2.55847305e-01 -9.10092816e-02  2.11859465e-01  9.32272747e-02\n",
      " -1.63740218e-01 -4.45404261e-01  2.55138785e-01  3.16293001e-01\n",
      " -2.67182022e-01 -3.24095517e-01 -1.37831882e-01 -4.00891244e-01\n",
      "  2.40017889e-05  4.15285788e-02  2.55168062e-02 -2.56034017e-01\n",
      "  1.12494066e-01 -8.41944069e-02 -8.90707895e-02 -3.30588758e-01\n",
      " -4.00519930e-02  3.16697747e-01  1.17731757e-01  4.68406305e-02\n",
      "  5.03130369e-02  1.96792975e-01 -2.86228806e-01  1.43448085e-01\n",
      " -6.05412014e-02 -8.46401453e-02  2.22788304e-02  2.00624779e-01\n",
      " -1.87066823e-01 -3.05731241e-02  1.93206612e-02  2.33878955e-01\n",
      " -1.47629499e-01 -1.38299137e-01 -5.80723062e-02 -2.44014874e-01\n",
      "  1.52438255e-02 -3.16461951e-01 -3.14271510e-01 -9.72499847e-02\n",
      "  3.41878742e-01  3.40494956e-03 -1.16632216e-01  3.97241004e-02\n",
      " -1.17706200e-02 -9.07134637e-02  1.66977808e-01  8.40164945e-02\n",
      " -1.14028901e-01 -7.27838129e-02 -6.90624714e-02  3.25847454e-02\n",
      " -9.35799405e-02  9.07470286e-02  5.53231388e-02  1.39155984e-01\n",
      "  1.45286709e-01  1.87500656e-01 -1.25986021e-02 -8.13336596e-02\n",
      "  4.40879650e-02  1.12992764e-01 -1.72919065e-01  4.07399893e-01\n",
      " -2.95120120e-01  2.78510928e-01  2.24579796e-01  2.10193098e-01\n",
      "  1.13179870e-01  4.70833965e-02  1.22866929e-01  2.39691034e-01\n",
      " -6.27028421e-02  7.40090907e-02  8.25824589e-02  1.68325193e-02\n",
      " -5.90919890e-02 -1.41781449e-01  4.81778905e-02  4.81204800e-02\n",
      "  1.03498839e-01  6.66963905e-02 -3.79356518e-02 -3.62837091e-02\n",
      "  1.02167629e-01  1.77541733e-01  1.07066318e-01 -3.74067724e-02\n",
      "  1.49975777e-01 -3.13185573e-01  2.84803994e-02 -5.77490814e-02\n",
      " -2.01857127e-02 -2.35966414e-01 -1.21093422e-01 -1.23869628e-01]\n",
      "----------------------------------------------------------------------\n",
      "It_(2017_film):\n",
      "[-0.33313584 -0.15521866  0.10200609  0.11667838 -0.20584412 -0.33820048\n",
      "  0.2569346   0.35711548 -0.20139113 -0.38024956 -0.06615292 -0.39801243\n",
      "  0.00149141  0.12127869  0.04585932 -0.18184584  0.13077188 -0.00244536\n",
      " -0.06552175 -0.34113812  0.04769408  0.23258024  0.17307746  0.13966726\n",
      "  0.04457501  0.22542748 -0.2857309   0.17140621 -0.1189306  -0.12847361\n",
      " -0.0012276   0.14838657 -0.12579188 -0.11085933 -0.0025588   0.20780045\n",
      " -0.11604435 -0.11931804 -0.06679532 -0.24443236  0.07857886 -0.29012677\n",
      " -0.27613172 -0.06354048  0.37573946  0.02928172 -0.15427825 -0.01295416\n",
      "  0.09534096 -0.02140916  0.02898037 -0.0268286  -0.08502097 -0.14841214\n",
      " -0.03476233  0.1299692  -0.03132332  0.05977914  0.0327932   0.11016367\n",
      "  0.06175554  0.20008303  0.00120508 -0.01342054  0.04875164  0.14265448\n",
      " -0.09138454  0.42241108 -0.26878753  0.24989419  0.18548276  0.2518992\n",
      "  0.09721799  0.03304676  0.13491693  0.23230861  0.03742503  0.11825707\n",
      " -0.00875072  0.02854079 -0.06841066 -0.04374767  0.09069071  0.13475661\n",
      "  0.09170885  0.02896952 -0.11022661 -0.08550367  0.15333252  0.186871\n",
      "  0.06758063 -0.04852235  0.1682076  -0.29896486  0.04317721  0.04112705\n",
      " -0.04377408 -0.22588879 -0.05163534 -0.1156179 ]\n",
      "----------------------------------------------------------------------\n",
      "Star_Wars_(film):\n",
      "[-0.32482284  0.02102414  0.20784417  0.12725186  0.02727031 -0.43290457\n",
      "  0.3045499   0.38227683 -0.21219926 -0.33773792 -0.18913226 -0.5090853\n",
      "  0.06904376 -0.11053517  0.04946258 -0.22612293  0.13434441 -0.08950502\n",
      " -0.13775681 -0.28462383  0.09704888  0.29488257  0.04554145 -0.01835115\n",
      "  0.03313146  0.24587677 -0.23627196  0.05550746 -0.15552142 -0.0701921\n",
      " -0.02532793  0.11399946 -0.2089996   0.14140642 -0.00513493  0.22317693\n",
      " -0.13355385 -0.05238649 -0.01338822 -0.20046993 -0.00329797 -0.20985064\n",
      " -0.31938073 -0.07756949  0.17935604  0.01501971 -0.08236276  0.06379449\n",
      "  0.04088787  0.09171669  0.21790698  0.09525158 -0.04337204 -0.06263091\n",
      " -0.03506758  0.11699505 -0.05411792 -0.0325374  -0.00872359  0.15425983\n",
      " -0.04092145  0.2643075   0.0129586  -0.13299002 -0.05154236  0.2174666\n",
      " -0.2298701   0.2856469  -0.23069565  0.19494289  0.19334152  0.12772238\n",
      "  0.09899104  0.05712695  0.05892051  0.1860655   0.04659598  0.09265408\n",
      "  0.09781326 -0.16724505 -0.07826121 -0.04209495  0.0584335   0.065287\n",
      "  0.02010962  0.09181128 -0.04005593 -0.09286655  0.05227313  0.0109455\n",
      "  0.13750558  0.0081895   0.2797687  -0.12640527  0.00444199  0.00938574\n",
      " -0.02173689 -0.20144369 -0.00366692 -0.18900676]\n",
      "----------------------------------------------------------------------\n",
      "Star_Trek:_The_Motion_Picture:\n",
      "[-3.52407068e-01 -4.91917953e-02  1.08078279e-01  1.45706385e-01\n",
      " -9.76150185e-02 -3.31219703e-01  2.33881384e-01  3.28080416e-01\n",
      " -2.67617017e-01 -4.15300310e-01 -1.27966300e-01 -4.00954872e-01\n",
      "  4.84035760e-02  1.16105273e-01  5.00224903e-02 -2.28767782e-01\n",
      "  1.77471086e-01 -4.95652808e-03 -5.43379523e-02 -3.60152930e-01\n",
      "  3.58727723e-02  2.57845700e-01  1.75401330e-01  1.51703864e-01\n",
      "  1.44534726e-02  1.93606049e-01 -2.64966369e-01  1.39833674e-01\n",
      " -9.01804268e-02 -1.11269571e-01 -8.97999331e-02  2.01123446e-01\n",
      " -9.81983617e-02  2.80975346e-02  3.09058670e-02  2.63322353e-01\n",
      " -1.88434377e-01 -1.25445381e-01 -7.23439902e-02 -2.04836398e-01\n",
      "  2.41062865e-02 -2.24292129e-01 -3.20155561e-01 -1.09997988e-01\n",
      "  3.83151561e-01  2.74513140e-02 -9.71921086e-02  4.44918638e-03\n",
      "  5.62706143e-02 -3.70981777e-03  8.33196342e-02  2.68967431e-02\n",
      " -6.10510930e-02 -7.87458718e-02 -3.35159600e-02  1.12667665e-01\n",
      " -1.10342033e-01  1.99246537e-02  5.96930459e-02  2.23136112e-01\n",
      "  6.39308542e-02  1.09455347e-01 -2.14376785e-02  1.55240437e-02\n",
      " -3.87229782e-04  2.06248686e-01 -1.67770192e-01  3.68068218e-01\n",
      " -2.49866039e-01  2.74598062e-01  1.54865712e-01  2.26969317e-01\n",
      "  9.71433222e-02  4.62324694e-02  1.30030051e-01  2.00879022e-01\n",
      " -2.59607937e-02  1.11206062e-01  1.74432714e-02 -9.93584022e-02\n",
      " -1.27803758e-01 -8.54296610e-02  4.90969531e-02  1.47253916e-01\n",
      "  3.50758620e-02 -2.79503688e-03 -4.22280990e-02 -7.24681988e-02\n",
      "  7.05542788e-02  1.34353563e-01  9.35140625e-02 -2.88296491e-02\n",
      "  2.02212319e-01 -1.88140988e-01  6.23840606e-03 -1.30846296e-02\n",
      " -9.64111909e-02 -2.08531514e-01 -4.36863862e-02 -4.45128754e-02]\n",
      "----------------------------------------------------------------------\n",
      "Frozen_(2013_film):\n",
      "[-0.36159378 -0.02128462  0.15891066  0.17307293 -0.09119671 -0.40050384\n",
      "  0.26745173  0.37895322 -0.23376283 -0.41326922 -0.1613442  -0.5208194\n",
      "  0.05092986 -0.02868378  0.01680033 -0.14987226  0.14373353  0.02874078\n",
      " -0.18781713 -0.29933184  0.07402957  0.2530415   0.16327693  0.04543809\n",
      " -0.00819386  0.19147037 -0.18116808  0.17966542 -0.03681418 -0.19779897\n",
      " -0.08554317  0.15202238 -0.1355721  -0.02109314  0.01841842  0.1662072\n",
      " -0.13272771 -0.06635266 -0.00992909 -0.25895056  0.08193677 -0.20319612\n",
      " -0.28121144 -0.08302815  0.28986248  0.06024585 -0.22293921  0.01039273\n",
      "  0.1381269   0.02242294  0.12944143 -0.06426962 -0.05359993 -0.12097646\n",
      " -0.01696751  0.04078567 -0.03243402 -0.05960737 -0.09453239  0.13280784\n",
      "  0.05630736  0.20081034  0.06582176 -0.06826321  0.00345136  0.19598025\n",
      " -0.20071232  0.44486374 -0.18000147  0.13629182  0.19379027  0.1616796\n",
      "  0.07779452  0.05730315  0.08847734  0.17524822 -0.02841449  0.15282722\n",
      "  0.09288194 -0.0654622  -0.07509606 -0.07271769  0.11994147  0.02105549\n",
      " -0.0562986   0.05204576 -0.0518734  -0.03683202  0.04223563  0.16266559\n",
      "  0.16020338  0.0266928   0.15546274 -0.19000049  0.03330111  0.01379873\n",
      "  0.02819425 -0.18236952 -0.02558587 -0.20203352]\n",
      "----------------------------------------------------------------------\n",
      "Black_Panther_(film):\n",
      "[-0.29892898 -0.08529729  0.17610042  0.15717138 -0.08432285 -0.42973801\n",
      "  0.25384974  0.28723082 -0.19812553 -0.35470372 -0.13059056 -0.51205456\n",
      "  0.03064353 -0.00765244  0.02648403 -0.1762276   0.14904656 -0.02756987\n",
      " -0.1595202  -0.3508789   0.10822587  0.25264394  0.09074821  0.01489506\n",
      "  0.03295605  0.17074002 -0.24526256  0.11558752 -0.12218663 -0.11403788\n",
      " -0.10166463  0.14365557 -0.15063134  0.03270244 -0.03863913  0.1070546\n",
      " -0.1476307  -0.12347554 -0.09043932 -0.24935329  0.03747018 -0.17234862\n",
      " -0.28296366 -0.11466323  0.31142095  0.0172567  -0.16002367  0.11499736\n",
      "  0.10869046  0.01381341  0.12623572  0.0368997  -0.04055014 -0.10605837\n",
      " -0.06428401  0.0817387  -0.05641112 -0.01671029 -0.0126969   0.12269332\n",
      "  0.01144442  0.14292927  0.07059567 -0.05676694 -0.04660654  0.2339513\n",
      " -0.17369491  0.35089052 -0.19120814  0.16607559  0.24250685  0.11129422\n",
      "  0.09572688  0.02772792  0.06762523  0.24456564  0.04270837  0.11853401\n",
      "  0.09033729 -0.09212377 -0.13688292 -0.06409495  0.01045327  0.11076953\n",
      " -0.01324057  0.04406696 -0.05169597 -0.11151263  0.06243157  0.08207491\n",
      "  0.07609054  0.08261076  0.2470853  -0.15649953  0.00396792 -0.0333521\n",
      "  0.01010073 -0.17006479 -0.06291092 -0.13052458]\n",
      "----------------------------------------------------------------------\n",
      "The_Cabinet_of_Dr._Caligari:\n",
      "[-0.2728739  -0.1297752   0.09561699  0.06055263 -0.14749683 -0.33953848\n",
      "  0.2661987   0.39294887 -0.2724304  -0.3198506  -0.20036525 -0.33177027\n",
      "  0.07133952  0.11518419 -0.00941755 -0.13438484  0.1575608   0.05461003\n",
      " -0.11344895 -0.35985646  0.01242223  0.25809553  0.13295422  0.08208101\n",
      " -0.03944273  0.19805612 -0.20466667  0.05971006 -0.1245843  -0.11508168\n",
      " -0.04034727  0.11672361 -0.09395938 -0.04420874  0.01824704  0.16287398\n",
      " -0.07432861 -0.20076238  0.01841492 -0.23918845 -0.02882151 -0.23966426\n",
      " -0.24875997 -0.1565411   0.32713535 -0.02177605 -0.13330777 -0.07019287\n",
      "  0.08907408 -0.0176368   0.06652891 -0.07728062 -0.08299375 -0.1862648\n",
      " -0.14088467  0.10100125 -0.12928589 -0.00473144  0.03290696  0.22112204\n",
      "  0.04437524  0.03258464  0.02256582 -0.0028593  -0.01712746  0.23434882\n",
      " -0.16808757  0.2980177  -0.22738013  0.19049284  0.20271821  0.2647916\n",
      "  0.13981335  0.00935769  0.12652792  0.26337013 -0.03005289  0.04867831\n",
      "  0.05012656  0.01360679 -0.2307223  -0.14448163  0.01467895  0.01755258\n",
      " -0.0362385   0.00147253 -0.03331564  0.0011735   0.10164499  0.15548857\n",
      "  0.08635757 -0.03482582  0.19881718 -0.28957838  0.12006808  0.03409444\n",
      " -0.02900712 -0.19426739  0.01478216 -0.08715963]\n",
      "----------------------------------------------------------------------\n",
      "The_Shining_(film):\n",
      "[-0.26759005 -0.18967237  0.20535801  0.128192   -0.13527015 -0.4605766\n",
      "  0.3359091   0.3220334  -0.23465836 -0.2947222  -0.21349205 -0.46906164\n",
      "  0.02517246 -0.00491639 -0.02766002 -0.12421212  0.17387462 -0.04305893\n",
      " -0.1500027  -0.28325355 -0.01017332  0.3167194   0.07503768  0.02313558\n",
      "  0.04498537  0.22658855 -0.29338226  0.08398804 -0.11113252 -0.1092322\n",
      "  0.01083335  0.15200369 -0.20507742 -0.04583644 -0.02175594  0.10423828\n",
      " -0.07923288 -0.12435397 -0.01270313 -0.3056584   0.02491975 -0.25317132\n",
      " -0.27577698 -0.08085077  0.29247582 -0.01839411 -0.09705058  0.05695116\n",
      "  0.01637354  0.08722714  0.1551802   0.04845184 -0.06875604 -0.10556974\n",
      " -0.05732514  0.10192763 -0.07776386  0.03921901  0.07424086  0.13582438\n",
      " -0.02829527  0.17395626 -0.02179305 -0.10367031 -0.06558167  0.24497207\n",
      " -0.22112292  0.3020305  -0.27610725  0.20445682  0.1652132   0.2021554\n",
      "  0.148899    0.0381734   0.13066603  0.2291635   0.06012179  0.09576455\n",
      "  0.09074914 -0.02384142 -0.16331492 -0.05203882 -0.03847135  0.04263848\n",
      " -0.00097886  0.03835471 -0.05583376 -0.13111149  0.10693281  0.14921829\n",
      "  0.05187251 -0.05892298  0.23210032 -0.20275381  0.07858121 -0.01680571\n",
      "  0.00651753 -0.1964754  -0.02266236 -0.14451933]\n",
      "----------------------------------------------------------------------\n",
      "Scream_(franchise):\n",
      "[-2.94272155e-01 -5.01878075e-02  1.78556859e-01  1.59722716e-01\n",
      " -1.64429739e-01 -3.58623147e-01  2.28615671e-01  3.37690383e-01\n",
      " -2.57253468e-01 -4.40753758e-01 -1.24301702e-01 -4.09739554e-01\n",
      "  6.90004379e-02  1.64709505e-04  6.51279762e-02 -2.21728936e-01\n",
      "  1.43267974e-01 -4.12062146e-02 -6.19230047e-02 -2.98094839e-01\n",
      "  2.51954757e-02  2.04942837e-01  5.91053180e-02  1.22629508e-01\n",
      "  1.31130487e-01  2.31305182e-01 -2.82823086e-01  1.07941076e-01\n",
      " -1.55102387e-01 -1.66772500e-01 -3.38816307e-02  1.64077491e-01\n",
      " -8.18343312e-02 -4.50498573e-02  2.40613651e-02  1.81971982e-01\n",
      " -1.70233771e-01 -1.49229750e-01 -2.59298105e-02 -2.30576962e-01\n",
      "  3.98517288e-02 -2.07194269e-01 -3.13296378e-01 -1.10057682e-01\n",
      "  3.45600367e-01 -3.06911469e-02 -8.69024172e-02  1.29205603e-02\n",
      "  8.45250785e-02 -1.29713891e-02  1.52111650e-01  5.30406311e-02\n",
      " -1.09611720e-01 -1.01352610e-01 -1.02797292e-01  1.22326814e-01\n",
      " -7.28128180e-02  1.71426535e-02  9.45768654e-02  1.86593756e-01\n",
      "  3.17253284e-02  2.52627611e-01  4.44859751e-02 -1.15960225e-01\n",
      " -2.74562296e-02  2.04335734e-01 -1.04945980e-01  3.84317249e-01\n",
      " -2.30010554e-01  2.45730743e-01  1.72610730e-01  2.50295043e-01\n",
      "  5.24199456e-02  2.77973190e-02  1.39983967e-01  1.71096399e-01\n",
      " -9.92242340e-03  5.40953018e-02 -2.58019799e-03 -1.03474734e-02\n",
      " -6.02254942e-02 -8.36082622e-02  8.33879635e-02  8.75819549e-02\n",
      "  1.59431800e-01  7.04380795e-02 -1.29087687e-01 -5.23105785e-02\n",
      "  1.03483051e-01  2.27325186e-01  9.85256955e-02  5.20246848e-02\n",
      "  1.71970859e-01 -3.05170149e-01 -1.32883452e-02  4.44834121e-02\n",
      " -4.52874489e-02 -2.57010341e-01 -2.18494404e-02 -5.18537909e-02]\n",
      "----------------------------------------------------------------------\n",
      "Batman_v_Superman:_Dawn_of_Justice:\n",
      "[-0.3309461  -0.07158426  0.12863068  0.18419968 -0.09905265 -0.40841994\n",
      "  0.23830697  0.2612337  -0.23421723 -0.34162605 -0.20438442 -0.47573894\n",
      "  0.11056558 -0.0394827   0.01130963 -0.20295519  0.12211667  0.01993949\n",
      " -0.12208009 -0.34470963  0.0319979   0.28081912  0.06735522  0.06888623\n",
      " -0.00187485  0.2055319  -0.27324417  0.12929831 -0.14277193 -0.19170755\n",
      " -0.04521147  0.16616829 -0.12294964  0.09242002 -0.06076897  0.20646855\n",
      " -0.18910567 -0.1195951  -0.04951963 -0.25774306  0.07408734 -0.17680243\n",
      " -0.2612669  -0.09130273  0.2763232  -0.00612637 -0.13469861  0.05850583\n",
      "  0.13042924  0.10172072  0.10986966  0.0605285  -0.0281906  -0.17576544\n",
      " -0.05433591  0.09246436 -0.05062723 -0.04199369  0.02889374  0.18661335\n",
      "  0.02826232  0.15796551  0.05560628 -0.12225747 -0.04266658  0.20971845\n",
      " -0.13383435  0.3390653  -0.22113392  0.22757214  0.21616024  0.22535335\n",
      "  0.09176984  0.01751825  0.12095363  0.21409655  0.03664115  0.08356737\n",
      "  0.09895127 -0.10264599 -0.12034765 -0.01442515  0.04262732  0.09679921\n",
      "  0.0283785   0.02808601 -0.10864563 -0.13915238  0.09593903  0.10536113\n",
      "  0.14196724  0.05671563  0.19328013 -0.18903896  0.01590599 -0.03813434\n",
      " -0.08570012 -0.167066   -0.0350875  -0.06077227]\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "####################################################################################\n",
    "###  2. Use the first paragraph’s words and take an average on these embeddings  ###\n",
    "####################################################################################\n",
    "def find_first_paragraph(text: str):\n",
    "    return text.split(\"\\n\")[0]\n",
    "\n",
    "def average_first_para_words(data_list: List[Dict]=train_data_list, mode: str=\"title\") -> Dict:\n",
    "    doc_embeddings = {}\n",
    "    if mode not in [\"title\", \"label\"]:\n",
    "        raise ValueError(\"Please input a valid mode: ['title', 'label']\")\n",
    "    for line in data_list:\n",
    "        doc_info = line[mode]\n",
    "        valid_word_embeddings = [model.wv[word] for word in utils.simple_preprocess(find_first_paragraph(line[\"text\"])) if word in model.wv]\n",
    "        # since the number of words in the first paragraph is so small, it may occur `ZeroDivisonError` in the computation, here I use a \n",
    "        # try-except flow to handle this exception\n",
    "        try:\n",
    "            doc_embedding = sum(valid_word_embeddings) / len(valid_word_embeddings)\n",
    "        except:\n",
    "            doc_embedding = [0 for _ in range(len(valid_word_embeddings))]\n",
    "        doc_embeddings[doc_info] = doc_embedding\n",
    "\n",
    "\n",
    "    return doc_embeddings\n",
    "\n",
    "average_first_para_words_embedding = average_first_para_words()\n",
    "print_document_embeddings(average_first_para_words_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b65aeca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Citizen_Kane:\n",
      "[ 0.5673132   2.5735762  -2.4015522   1.4646828   2.4414494   0.23134758\n",
      " -0.7582788   1.4945705  -1.2989575  -0.5255742   3.6375487   1.2254568\n",
      "  0.31390887 -0.2291875  -3.7744157  -0.46353722 -2.3252726   1.150479\n",
      " -4.9491506   1.2770969  -3.5225692  -1.2958678   0.58023304  1.9822471\n",
      " -3.9455993  -3.185178   -3.0894163  -0.43998092  4.9467807  -2.748377\n",
      " -2.1442611  -0.11394686  1.90851    -0.13847026 -0.10003136 -0.466067\n",
      "  0.05659573 -4.338431   -1.7576983   2.7716398  -1.7794051  -1.3922782\n",
      "  2.7097208  -5.3522778   1.5797373  -5.0972633   3.58482     0.3739586\n",
      "  1.1834635   2.3040378   2.143242    2.9439483   1.2628907  -3.2546782\n",
      "  1.4437549   1.6612926  -0.9021918   0.99941665 -4.7778273   0.36283186\n",
      " -3.8838646   3.8384051   1.9242661  -2.3999853  -0.79136986  0.56406\n",
      "  2.6099274  -2.3764508   0.6013121  -2.1730378  -3.0501795   5.2701416\n",
      " -3.2049825  -4.548285   -0.66558415  2.3368027   1.7597922  -2.7117915\n",
      " -0.04448551  5.824465   -3.473989    3.1053188  -2.1670802   2.7520854\n",
      " -1.7869328   1.3854716  -4.0840464   3.2706354   0.2533972   2.8404844\n",
      " -0.39616102 -1.3686566  -2.617183   -0.140538    2.6700933   0.7842656\n",
      "  0.9994839   0.3278415  -0.50786114  3.4998176 ]\n",
      "----------------------------------------------------------------------\n",
      "It_(2017_film):\n",
      "[ 3.007041    0.4794103  -0.22739406  0.5657301  -4.3527236  -0.30585784\n",
      "  0.16834836 -0.03217258  1.3720244   0.92593575  2.4190793  -1.0050429\n",
      " -1.8542925  -5.7580166   1.5670526  -0.75289154  0.3944642   3.8603501\n",
      " -8.065986    5.065213    0.5333384   2.0044944   2.5325623   0.5839458\n",
      " -2.988004   -0.20618379 -5.7747912   1.3671402   0.7469936  -4.761253\n",
      "  0.36433893 -0.07189909  2.1233845   0.63412416 -1.5308417  -4.180588\n",
      "  3.173535   -6.896771   -3.578587    2.745237    2.1446161  -2.6728098\n",
      "  4.719715   -6.066873   -1.5356703  -2.8558345   4.462699    0.5076254\n",
      "  2.4850771  -5.4446826  -1.4659373   1.732643   -0.5023979  -0.7076746\n",
      " -3.19998     0.2947149  -1.0253558   0.31379882 -1.9657322   1.4032073\n",
      "  3.443142    1.0210003   1.0560201  -3.0178251  -4.847818   -3.0178132\n",
      "  0.87670594 -0.6318896   1.7236342   2.926455   -2.9789484   0.58909667\n",
      " -2.779907   -3.863636   -1.6212093   2.461551    1.21497    -3.1785767\n",
      "  1.214181   -1.6999621  -0.9838536   4.270775   -1.617152   -0.7084269\n",
      " -2.992738    1.9926763   1.1790452  -0.07286826  1.489615    0.1902079\n",
      " -2.175301    0.02518745  0.01728863  2.0921571   3.1342103   2.0653863\n",
      "  1.1627458   0.31474048 -0.76703364 -2.6997666 ]\n",
      "----------------------------------------------------------------------\n",
      "Star_Wars_(film):\n",
      "[-0.04461201  2.0681024  -2.0230532  -0.53444606  1.1617588  -1.4213984\n",
      " -3.3941038   0.5102109  -2.2953284   0.72371924  0.8744286  -0.63333964\n",
      "  1.1280591  -0.12694697 -1.1387458  -2.3610604  -0.78083014 -0.03956677\n",
      " -6.970574    2.3664145   0.5888356   2.2604291   0.7540109   4.0254736\n",
      " -1.5468912  -4.0437546  -4.699165   -1.6562505  -1.4133661  -0.3692864\n",
      " -2.8781211  -1.923313   -3.111862   -3.3962595   0.9656238   0.06775919\n",
      "  1.0954614  -5.9999332  -3.9207714  -1.332268    2.1201317   0.6330921\n",
      "  1.5752044  -3.0645022  -2.073578   -1.0674067   3.8668792   4.7865696\n",
      "  0.81991374  0.5996558  -0.54536533 -0.8908526  -1.9897116   1.6095631\n",
      " -0.07184846  3.222915   -2.0775943  -0.37247428  0.13674995  2.3835344\n",
      " -0.2900991   1.107821    5.19201    -2.8621259  -2.206598   -2.0837495\n",
      " -1.5553064  -2.292441    0.21084988 -0.1140226  -4.206572    0.47414622\n",
      " -4.9801373  -4.11902     0.3274403   2.5738053  -1.732095   -4.510747\n",
      "  3.159381    5.722932   -0.36611912  3.865425   -1.3122144  -1.1632346\n",
      " -4.3019357  -0.38962677 -1.7716216  -1.3746005  -0.66333896  0.4518442\n",
      " -1.4023854  -1.762262   -1.9815685  -1.8075202   2.8276181   1.3610314\n",
      "  0.5242927   1.4995022  -2.960002   -0.14531659]\n",
      "----------------------------------------------------------------------\n",
      "Star_Trek:_The_Motion_Picture:\n",
      "[-1.2779815   1.8239404  -1.4450884   1.0833883   0.24616347 -1.7428218\n",
      " -0.04764467 -0.48519957 -0.9646263   0.39895597  1.2064021   0.3488906\n",
      " -0.08515905 -0.04519758 -2.175012   -1.565641   -1.13362    -1.3694086\n",
      " -8.341322    1.9723854  -1.4058073   2.7762642  -0.2795657   4.6513133\n",
      " -0.6264853  -1.9690127  -6.955882    0.97577035  1.5788025  -1.9063888\n",
      " -2.9268613  -0.4969641  -1.8558643  -1.0726376  -2.1923811   0.19363998\n",
      "  2.1152837  -8.152036   -6.6046233   1.4477034   0.5686433   0.5639551\n",
      " -2.5840385  -3.6607795   4.612481   -4.5776963   2.874889    6.3577237\n",
      "  4.7694654   0.30138436  1.7956828  -0.34844598 -1.6326907  -0.5271932\n",
      "  0.8669898  -0.97065806  1.0915908   0.9825952  -5.9433107  -0.07318902\n",
      " -1.2824502   2.8032162   1.8059068  -2.9650223  -3.2965102  -0.83596504\n",
      " -1.5270443  -2.23883     1.4862853  -0.77167255 -4.1077323   2.8250682\n",
      " -1.7475911  -3.4005177  -3.3752167   3.6620104   0.2586001  -1.3839089\n",
      "  3.0761282   3.7043393  -0.0925675   2.2192028  -0.7230336   2.6308172\n",
      " -5.511721    2.2437084  -2.45759    -0.52535754 -0.41690865  0.64473146\n",
      "  2.3087263  -0.15448242 -2.3100474  -1.3259441   1.5838993   0.5964573\n",
      "  0.5508524   2.1047227   0.5924352   2.8277848 ]\n",
      "----------------------------------------------------------------------\n",
      "Frozen_(2013_film):\n",
      "[ 0.6985771  -0.52112496 -0.96994704  0.8989009  -2.3751755  -1.1759244\n",
      " -4.2380466  -2.555248    0.7165516   2.1293938   3.4195988  -1.175347\n",
      "  0.8448141  -4.3231287   0.74015987 -1.8741865  -0.36940762  2.2406886\n",
      " -4.084478    7.172799   -0.28233942  4.8091664   1.6539767  -0.04490083\n",
      " -3.996015   -2.8223002  -1.9075675  -1.4278338  -0.71391374 -2.0013483\n",
      "  0.7607467  -1.5087799  -0.8246714   1.0317258  -0.2062747  -2.0977194\n",
      "  1.6463822  -6.0429993  -0.24717313  2.0765624  -0.08606219  0.06369872\n",
      "  1.1036534  -4.5959277  -0.38289422 -0.8324144   3.670402    1.1695824\n",
      "  1.5807579  -1.6082026  -0.99120504  0.08049351 -1.0249505   1.9067235\n",
      " -4.7764173  -1.4088027   1.2757653   3.0292525  -2.40339     3.547286\n",
      "  0.02322502  1.6196519   1.9236195   0.6084374  -1.661122   -3.8908734\n",
      " -0.1939957   2.1048899   0.17700636  0.5544108  -3.3236375  -2.6666627\n",
      " -6.094781   -2.5284176   0.37560394  3.2924302   0.62731546 -5.0744734\n",
      "  0.7388974   1.4002001  -0.05133491  4.587933   -2.30802    -1.9260637\n",
      "  0.70111024 -1.0904647  -2.3677657  -1.7967179   1.527324    2.5013144\n",
      " -3.8939905   0.64166635 -1.0887232  -0.38461748  4.173871    1.9840844\n",
      " -2.431401   -0.0930728  -2.8387508  -0.6648132 ]\n",
      "----------------------------------------------------------------------\n",
      "Black_Panther_(film):\n",
      "[ 2.3367422  -0.39228472 -2.5604937   1.4746664  -2.2198393  -0.05288075\n",
      " -1.8296788  -2.2724893   0.14796084 -0.27776426  2.078564    0.11470333\n",
      " -0.9722904  -5.075229    4.4340496  -4.122185   -0.29047725  0.27643383\n",
      " -4.6090956   4.8324475   1.7644598   1.229747    0.67610633 -0.47015846\n",
      " -3.9130025   0.11382085 -1.7836303  -0.79026747 -1.2225521  -6.966354\n",
      " -0.05813375 -3.8066626  -1.0816344  -1.3559724  -0.5795284  -2.6170468\n",
      "  0.5734588  -3.3567767  -5.3963914   1.677507    0.99513274 -1.937936\n",
      "  1.873354   -5.094266   -1.7511357  -3.689062    3.1287298   4.2620296\n",
      "  1.90398    -2.8135784  -0.09255806  1.6240251   0.03706336 -0.93951356\n",
      " -1.2115457   1.6682669   0.12966034  0.82670325 -0.7094076   1.993207\n",
      " -0.38819128  2.2086768   0.5946336  -2.8605058  -7.8720493  -2.5756514\n",
      "  1.6622536   3.6627512   2.5016873   1.7272147  -2.3114905  -2.095875\n",
      " -3.2571049  -1.9770461  -1.894483    1.9161537  -0.9648718  -4.077874\n",
      "  3.2020123   1.3081188   2.06512     4.4874954  -1.5545963  -1.2408444\n",
      " -2.0064623   0.98379165 -1.2072264  -0.14139718 -2.320686   -2.015365\n",
      " -1.6856092   3.0851421  -0.54184407 -1.821313    3.8022494  -1.5622443\n",
      " -1.5091673   1.0450264   0.3277612  -1.2867336 ]\n",
      "----------------------------------------------------------------------\n",
      "The_Cabinet_of_Dr._Caligari:\n",
      "[-1.2301518   1.2818699  -3.375553    3.3202808   2.3000288  -1.1872758\n",
      " -2.5439231  -0.5978454  -4.0789905   1.7742152   1.6742554   1.2366279\n",
      "  2.3564768  -0.98121    -3.1615548   4.471303    0.35736728 -3.0200207\n",
      " -4.1594324  -0.22708698  2.0704978   0.16893981 -1.2068886   0.61992955\n",
      " -1.3725297  -3.882248   -0.88896513 -0.3318299   1.378285    0.0783795\n",
      " -1.5457524  -3.070782   -1.0234679  -2.6350436   2.5673866   0.3547031\n",
      " -5.4137836  -5.441481   -3.4499097  -0.68903327  4.364941   -1.4473692\n",
      "  3.1778548  -1.77999     0.92498094 -5.3804727   3.8355618   0.13857451\n",
      "  0.52814794  1.0528455  -3.1259558   1.3408744   0.8452972   0.29907617\n",
      "  5.479296    1.6852189  -0.8444763   0.23597404 -3.263886    1.3761922\n",
      " -1.1050484   2.9863217   2.3579748  -0.51759243  1.9359313  -0.51835436\n",
      "  0.33129007  4.360032   -0.5927248  -5.6545877  -3.8692896   2.6516027\n",
      " -1.6786747  -3.4070678   0.5759308   4.160958    0.75873    -2.3288379\n",
      " -0.38997194 -1.1679199   0.655412    4.5269594  -4.037761    0.25089154\n",
      " -1.7178886  -0.913759    0.11417661  0.41174278 -0.2487907   1.8025191\n",
      "  0.648071   -0.9042114  -0.57620496 -2.631732    2.9324389   1.7203692\n",
      "  2.8092844   2.1551054  -1.0241758  -1.0694971 ]\n",
      "----------------------------------------------------------------------\n",
      "The_Shining_(film):\n",
      "[ 1.5819947   2.6341786  -0.85077614  1.5018394  -3.040422    0.7210887\n",
      " -2.8546948   0.18814667  0.7668393   0.9885199   2.7259235   3.426601\n",
      " -2.183807    0.12924288  0.9007599   1.4774703  -0.15836717  4.411796\n",
      " -6.409998    3.8285043  -0.02828225 -0.40352124  0.74818933  0.42653537\n",
      " -3.8487215  -2.3587387  -6.9982824   1.1657692   0.9032338  -5.484077\n",
      " -1.4870573   1.6988285   1.9377954  -0.9458552   0.27462903 -0.23185326\n",
      " -2.509972   -4.9302087  -2.3554952   0.99048185 -2.089018   -2.1404345\n",
      "  3.3549025  -4.3934717   0.94935477 -4.039925    2.4258287  -0.5641942\n",
      "  1.7455695   1.6352408   0.3134049   4.214177    1.1558865  -0.2979468\n",
      "  0.607698    1.5692412  -0.8214235   0.21730776 -0.3193949  -0.3497319\n",
      "  3.2695377   0.29812187  3.3322513  -0.7684343   1.8355694  -2.0363464\n",
      "  2.2001345   1.7052205   0.84946024 -3.027648    0.06691545  1.4874897\n",
      " -1.8238384  -2.8189387  -0.8584527   2.2862794   1.0477738  -0.5362313\n",
      " -0.67078596 -0.30877212 -1.0666102   1.7696024  -1.5978621   1.2060281\n",
      " -5.1801357   1.4625549  -0.93658715  2.2024803   1.8025429  -1.856017\n",
      " -1.479675   -0.8221947  -4.121473    0.38760585  1.9184214   3.5001054\n",
      "  3.3379314   0.9112173  -1.2485418   0.8519743 ]\n",
      "----------------------------------------------------------------------\n",
      "Scream_(franchise):\n",
      "[-1.4612925  -1.0244328  -4.0572076   0.99003977 -0.6331899  -1.6684619\n",
      " -3.2369766   1.0518959  -0.18428825 -3.7921517  -0.12019099 -0.41349646\n",
      " -0.96212053 -0.74382204  1.8248628   0.88621634  0.20889954  1.2624224\n",
      " -5.542511   11.097502    2.9078608   2.384917   -1.1095927   4.597358\n",
      " -2.6478329  -0.74457055 -4.6388683  -4.7795205   0.09984871 -6.0022583\n",
      " -2.0457616  -1.2808989   1.9255183  -1.2095033  -0.39108804  1.3633212\n",
      " -4.702398   -6.9234824  -4.3624067   3.1564918  -0.34178105 -0.63618714\n",
      " -0.27774534 -4.5697446   0.47511983 -1.3071648   2.7391803   0.02099137\n",
      " -0.5657233  -0.0436838   0.31007153  3.1157398   0.99777985  1.6151553\n",
      "  1.2588551   1.3175503  -1.6043891  -0.46258238 -4.6513186   0.8946191\n",
      "  1.5022584   3.464331   -0.21671732  1.1078806  -3.4952168  -4.8490973\n",
      " -0.14475024 -1.6972911   1.6606982   1.3838935  -1.0432068  -1.9375204\n",
      " -5.415374   -3.9087346   1.4316847   3.356345   -2.0707824  -0.7106283\n",
      "  1.7057977  -0.06461073 -0.50055265  6.6012373   1.398318   -1.5748647\n",
      " -2.2963393   2.4655395  -1.2007778  -1.7208954  -1.7627568   1.1288027\n",
      " -1.3614879   1.4148753  -3.9507027  -0.69567484 -2.2886715   3.3791277\n",
      " -1.1739254   2.0640292   2.0953035   4.5269284 ]\n",
      "----------------------------------------------------------------------\n",
      "Batman_v_Superman:_Dawn_of_Justice:\n",
      "[ 3.2826314  -0.8089114  -1.5405643  -0.11913919 -1.5107571   0.09268621\n",
      " -3.6226037  -2.7315683   0.92780405 -0.12667264  3.5052793   1.3551645\n",
      " -1.998411   -4.7171583   4.2259064  -3.8394873  -1.1269447   1.982459\n",
      " -3.8097713   3.437581    0.5692963   1.4014937   2.5881424   4.05551\n",
      " -2.6622055  -0.35422763 -3.584322   -0.22420342 -1.6197103  -6.4293017\n",
      " -0.39493343  1.790434    1.0545998  -1.6555467  -0.09874634 -1.1655186\n",
      "  1.1974175  -7.770336   -2.2086475   1.720804    2.7774725  -1.738919\n",
      "  5.200615   -4.9991536  -3.7109268  -3.6473768   4.315802    2.7654936\n",
      "  0.16253643 -3.9913979  -2.02675     1.9291731  -0.27077147 -0.21691623\n",
      " -1.6583582   4.52986    -2.2138677   0.09411067 -0.87574965  1.3420068\n",
      "  1.1160568  -1.2241026   1.427456   -2.3700514  -2.0233357  -2.3782256\n",
      "  1.0692129  -0.6539546   0.5747871   2.5355725  -2.480668   -0.5612706\n",
      " -2.086974   -3.0909967  -0.5417096   4.609638   -0.44538537 -4.692577\n",
      "  1.0334064   0.81177235  1.9103826   5.9574456  -0.36693886 -2.9527085\n",
      " -2.7465875   0.2480336   1.7640555   0.87562543 -1.241567    1.498015\n",
      " -0.82771784  1.8500363  -1.4750065   1.8000764   3.2260768  -0.05582083\n",
      " -3.486379    0.09488977  1.3171502  -4.5413127 ]\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "####################################################################################\n",
    "###             3. Use the doc2vec algorithm to present each document            ###\n",
    "####################################################################################\n",
    "def build_doc2vec() -> Doc2Vec:\n",
    "    \"\"\"\n",
    "    Build the `Doc2Vec` model based on the `train_data_list`\n",
    "    \"\"\"\n",
    "    tagged_data = [TaggedDocument(words=doc.split(), tags=[str(i)]) for i, doc in enumerate([line[\"text\"] for line in train_data_list])]\n",
    "    model = Doc2Vec(vector_size=100, window=5, min_count=5, epochs=10)\n",
    "    model.build_vocab(tagged_data)\n",
    "    model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "    return model\n",
    "\n",
    "doc2vec_model = build_doc2vec()\n",
    "\n",
    "def doc2vec_model_embeddings(data_list: List[Dict]=train_data_list, mode: str=\"title\") -> tuple:\n",
    "    embeddings = {}\n",
    "    if mode not in [\"title\", \"label\"]:\n",
    "        ValueError(\"Please input a valid mode: ['title', 'label']\")\n",
    "    for line in data_list:\n",
    "        embeddings[line[mode]] = doc2vec_model.infer_vector(line[\"text\"].split())\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "doc2vec_model_embedding = doc2vec_model_embeddings()\n",
    "print_document_embeddings(doc2vec_model_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091195ab-834c-4184-a8f2-490cd26b7ff4",
   "metadata": {},
   "source": [
    "### Task 4 Build classifier to test docs\n",
    "> Build softmax regression model to classifier testing documents based on these training doc embeddings. Does it getting better than Naive Bayes'? (You have 3 models.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "98c43b8e-603f-4352-982c-f90d85e9f790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:             Use the average of embeddings of all words in each document, accuracy: 0.000000\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (100,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 43\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text, method \u001b[38;5;129;01min\u001b[39;00m methods\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     42\u001b[0m     softmax_model \u001b[38;5;241m=\u001b[39m softmaxModel(train_data_list, method)\n\u001b[1;32m---> 43\u001b[0m     \u001b[43msoftmax_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_data_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel: \u001b[39m\u001b[38;5;132;01m{:>71}\u001b[39;00m\u001b[38;5;124m, accuracy: \u001b[39m\u001b[38;5;132;01m{:>.6f}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(text, softmax_model\u001b[38;5;241m.\u001b[39maccuracy()))\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m softmax_model\n",
      "Cell \u001b[1;32mIn[34], line 29\u001b[0m, in \u001b[0;36msoftmaxModel.predict\u001b[1;34m(self, test_data)\u001b[0m\n\u001b[0;32m     27\u001b[0m test_embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__method(test_data)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__ytest \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(test_embedding\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__ypred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtest_embedding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__ypred\n",
      "File \u001b[1;32mc:\\Users\\雪浪\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_base.py:351\u001b[0m, in \u001b[0;36mLinearClassifierMixin.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;124;03mPredict class labels for samples in X.\u001b[39;00m\n\u001b[0;32m    339\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;124;03m    Vector containing the class labels for each sample.\u001b[39;00m\n\u001b[0;32m    349\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    350\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(X)\n\u001b[1;32m--> 351\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecision_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(scores\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    353\u001b[0m     indices \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(scores \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mint\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\雪浪\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_base.py:332\u001b[0m, in \u001b[0;36mLinearClassifierMixin.decision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    329\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    330\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(X)\n\u001b[1;32m--> 332\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    333\u001b[0m scores \u001b[38;5;241m=\u001b[39m safe_sparse_dot(X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_\u001b[38;5;241m.\u001b[39mT, dense_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_\n\u001b[0;32m    334\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39mreshape(scores, (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,)) \u001b[38;5;28;01mif\u001b[39;00m scores\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m scores\n",
      "File \u001b[1;32mc:\\Users\\雪浪\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:633\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    631\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 633\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    635\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32mc:\\Users\\雪浪\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:997\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    995\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    996\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 997\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43m_asarray_with_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    998\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[0;32m    999\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1000\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m   1001\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\雪浪\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:521\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[0;32m    519\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39marray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    520\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 521\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39masarray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    523\u001b[0m \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[0;32m    524\u001b[0m \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[0;32m    525\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array)\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (100,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class softmaxModel:\n",
    "    def __init__(self, data: List[Dict], method: Callable) -> None:\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        self.__model = None\n",
    "        self.__ytest = None\n",
    "        self.__ypred = None\n",
    "        self.__method = method               # the three different models in task 3\n",
    "        self.__train(method(data, \"label\"))  # use `method(data)` to get the embedding\n",
    "\n",
    "    def __train(self, embedding: Dict):\n",
    "        \"\"\"\n",
    "        Train a softmax classifier based on the embedding in train data\n",
    "        \"\"\"\n",
    "        X, y = list(embedding.values()), list(embedding.keys())\n",
    "        self.X, self.y = X, y\n",
    "        self.__model = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\").fit(self.X, self.y)\n",
    "\n",
    "\n",
    "    def predict(self, test_data: List[Dict]) -> List:\n",
    "        \"\"\"\n",
    "        Return the pred value of y based on the trained model and given test data\n",
    "        \"\"\"\n",
    "        test_embedding = self.__method(test_data, \"label\")\n",
    "        self.__ytest = list(test_embedding.keys())\n",
    "        self.__ypred = self.__model.predict(list(test_embedding.values()))\n",
    "        return self.__ypred\n",
    "\n",
    "    def accuracy(self) -> float:\n",
    "        return accuracy_score(self.__ytest, self.__ypred)\n",
    "    \n",
    "methods = {\n",
    "    \"Use the average of embeddings of all words in each document\": average_all_words, \n",
    "    \"Use the first paragraph’s words and take an average on these embeddings\": average_first_para_words, \n",
    "    \"Use the doc2vec algorithm to present each document\": doc2vec_model_embeddings\n",
    "}\n",
    "\n",
    "for text, method in methods.items():\n",
    "    softmax_model = softmaxModel(train_data_list, method)\n",
    "    softmax_model.predict(test_data_list)\n",
    "    print(\"Model: {:>71}, accuracy: {:>.6f}\".format(text, softmax_model.accuracy()))\n",
    "    del softmax_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8206de2a-6d96-4016-b74e-4a08da2e1cab",
   "metadata": {},
   "source": [
    "### Task 5 Use t-SNE to project doc vectors\n",
    "\n",
    "> Use t-SNE to project training document embeddings into 2d and plot them out for each of the above choices. Each point should have a specific color (represent a particular cluster). You may need to try different parameters of t-SNE. One can find more details about t-SNE in this [excellent article](https://distill.pub/2016/misread-tsne/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907fe1b9-0650-438c-ad75-49e6632bf6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
