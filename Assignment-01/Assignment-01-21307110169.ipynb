{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7186371d-a854-4609-8158-8537db05c369",
   "metadata": {},
   "source": [
    "## Instruction\n",
    "\n",
    "> 1. Rename Assignment-01-###.ipynb where ### is your student ID.\n",
    "> 2. The deadline of Assignment-01 is 23:59pm, 03-31-2024\n",
    ">\n",
    "> 3. In this assignment, you will\n",
    ">    1) explore Wikipedia text data\n",
    ">    2) build language models\n",
    ">    3) build NB and LR classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c23f2dd-e01e-4b02-9b17-c92885f8a428",
   "metadata": {},
   "source": [
    "## Task0 - Download datasets\n",
    "> Download the preprocessed data, enwiki-train.json and enwiki-test.json from the Assignment-01 folder. In the data file, each line contains a Wikipedia page with attributes, title, label, and text. There are 1000 records in the train file and 100 records in test file with ten categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cc7cb5-dd64-4886-8cd5-24f147288941",
   "metadata": {},
   "source": [
    "## Task1 - Data exploring and preprocessing\n",
    "\n",
    "> 1) Print out how many documents are in each class  (for both train and test dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ba34ae-48ae-438a-b3dd-c9d4cb3ee416",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Callable\n",
    "\n",
    "################################################################ \n",
    "###         define the function we need for later use        ###\n",
    "################################################################\n",
    "\n",
    "def load_json(file_path: str) -> list:\n",
    "    \"\"\"\n",
    "    Fetch the data from `.json` file and concat them into a list.\n",
    "\n",
    "    Input:\n",
    "    - file_path: The relative file path of the `.json` file\n",
    "\n",
    "    Returns:\n",
    "    - join_data_list: A list containing the data, with the format of [{'title':<>, 'label':<>, 'text':<>}, {}, ...]\n",
    "    \"\"\"\n",
    "    join_data_list = []\n",
    "    with open(file_path, \"r\") as json_file:\n",
    "        for line in json_file:\n",
    "            line = line.strip()\n",
    "            # guaranteen the line is not empty\n",
    "            if line: \n",
    "                join_data_list.append(json.loads(line))\n",
    "    return join_data_list\n",
    "\n",
    "def iterate_line_in_list(data_list: list, f: Callable) -> dict:\n",
    "    \"\"\"\n",
    "    Iterate the `data_list` while recording the class.\n",
    "\n",
    "    Input:\n",
    "    - data_list: A list containing (train/test) data, with the format of [{'title':<>, 'label':<>, 'text':<>}, {}, ...]\n",
    "    - type: The type of the data, default is \"train\". Can take the value of \"train\" or \"test\"\n",
    "    - f: A function to compute the number of documents, sentences e.t.c. in each `line`\n",
    "\n",
    "    Output:\n",
    "    - class_dict: A list containing dictionaries with (key, value) as (<class>, <number_of_documents>)\n",
    "    \"\"\"\n",
    "    class_dict = {}\n",
    "    for line in data_list:\n",
    "        line_class = line['label']\n",
    "        class_dict[line_class] = class_dict.get(line_class, 0) + f(line['text'])  # if the class doesn't exist, set the value as 0\n",
    "    return class_dict\n",
    "\n",
    "################################################################ \n",
    "###                        end define                        ###\n",
    "################################################################\n",
    "\n",
    "def count_docs(text):\n",
    "    return 1\n",
    "\n",
    "def print_docs_in_class(class_dict: dict, type: str = \"train\") -> None:\n",
    "    print(\"The number of documents in each class for \" + type + \" dataset is: \\n\")\n",
    "    for _class, _times in class_dict.items():\n",
    "        print(\"There are {:>3} documents in class {:>10}\".format(_times, _class))\n",
    "    print('-'*50)\n",
    "\n",
    "\n",
    "# Fetch data from the json file\n",
    "    \n",
    "train_file_path, test_file_path = \"enwiki-train.json\", \"enwiki-test.json\"\n",
    "train_data_list, test_data_list = map(load_json, [train_file_path, test_file_path])\n",
    "\n",
    "# print out the number of documents of each class in train and test dataset\n",
    "\n",
    "train_docs_num = iterate_line_in_list(train_data_list, count_docs)\n",
    "test_docs_num = iterate_line_in_list(test_data_list, count_docs)\n",
    "\n",
    "print_docs_in_class(train_docs_num)\n",
    "print_docs_in_class(test_docs_num, \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1a2d5c-b719-41ec-8f14-04e092517eb9",
   "metadata": {},
   "source": [
    "> 2) Print out the average number of sentences in each class.\n",
    ">    You may need to use sentence tokenization of NLTK.\n",
    ">    (for both train and test dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e85dc7-d50b-406a-9550-b2063f236ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def count_sents(text):\n",
    "    return len(sent_tokenize(text))\n",
    "\n",
    "def print_ave_sents_in_class(class_dict: dict, type: str = \"train\"):\n",
    "    # get the dict of number of documents in each class based on the input type\n",
    "    if type == \"train\":\n",
    "        docs_num_class = train_docs_num\n",
    "    elif type == \"test\":\n",
    "        docs_num_class = test_docs_num\n",
    "    else:\n",
    "        raise TypeError\n",
    "    \n",
    "    # print the result\n",
    "    print(\"The average number of sentences in each class for \" + type + \" dataset is: \\n\")\n",
    "    for _class, _times in class_dict.items():\n",
    "        print(\"There are average {:>7.2f} sentences in class {:>10}\".format(_times / docs_num_class[_class], _class))\n",
    "    print('-'*50)\n",
    "\n",
    "\n",
    "train_ave_sents = iterate_line_in_list(train_data_list, count_sents)\n",
    "test_ave_sents = iterate_line_in_list(test_data_list, count_sents)\n",
    "\n",
    "print_ave_sents_in_class(train_ave_sents)\n",
    "print_ave_sents_in_class(test_ave_sents, \"test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502ef409-65a5-4d07-9b69-c5986569970a",
   "metadata": {},
   "source": [
    "> 3) Print out the average number of tokens in each class\n",
    ">    (for both train and test dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d7628e-762c-49fe-804a-796fa0265af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def count_tokens(text):\n",
    "    return len(word_tokenize(text))\n",
    "\n",
    "def print_ave_tokens_in_class(class_dict: dict, type: str = \"train\"):\n",
    "    # get the dict of number of documents in each class based on the input type\n",
    "    if type == \"train\":\n",
    "        docs_num_class = train_docs_num\n",
    "    elif type == \"test\":\n",
    "        docs_num_class = test_docs_num\n",
    "    else:\n",
    "        raise TypeError\n",
    "    \n",
    "    # print the result\n",
    "    print(\"The average number of tokens in each class for \" + type + \" dataset is: \\n\")\n",
    "    for _class, _times in class_dict.items():\n",
    "        print(\"There are average {:>8.2f} tokens in class {:>10}\".format(_times / docs_num_class[_class], _class))\n",
    "    print('-'*50)\n",
    "\n",
    "train_ave_tokens = iterate_line_in_list(train_data_list, count_tokens)\n",
    "test_ave_tokens = iterate_line_in_list(test_data_list, count_tokens)\n",
    "\n",
    "print_ave_tokens_in_class(train_ave_tokens)\n",
    "print_ave_tokens_in_class(test_ave_tokens, \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d999a66-977c-469b-939c-c3934210972e",
   "metadata": {},
   "source": [
    "> 4) For each sentence in the document, remove punctuations and other special characters so that each sentence only contains English words and numbers. To make your life easier, you can make all words as lower cases. For each class, print out the first article's name and the processed first 40 words. (for both train and test dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819a28ef-0821-4ace-be70-fb9b249d355b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from copy import deepcopy\n",
    "\n",
    "def clean_doc(document: str) -> list:\n",
    "    document = document.lower()\n",
    "    cleaned_document = []\n",
    "    sentences = sent_tokenize(document)\n",
    "    for sentence in sentences:\n",
    "        # remove punctuations and special characters\n",
    "        sentence = re.sub(r'[^a-zA-Z0-9\\s]', '', sentence)\n",
    "        # remove extra whitespaces\n",
    "        sentence = re.sub(r'\\s+', ' ', sentence).strip()\n",
    "        cleaned_document.append(sentence)\n",
    "    return cleaned_document\n",
    "\n",
    "def process_data_list(data_list: list, type: str = \"train\") -> list:\n",
    "    explored = []\n",
    "    print(\"The result of the \" + type + \" data list:\")\n",
    "    # process the data_list\n",
    "    for line in data_list:\n",
    "        class_label = line[\"label\"]\n",
    "        former_line_text = line[\"text\"]                         # former text\n",
    "        line[\"sentences\"] = clean_doc(line[\"text\"])             # cleaned sentences list\n",
    "        line[\"text\"] = \". \".join(line[\"sentences\"]) + \".\"       # join the sentence list with \". \" to generate the processed text\n",
    "        if class_label not in explored:\n",
    "            explored.append(class_label)\n",
    "            # print the result\n",
    "            print()\n",
    "            print(\"The first article's name of class {:>10} is {:>20}\".format(class_label, line[\"title\"]))\n",
    "            print(\"The cleaned text is: [{}] ==> [{}]\".format(former_line_text[:40], line[\"text\"][:40]))\n",
    "    print(\"-\"*120)\n",
    "    return data_list\n",
    "\n",
    "# make a deepcopy of the origin data list to avoid over-write\n",
    "cleaned_train_data_list = deepcopy(train_data_list)\n",
    "cleaned_test_data_list = deepcopy(test_data_list)\n",
    "\n",
    "# process the copyed data list in place by `process_data_list`\n",
    "cleaned_train_data_list = process_data_list(cleaned_train_data_list)\n",
    "cleaned_test_data_list = process_data_list(cleaned_test_data_list, \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6c7ea9-18fa-43f1-bded-625c09a947b1",
   "metadata": {},
   "source": [
    "## Task2 - Build language models\n",
    "\n",
    "> 1) Based on the training dataset, build unigram, bigram, and trigram language models using Add-one smoothing technique. It is encouraged to implement models by yourself. If you use public code, please cite it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e979b1-235f-4663-8acc-00257735e209",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from itertools import product\n",
    "import math\n",
    "\n",
    "class NGramModels(object):\n",
    "    def __init__(self, n, laplace=1) -> None:\n",
    "        self.n = n\n",
    "        self.laplace = laplace\n",
    "        self._model = None\n",
    "        self._tokens = None\n",
    "        self._vocab = None\n",
    "        self._masks = list(reversed(list(product((0,1), repeat=n))))\n",
    "    \n",
    "    def _preprocess(self, sentences: list) -> list:\n",
    "        \"\"\"\n",
    "        Preprocess the raw text by adding (n-1)*\"<s>\" (or one single <s>) on the front of the sentence \n",
    "        and replacing the tokens which occur only once with \"<UNK>\".\n",
    "\n",
    "        Input:\n",
    "        - sentences: A list with each element as a `sent_tokenized` sentence.\n",
    "\n",
    "        Return:\n",
    "        - tokens: A list containing the processed tokens\n",
    "        \"\"\"\n",
    "        sos = \"<s> \" * (self.n - 1) if self.n > 1 else \"<s> \"\n",
    "        tokenized_sentences = ['{}{} {}'.format(sos, sent, \"</s>\").split() for sent in sentences]\n",
    "        tokenized_sentences = [word for sublist in tokenized_sentences for word in sublist]  # flatten\n",
    "        # Replace tokens which appear only once in the corpus with <UNK>\n",
    "        vocab = nltk.FreqDist(tokenized_sentences)\n",
    "        tokens = [token if vocab[token] > 1 else \"<UNK>\" for token in tokenized_sentences]\n",
    "        return tokens\n",
    "    \n",
    "    def _smooth(self) -> dict:\n",
    "        \"\"\"\n",
    "        Smooth the frequency distribution based on Laplace smoothing.\n",
    "\n",
    "        Return:\n",
    "        - A dictionary {<ngram>: <count>, ...} containing the information of the frequency distribution\n",
    "        \"\"\"\n",
    "        vocab_size = len(self._vocab)\n",
    "        if self.n == 1:         # if n equals 1, we don't need to smooth it\n",
    "            return {(unigram,): count / vocab_size for unigram, count in self._vocab.items()}\n",
    "        else:\n",
    "            n_grams = nltk.ngrams(self._tokens, self.n)\n",
    "            n_vocab = nltk.FreqDist(n_grams)\n",
    "            n_minus_one_grams = nltk.ngrams(self._tokens, self.n-1)\n",
    "            n_minus_one_vocab = nltk.FreqDist(n_minus_one_grams)\n",
    "            return {ngram: (n_freq + self.laplace) / (n_minus_one_vocab[ngram[:-1]] + self.laplace * vocab_size) for ngram, n_freq in n_vocab.items()}\n",
    "\n",
    "    def train(self, sentences: list) -> None:\n",
    "        \"\"\"\n",
    "        Train the model based on the given raw text.\n",
    "\n",
    "        Input:\n",
    "        - sentences: A list with each element as a `sent_tokenized` sentence.\n",
    "        \"\"\"\n",
    "        tokens = self._preprocess(sentences)\n",
    "        self._tokens = tokens\n",
    "        self._vocab = nltk.FreqDist(self._tokens)\n",
    "        self._model = self._smooth()\n",
    "\n",
    "    def _find_match(self, ngram: tuple) -> str:\n",
    "        \"\"\"\n",
    "        Find the best match of the given ngram token in the trained model by masking the ngram in iteration\n",
    "\n",
    "        Input: \n",
    "        - ngram: A tuple representing a test ngram token\n",
    "\n",
    "        Return:\n",
    "        - tokens: The best match of the ngram in the trained model\n",
    "        \"\"\"\n",
    "        mask = lambda ngram, bitmask: tuple((token if flag == 1 else \"<UNK>\" for token, flag in zip(ngram, bitmask)))\n",
    "        possible_tokens = [mask(ngram, bitmask) for bitmask in self._masks]\n",
    "        for tokens in possible_tokens:\n",
    "            if tokens in self._model:\n",
    "                return tokens\n",
    "\n",
    "    def perplexity(self, test_sentences: list) -> float:\n",
    "        \"\"\"\n",
    "        Compute the perplexity of the given `test_sentences` based on the train tokens.\n",
    "\n",
    "        Input:\n",
    "        - test_sentences: A list containing the test sentences\n",
    "        \n",
    "        Return:\n",
    "        - perplexity: The perplexity of the test material, computed by the geomteric mean of the\n",
    "                      log probabilities. \n",
    "        \"\"\"\n",
    "        test_tokens = self._preprocess(test_sentences)\n",
    "        test_ngrams = nltk.ngrams(test_tokens, self.n)\n",
    "        known_ngrams  = (self._find_match((ngram,)) if isinstance(ngram, str) else self._find_match(ngram) for ngram in test_ngrams)\n",
    "        probabilities = [self._model[ngram] for ngram in known_ngrams]\n",
    "\n",
    "        return math.exp((-1 / len(test_tokens)) * sum(map(math.log, probabilities)))\n",
    "\n",
    "    def _best_candidate(self, prev: tuple, i: int, blacklist: list=[]) -> tuple:\n",
    "        \"\"\"\n",
    "        Find the best candidate from the trained model based on the previous text and blacklist.\n",
    "\n",
    "        Input:\n",
    "        - prev: A tuple containing the information of the previous text \n",
    "        - i: current index\n",
    "        - blacklist: A list of values that can't be taken\n",
    "\n",
    "        Return:\n",
    "        - candidate: A tuple with the format (<candidate_token>, <prob>)\n",
    "        \"\"\"\n",
    "        blacklist += [\"<UNK>\"]\n",
    "        candidates = [(ngram[-1], prob) for ngram, prob in self._model.items() if ngram[:-1] == prev] # find the candidates based on the trained moel\n",
    "        candidates = [candidate for candidate in candidates if candidate[0] not in blacklist]         # filter out the candidate in blacklist\n",
    "        candidates = sorted(candidates, key=lambda candidate: candidate[1], reverse=True)             # sort the candidates based on the prob\n",
    "        if len(candidates) == 0:\n",
    "            return (\"</s>\", 1)\n",
    "        return candidates[0 if prev != () and prev[-1] != \"<s>\" else i]\n",
    "\n",
    "    def generate(self, num: int, min_len: int=12, max_len: int=24):\n",
    "        \"\"\"\n",
    "        Generate sentences based on the trained model for given number of sentences, minimum length and maximun length\n",
    "\n",
    "        Input:\n",
    "        - num: The number of sentences we need to generate\n",
    "        - min_len: The minmum length of the generated sentence\n",
    "        - max_len: The maximum length of the generated sentence\n",
    "\n",
    "        Return (Yield):\n",
    "        - The generated sentence one by one\n",
    "        \"\"\"\n",
    "        for i in range(num):\n",
    "            sent, prob = [\"<s>\"] * max(1, self.n - 1), 1\n",
    "            while sent[-1] != \"</s>\":\n",
    "                prev = () if self.n == 1 else tuple(sent[-(self.n-1):])\n",
    "                blacklist = sent + ([\"</s>\"] if len(sent) < min_len else [])\n",
    "                next_token, next_prob = self._best_candidate(prev, i, blacklist)\n",
    "                sent.append(next_token)\n",
    "                prob *= next_prob\n",
    "                \n",
    "                if len(sent) >= max_len:\n",
    "                    sent.append(\"</s>\")\n",
    "\n",
    "            yield ' '.join(sent), -1/math.log(prob)\n",
    "\n",
    "# generate the train sentences from `cleaned_train_data_list`\n",
    "train_sentences = []\n",
    "for each in cleaned_train_data_list:\n",
    "    train_sentences.extend(each[\"sentences\"]) \n",
    "\n",
    "# unigram language model\n",
    "unigramModel = NGramModels(1)\n",
    "unigramModel.train(train_sentences)\n",
    "print(\"The unigram language model has been successfully built!\")\n",
    "# bigram language model\n",
    "bigramModel = NGramModels(2)\n",
    "bigramModel.train(train_sentences)\n",
    "print(\"The bigram language model has been successfully built!\")\n",
    "# trigram language model\n",
    "trigramModel = NGramModels(3)\n",
    "trigramModel.train(train_sentences)\n",
    "print(\"The trigram language model has been successfully built!\")\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb6edc5-78c5-4de0-8855-2a455d625c97",
   "metadata": {},
   "source": [
    "> 2) Report the perplexity of these 3 trained models on the testing dataset and explain your findings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acda9a7-606c-4fa6-918a-9ebe7745ad42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the test sentences from `cleaned_test_data_list`\n",
    "test_sentences = []\n",
    "for each in cleaned_test_data_list:\n",
    "    test_sentences.extend(each[\"sentences\"]) \n",
    "\n",
    "# compute the perplexity of the test dataset\n",
    "u_perp = unigramModel.perplexity(test_sentences)\n",
    "b_perp = bigramModel.perplexity(test_sentences)\n",
    "t_perp = trigramModel.perplexity(test_sentences)\n",
    "\n",
    "print(\"The perplexity of the testing dataset in unigram language model is {:>7.2f}\".format(round(u_perp, 2)))\n",
    "print(\"The perplexity of the testing dataset in bigram language model is {:>7.2f}\".format(round(b_perp, 2)))\n",
    "print(\"The perplexity of the testing dataset in trigram language model is {:>7.2f}\".format(round(t_perp, 2)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd75b35-134d-4629-ac87-ed84fcc9d8e4",
   "metadata": {},
   "source": [
    "> 3) Use each built model to generate five sentences and explain these generated patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6e1244-2b15-4845-a83b-5708797569b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sentence = 5\n",
    "for i, model in enumerate([unigramModel, bigramModel, trigramModel]):\n",
    "    print(\"-\" * 50)\n",
    "    print(\"For the {}-gram language model:\".format(i+1))\n",
    "    for sentence, prob in model.generate(num_sentence):\n",
    "        print(\"{} ({:.5f})\".format(sentence, prob))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1b4282-d67c-4543-88cf-1bd0518173c2",
   "metadata": {},
   "source": [
    "## Task3 - Build NB/LR classifiers\n",
    "\n",
    "> 1) Build a Naive Bayes classifier (with Laplace smoothing) and test your model on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9cdf1d-8147-4033-bbb0-9147f3647c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes to here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca80a4a-9db5-426f-a45a-b1c2c1e34423",
   "metadata": {},
   "source": [
    "> 2) Build a LR classifier. This question seems to be challenging. We did not directly provide features for samples. But just use your own method to build useful features. You may need to split the training dataset into train and validation so that some involved parameters can be tuned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae83301a-0529-4dc9-899b-504324f9a547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes to here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53a8b62-1ef4-4c2f-934d-f78551ad039e",
   "metadata": {},
   "source": [
    "> 3) Report Micro-F1 score and Macro-F1 score for these classifiers on testing dataset explain our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500abc8d-0e08-4a84-af6c-9d0e1e0fa035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes to here\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
