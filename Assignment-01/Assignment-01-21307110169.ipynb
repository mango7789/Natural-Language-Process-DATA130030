{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7186371d-a854-4609-8158-8537db05c369",
   "metadata": {},
   "source": [
    "## Instruction\n",
    "\n",
    "> 1. Rename Assignment-01-###.ipynb where ### is your student ID.\n",
    "> 2. The deadline of Assignment-01 is 23:59pm, 03-31-2024\n",
    ">\n",
    "> 3. In this assignment, you will\n",
    ">    1) explore Wikipedia text data\n",
    ">    2) build language models\n",
    ">    3) build NB and LR classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c23f2dd-e01e-4b02-9b17-c92885f8a428",
   "metadata": {},
   "source": [
    "## Task0 - Download datasets\n",
    "> Download the preprocessed data, enwiki-train.json and enwiki-test.json from the Assignment-01 folder. In the data file, each line contains a Wikipedia page with attributes, title, label, and text. There are 1000 records in the train file and 100 records in test file with ten categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cc7cb5-dd64-4886-8cd5-24f147288941",
   "metadata": {},
   "source": [
    "## Task1 - Data exploring and preprocessing\n",
    "\n",
    "> 1) Print out how many documents are in each class  (for both train and test dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18ba34ae-48ae-438a-b3dd-c9d4cb3ee416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of documents in each class for train dataset is: \n",
      "\n",
      "There are 100 documents in class       Film\n",
      "There are 100 documents in class       Book\n",
      "There are 100 documents in class Politician\n",
      "There are 100 documents in class     Writer\n",
      "There are 100 documents in class       Food\n",
      "There are  70 documents in class      Actor\n",
      "There are  80 documents in class     Animal\n",
      "There are 130 documents in class   Software\n",
      "There are 100 documents in class     Artist\n",
      "There are 120 documents in class    Disease\n",
      "------------------------------------------------------------\n",
      "The number of documents in each class for test dataset is: \n",
      "\n",
      "There are  10 documents in class       Film\n",
      "There are  10 documents in class       Book\n",
      "There are  10 documents in class Politician\n",
      "There are  10 documents in class     Writer\n",
      "There are  10 documents in class       Food\n",
      "There are  10 documents in class      Actor\n",
      "There are  10 documents in class     Animal\n",
      "There are  10 documents in class   Software\n",
      "There are  10 documents in class     Artist\n",
      "There are  10 documents in class    Disease\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from typing import Callable\n",
    "\n",
    "################################################################ \n",
    "###         define the function we need for later use        ###\n",
    "################################################################\n",
    "\n",
    "def load_json(file_path: str) -> list:\n",
    "    \"\"\"\n",
    "    Fetch the data from `.json` file and concat them into a list.\n",
    "\n",
    "    Input:\n",
    "    - file_path: The relative file path of the `.json` file\n",
    "\n",
    "    Returns:\n",
    "    - join_data_list: A list containing the data, with the format of [{'title':<>, 'label':<>, 'text':<>}, {}, ...]\n",
    "    \"\"\"\n",
    "    join_data_list = []\n",
    "    with open(file_path, \"r\") as json_file:\n",
    "        for line in json_file:\n",
    "            line = line.strip()\n",
    "            # guaranteen the line is not empty\n",
    "            if line: \n",
    "                join_data_list.append(json.loads(line))\n",
    "    return join_data_list\n",
    "\n",
    "def iterate_line_in_list(data_list: list, f: Callable) -> dict:\n",
    "    \"\"\"\n",
    "    Iterate the `data_list` while recording the class.\n",
    "\n",
    "    Input:\n",
    "    - data_list: A list containing (train/test) data, with the format of [{'title':<>, 'label':<>, 'text':<>}, {}, ...]\n",
    "    - type: The type of the data, default is \"train\". Can take the value of \"train\" or \"test\"\n",
    "    - f: A function to compute the number of documents, sentences e.t.c. in each `line`\n",
    "\n",
    "    Output:\n",
    "    - class_dict: A list containing dictionaries with (key, value) as (<class>, <number_of_documents>)\n",
    "    \"\"\"\n",
    "    class_dict = {}\n",
    "    for line in data_list:\n",
    "        line_class = line['label']\n",
    "        class_dict[line_class] = class_dict.get(line_class, 0) + f(line['text'])  # if the class doesn't exist, set the value as 0\n",
    "    return class_dict\n",
    "\n",
    "################################################################ \n",
    "###                        end define                        ###\n",
    "################################################################\n",
    "\n",
    "def count_docs(text):\n",
    "    return 1\n",
    "\n",
    "def print_docs_in_class(class_dict: dict, type: str = \"train\") -> None:\n",
    "    print(\"The number of documents in each class for \" + type + \" dataset is: \\n\")\n",
    "    for _class, _times in class_dict.items():\n",
    "        print(\"There are {:>3} documents in class {:>10}\".format(_times, _class))\n",
    "    print('-'*60)\n",
    "\n",
    "\n",
    "# Fetch data from the json file\n",
    "    \n",
    "train_file_path, test_file_path = \"enwiki-train.json\", \"enwiki-test.json\"\n",
    "train_data_list, test_data_list = map(load_json, [train_file_path, test_file_path])\n",
    "\n",
    "# print out the number of documents of each class in train and test dataset\n",
    "\n",
    "train_docs_num = iterate_line_in_list(train_data_list, count_docs)\n",
    "test_docs_num = iterate_line_in_list(test_data_list, count_docs)\n",
    "\n",
    "print_docs_in_class(train_docs_num)\n",
    "print_docs_in_class(test_docs_num, \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1a2d5c-b719-41ec-8f14-04e092517eb9",
   "metadata": {},
   "source": [
    "> 2) Print out the average number of sentences in each class.\n",
    ">    You may need to use sentence tokenization of NLTK.\n",
    ">    (for both train and test dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37e85dc7-d50b-406a-9550-b2063f236ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average number of sentences in each class for train dataset is: \n",
      "\n",
      "There are average  438.56 sentences in class       Film\n",
      "There are average  400.36 sentences in class       Book\n",
      "There are average  706.20 sentences in class Politician\n",
      "There are average  420.32 sentences in class     Writer\n",
      "There are average  175.24 sentences in class       Food\n",
      "There are average   76.70 sentences in class      Actor\n",
      "There are average   70.38 sentences in class     Animal\n",
      "There are average  260.95 sentences in class   Software\n",
      "There are average  306.47 sentences in class     Artist\n",
      "There are average  404.90 sentences in class    Disease\n",
      "------------------------------------------------------------\n",
      "The average number of sentences in each class for test dataset is: \n",
      "\n",
      "There are average  364.70 sentences in class       Film\n",
      "There are average  295.90 sentences in class       Book\n",
      "There are average  597.60 sentences in class Politician\n",
      "There are average  294.90 sentences in class     Writer\n",
      "There are average  107.60 sentences in class       Food\n",
      "There are average   30.70 sentences in class      Actor\n",
      "There are average   46.80 sentences in class     Animal\n",
      "There are average  160.10 sentences in class   Software\n",
      "There are average  234.00 sentences in class     Artist\n",
      "There are average  311.70 sentences in class    Disease\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def count_sents(text):\n",
    "    return len(sent_tokenize(text))\n",
    "\n",
    "def print_ave_sents_in_class(class_dict: dict, type: str = \"train\"):\n",
    "    # get the dict of number of documents in each class based on the input type\n",
    "    if type == \"train\":\n",
    "        docs_num_class = train_docs_num\n",
    "    elif type == \"test\":\n",
    "        docs_num_class = test_docs_num\n",
    "    else:\n",
    "        raise TypeError\n",
    "    \n",
    "    # print the result\n",
    "    print(\"The average number of sentences in each class for \" + type + \" dataset is: \\n\")\n",
    "    for _class, _times in class_dict.items():\n",
    "        print(\"There are average {:>7.2f} sentences in class {:>10}\".format(_times / docs_num_class[_class], _class))\n",
    "    print('-'*60)\n",
    "\n",
    "\n",
    "train_ave_sents = iterate_line_in_list(train_data_list, count_sents)\n",
    "test_ave_sents = iterate_line_in_list(test_data_list, count_sents)\n",
    "\n",
    "print_ave_sents_in_class(train_ave_sents)\n",
    "print_ave_sents_in_class(test_ave_sents, \"test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502ef409-65a5-4d07-9b69-c5986569970a",
   "metadata": {},
   "source": [
    "> 3) Print out the average number of tokens in each class\n",
    ">    (for both train and test dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4d7628e-762c-49fe-804a-796fa0265af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average number of tokens in each class for train dataset is: \n",
      "\n",
      "There are average 11895.28 tokens in class       Film\n",
      "There are average 10540.51 tokens in class       Book\n",
      "There are average 18644.30 tokens in class Politician\n",
      "There are average 11849.91 tokens in class     Writer\n",
      "There are average  3904.15 tokens in class       Food\n",
      "There are average  1868.84 tokens in class      Actor\n",
      "There are average  1521.92 tokens in class     Animal\n",
      "There are average  6302.30 tokens in class   Software\n",
      "There are average  8212.91 tokens in class     Artist\n",
      "There are average  9322.96 tokens in class    Disease\n",
      "------------------------------------------------------------\n",
      "The average number of tokens in each class for test dataset is: \n",
      "\n",
      "There are average  9292.90 tokens in class       Film\n",
      "There are average  7711.10 tokens in class       Book\n",
      "There are average 15204.30 tokens in class Politician\n",
      "There are average  8499.40 tokens in class     Writer\n",
      "There are average  2445.50 tokens in class       Food\n",
      "There are average   677.50 tokens in class      Actor\n",
      "There are average   885.60 tokens in class     Animal\n",
      "There are average  3972.80 tokens in class   Software\n",
      "There are average  5706.40 tokens in class     Artist\n",
      "There are average  6988.80 tokens in class    Disease\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def count_tokens(text):\n",
    "    return len(word_tokenize(text))\n",
    "\n",
    "def print_ave_tokens_in_class(class_dict: dict, type: str = \"train\"):\n",
    "    # get the dict of number of documents in each class based on the input type\n",
    "    if type == \"train\":\n",
    "        docs_num_class = train_docs_num\n",
    "    elif type == \"test\":\n",
    "        docs_num_class = test_docs_num\n",
    "    else:\n",
    "        raise TypeError\n",
    "    \n",
    "    # print the result\n",
    "    print(\"The average number of tokens in each class for \" + type + \" dataset is: \\n\")\n",
    "    for _class, _times in class_dict.items():\n",
    "        print(\"There are average {:>8.2f} tokens in class {:>10}\".format(_times / docs_num_class[_class], _class))\n",
    "    print('-'*60)\n",
    "\n",
    "train_ave_tokens = iterate_line_in_list(train_data_list, count_tokens)\n",
    "test_ave_tokens = iterate_line_in_list(test_data_list, count_tokens)\n",
    "\n",
    "print_ave_tokens_in_class(train_ave_tokens)\n",
    "print_ave_tokens_in_class(test_ave_tokens, \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d999a66-977c-469b-939c-c3934210972e",
   "metadata": {},
   "source": [
    "> 4) For each sentence in the document, remove punctuations and other special characters so that each sentence only contains English words and numbers. To make your life easier, you can make all words as lower cases. For each class, print out the first article's name and the processed first 40 words. (for both train and test dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "819a28ef-0821-4ace-be70-fb9b249d355b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result of the train data list:\n",
      "\n",
      "The first article's name of class       Film is         Citizen_Kane\n",
      "The cleaned text is: [Citizen Kane is a 1941 American drama fi] ==> [citizen kane is a 1941 american drama fi]\n",
      "\n",
      "The first article's name of class       Book is The_Spirit_of_the_Age\n",
      "The cleaned text is: [The Spirit of the Age (full title \"The S] ==> [the spirit of the age full title the spi]\n",
      "\n",
      "The first article's name of class Politician is    Charles_de_Gaulle\n",
      "The cleaned text is: [Charles André Joseph Marie de Gaulle (; ] ==> [charles andr joseph marie de gaulle 22 n]\n",
      "\n",
      "The first article's name of class     Writer is        Mircea_Eliade\n",
      "The cleaned text is: [Mircea Eliade (; – April 22, 1986) was a] ==> [mircea eliade april 22 1986 was a romani]\n",
      "\n",
      "The first article's name of class       Food is       Korean_cuisine\n",
      "The cleaned text is: [ \n",
      "Korean cuisine has evolved through cen] ==> [korean cuisine has evolved through centu]\n",
      "\n",
      "The first article's name of class      Actor is       Roman_Polanski\n",
      "The cleaned text is: [Roman Polanski ( ; ; born Raymond Thierr] ==> [roman polanski born raymond thierry lieb]\n",
      "\n",
      "The first article's name of class     Animal is      Oesophagostomum\n",
      "The cleaned text is: [Oesophagostomum is a genus of parasitic ] ==> [oesophagostomum is a genus of parasitic ]\n",
      "\n",
      "The first article's name of class   Software is Android_(operating_system)\n",
      "The cleaned text is: [Android is a mobile operating system bas] ==> [android is a mobile operating system bas]\n",
      "\n",
      "The first article's name of class     Artist is           Mihai_Olos\n",
      "The cleaned text is: [Mihai Olos (born 26 February 1940 in Ari] ==> [mihai olos born 26 february 1940 in arin]\n",
      "\n",
      "The first article's name of class    Disease is    Domestic_violence\n",
      "The cleaned text is: [Domestic violence (also called domestic ] ==> [domestic violence also called domestic a]\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "The result of the test data list:\n",
      "\n",
      "The first article's name of class       Film is Monty_Python's_Life_of_Brian\n",
      "The cleaned text is: [Monty Python's Life of Brian, also known] ==> [monty pythons life of brian also known a]\n",
      "\n",
      "The first article's name of class       Book is         Cousin_Bette\n",
      "The cleaned text is: [La Cousine Bette (, \"Cousin Bette\") is a] ==> [la cousine bette cousin bette is an 1846]\n",
      "\n",
      "The first article's name of class Politician is    Olusegun_Obasanjo\n",
      "The cleaned text is: [Chief Olusegun Matthew Okikiola Aremu Ob] ==> [chief olusegun matthew okikiola aremu ob]\n",
      "\n",
      "The first article's name of class     Writer is         Horia_Gârbea\n",
      "The cleaned text is: [Horia-Răzvan Gârbea or Gîrbea (; born Au] ==> [horiarzvan grbea or grbea born august 10]\n",
      "\n",
      "The first article's name of class       Food is          Sponge_cake\n",
      "The cleaned text is: [Sponge cake is a light cake made with eg] ==> [sponge cake is a light cake made with eg]\n",
      "\n",
      "The first article's name of class      Actor is       Kom_Chuanchuen\n",
      "The cleaned text is: [Akom Preedakul (, , ; 5 January 1958 – 3] ==> [akom preedakul 5 january 1958 30 april 2]\n",
      "\n",
      "The first article's name of class     Animal is Articulata_hypothesis\n",
      "The cleaned text is: [The Articulata hypothesis is the groupin] ==> [the articulata hypothesis is the groupin]\n",
      "\n",
      "The first article's name of class   Software is                 Unix\n",
      "The cleaned text is: [Unix (; trademarked as UNIX) is a family] ==> [unix trademarked as unix is a family of ]\n",
      "\n",
      "The first article's name of class     Artist is     Camille_Pissarro\n",
      "The cleaned text is: [Camille Pissarro ( , ; 10 July 1830 – 13] ==> [camille pissarro 10 july 1830 13 novembe]\n",
      "\n",
      "The first article's name of class    Disease is Staphylococcus_aureus\n",
      "The cleaned text is: [Staphylococcus aureus is a Gram-positive] ==> [staphylococcus aureus is a grampositive ]\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from copy import deepcopy\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def clean_doc(document: str) -> list:\n",
    "    document = document.lower()\n",
    "    cleaned_document = []\n",
    "    sentences = sent_tokenize(document)\n",
    "    for sentence in sentences:\n",
    "        # remove punctuations and special characters\n",
    "        sentence = re.sub(r'[^a-zA-Z0-9\\s]', '', sentence)\n",
    "        # remove extra whitespaces\n",
    "        sentence = re.sub(r'\\s+', ' ', sentence).strip()\n",
    "        cleaned_document.append(sentence)\n",
    "    return cleaned_document\n",
    "\n",
    "def process_data_list(data_list: list, type: str = \"train\") -> list:\n",
    "    explored = []\n",
    "    print(\"The result of the \" + type + \" data list:\")\n",
    "    # process the data_list\n",
    "    for line in data_list:\n",
    "        class_label = line[\"label\"]\n",
    "        former_line_text = line[\"text\"]                         # former text\n",
    "        line[\"sentences\"] = clean_doc(line[\"text\"])             # cleaned sentences list\n",
    "        line[\"text\"] = \". \".join(line[\"sentences\"]) + \".\"       # join the sentence list with \". \" to generate the processed text\n",
    "        if class_label not in explored:\n",
    "            explored.append(class_label)\n",
    "            # print the result\n",
    "            print()\n",
    "            print(\"The first article's name of class {:>10} is {:>20}\".format(class_label, line[\"title\"]))\n",
    "            print(\"The cleaned text is: [{}] ==> [{}]\".format(former_line_text[:40], line[\"text\"][:40]))\n",
    "    print(\"-\"*120)\n",
    "    return data_list\n",
    "\n",
    "# make a deepcopy of the origin data list to avoid over-write\n",
    "cleaned_train_data_list = deepcopy(train_data_list)\n",
    "cleaned_test_data_list = deepcopy(test_data_list)\n",
    "\n",
    "# process the copyed data list in place by `process_data_list`\n",
    "cleaned_train_data_list = process_data_list(cleaned_train_data_list)\n",
    "cleaned_test_data_list = process_data_list(cleaned_test_data_list, \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6c7ea9-18fa-43f1-bded-625c09a947b1",
   "metadata": {},
   "source": [
    "## Task2 - Build language models\n",
    "\n",
    "> 1) Based on the training dataset, build unigram, bigram, and trigram language models using Add-one smoothing technique. It is encouraged to implement models by yourself. If you use public code, please cite it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0e979b1-235f-4663-8acc-00257735e209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The unigram language model has been successfully built!\n",
      "The  bigram language model has been successfully built!\n",
      "The trigram language model has been successfully built!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from itertools import product\n",
    "import math\n",
    "\n",
    "'''\n",
    "The framework of the class `NGramModels` follows from the repo \"https://github.com/joshualoehr/ngram-language-model\" with some \\\n",
    "modification to fit into the task 1.\n",
    "'''\n",
    "\n",
    "class NGramModels(object):\n",
    "    def __init__(self, n, laplace=1) -> None:\n",
    "        self.n = n\n",
    "        self.laplace = laplace\n",
    "        self._model = None\n",
    "        self._tokens = None\n",
    "        self._vocab = None\n",
    "        self._masks = list(reversed(list(product((0,1), repeat=n))))\n",
    "    \n",
    "    def _preprocess(self, sentences: list) -> list:\n",
    "        \"\"\"\n",
    "        Preprocess the raw text by adding (n-1)*\"<s>\" (or one single <s>) on the front of the sentence \n",
    "        and replacing the tokens which occur only once with \"<UNK>\".\n",
    "\n",
    "        Input:\n",
    "        - sentences: A list with each element as a `sent_tokenized` sentence.\n",
    "\n",
    "        Return:\n",
    "        - tokens: A list containing the processed tokens\n",
    "        \"\"\"\n",
    "        sos = \"<s> \" * (self.n - 1) if self.n > 1 else \"<s> \"\n",
    "        tokenized_sentences = ['{}{} {}'.format(sos, sent, \"</s>\").split() for sent in sentences]\n",
    "        tokenized_sentences = [token for sublist in tokenized_sentences for token in sublist]  # flatten\n",
    "        # Replace tokens which appear only once in the corpus with <UNK>\n",
    "        vocab = nltk.FreqDist(tokenized_sentences)\n",
    "        tokens = [token if vocab[token] > 1 else \"<UNK>\" for token in tokenized_sentences]\n",
    "        return tokens\n",
    "    \n",
    "    def _smooth(self) -> dict:\n",
    "        \"\"\"\n",
    "        Smooth the frequency distribution based on Laplace smoothing.\n",
    "\n",
    "        Return:\n",
    "        - A dictionary {<ngram>: <count>, ...} containing the information of the frequency distribution\n",
    "        \"\"\"\n",
    "        vocab_size = len(self._vocab)\n",
    "        if self.n == 1:         # if n equals 1, we don't need to smooth it\n",
    "            num_tokens = len(self._tokens)\n",
    "            return {(unigram,): count / num_tokens for unigram, count in self._vocab.items()}\n",
    "        else:\n",
    "            n_grams = nltk.ngrams(self._tokens, self.n)\n",
    "            n_vocab = nltk.FreqDist(n_grams)\n",
    "            n_minus_one_grams = nltk.ngrams(self._tokens, self.n-1)\n",
    "            n_minus_one_vocab = nltk.FreqDist(n_minus_one_grams)\n",
    "            return {ngram: (n_freq + self.laplace) / (n_minus_one_vocab[ngram[:-1]] + self.laplace * vocab_size) for ngram, n_freq in n_vocab.items()}\n",
    "\n",
    "    def train(self, sentences: list) -> None:\n",
    "        \"\"\"\n",
    "        Train the model based on the given raw text.\n",
    "\n",
    "        Input:\n",
    "        - sentences: A list with each element as a `sent_tokenized` sentence.\n",
    "        \"\"\"\n",
    "        tokens = self._preprocess(sentences)\n",
    "        self._tokens = tokens\n",
    "        self._vocab = nltk.FreqDist(self._tokens)\n",
    "        self._model = self._smooth()\n",
    "\n",
    "    def _find_match(self, ngram: tuple) -> str:\n",
    "        \"\"\"\n",
    "        Find the best match of the given ngram token in the trained model by masking the ngram in iteration\n",
    "\n",
    "        Input: \n",
    "        - ngram: A tuple representing a test ngram token\n",
    "\n",
    "        Return:\n",
    "        - tokens: The best match of the ngram in the trained model\n",
    "        \"\"\"\n",
    "        mask = lambda ngram, bitmask: tuple((token if flag == 1 else \"<UNK>\" for token, flag in zip(ngram, bitmask)))\n",
    "        possible_tokens = [mask(ngram, bitmask) for bitmask in self._masks]\n",
    "        for tokens in possible_tokens:\n",
    "            if tokens in self._model:\n",
    "                return tokens\n",
    "\n",
    "    def perplexity(self, test_sentences: list) -> float:\n",
    "        \"\"\"\n",
    "        Compute the perplexity of the given `test_sentences` based on the train tokens.\n",
    "\n",
    "        Input:\n",
    "        - test_sentences: A list containing the test sentences\n",
    "        \n",
    "        Return:\n",
    "        - perplexity: The perplexity of the test material, computed by the geomteric mean of the\n",
    "                      log probabilities. \n",
    "        \"\"\"\n",
    "        test_tokens = self._preprocess(test_sentences)\n",
    "        test_ngrams = nltk.ngrams(test_tokens, self.n)\n",
    "        known_ngrams  = (self._find_match((ngram,)) if isinstance(ngram, str) else self._find_match(ngram) for ngram in test_ngrams)\n",
    "        probabilities = [self._model[ngram] for ngram in known_ngrams]\n",
    "\n",
    "        return math.exp((-1 / len(test_tokens)) * sum(map(math.log, probabilities)))\n",
    "\n",
    "    def _best_candidate(self, prev: tuple, i: int, blacklist: list=[]) -> tuple:\n",
    "        \"\"\"\n",
    "        Find the best candidate from the trained model based on the previous text and blacklist.\n",
    "\n",
    "        Input:\n",
    "        - prev: A tuple containing the information of the previous text \n",
    "        - i: current index\n",
    "        - blacklist: A list of values that can't be taken\n",
    "\n",
    "        Return:\n",
    "        - candidate: A tuple with the format (<candidate_token>, <prob>)\n",
    "        \"\"\"\n",
    "        blacklist += [\"<UNK>\"]\n",
    "        candidates = [(ngram[-1], prob) for ngram, prob in self._model.items() if ngram[:-1] == prev] # find the candidates based on the trained moel\n",
    "        candidates = [candidate for candidate in candidates if candidate[0] not in blacklist]         # filter out the candidate in blacklist\n",
    "        candidates = sorted(candidates, key=lambda candidate: candidate[1], reverse=True)             # sort the candidates based on the prob\n",
    "        if len(candidates) == 0:\n",
    "            return (\"</s>\", 1)\n",
    "        return candidates[0 if prev != () and prev[-1] != \"<s>\" else i]\n",
    "\n",
    "    def generate(self, num: int, min_len: int=12, max_len: int=24):\n",
    "        \"\"\"\n",
    "        Generate sentences based on the trained model for given number of sentences, minimum length and maximun length\n",
    "\n",
    "        Input:\n",
    "        - num: The number of sentences we need to generate\n",
    "        - min_len: The minmum length of the generated sentence\n",
    "        - max_len: The maximum length of the generated sentence\n",
    "\n",
    "        Return (Yield):\n",
    "        - The generated sentence one by one\n",
    "        \"\"\"\n",
    "        for i in range(num):\n",
    "            sent, prob = [\"<s>\"] * max(1, self.n - 1), 1\n",
    "            while sent[-1] != \"</s>\":\n",
    "                prev = () if self.n == 1 else tuple(sent[-(self.n-1):])\n",
    "                blacklist = sent + ([\"</s>\"] if len(sent) < min_len else [])\n",
    "                next_token, next_prob = self._best_candidate(prev, i, blacklist)\n",
    "                sent.append(next_token)\n",
    "                prob *= next_prob\n",
    "                \n",
    "                if len(sent) >= max_len:\n",
    "                    sent.append(\"</s>\")\n",
    "\n",
    "            yield ' '.join(sent), -1/math.log(prob)\n",
    "\n",
    "# generate the train sentences from `cleaned_train_data_list`\n",
    "train_sentences = []\n",
    "for each in cleaned_train_data_list:\n",
    "    train_sentences.extend(each[\"sentences\"]) \n",
    "\n",
    "# unigram language model\n",
    "unigramModel = NGramModels(1)\n",
    "unigramModel.train(train_sentences)\n",
    "print(\"The unigram language model has been successfully built!\")\n",
    "# bigram language model\n",
    "bigramModel = NGramModels(2)\n",
    "bigramModel.train(train_sentences)\n",
    "print(\"The  bigram language model has been successfully built!\")\n",
    "# trigram language model\n",
    "trigramModel = NGramModels(3)\n",
    "trigramModel.train(train_sentences)\n",
    "print(\"The trigram language model has been successfully built!\")\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb6edc5-78c5-4de0-8855-2a455d625c97",
   "metadata": {},
   "source": [
    "> 2) Report the perplexity of these 3 trained models on the testing dataset and explain your findings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3acda9a7-606c-4fa6-918a-9ebe7745ad42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The perplexity of the testing dataset in unigram language model is 1155.88\n",
      "The perplexity of the testing dataset in  bigram language model is 1587.09\n",
      "The perplexity of the testing dataset in trigram language model is 3859.54\n"
     ]
    }
   ],
   "source": [
    "# generate the test sentences from `cleaned_test_data_list`\n",
    "test_sentences = []\n",
    "for each in cleaned_test_data_list:\n",
    "    test_sentences.extend(each[\"sentences\"]) \n",
    "\n",
    "# compute the perplexity of the test dataset\n",
    "u_perp = unigramModel.perplexity(test_sentences)\n",
    "b_perp = bigramModel.perplexity(test_sentences)\n",
    "t_perp = trigramModel.perplexity(test_sentences)\n",
    "\n",
    "print(\"The perplexity of the testing dataset in unigram language model is {:>7.2f}\".format(round(u_perp, 2)))\n",
    "print(\"The perplexity of the testing dataset in  bigram language model is {:>7.2f}\".format(round(b_perp, 2)))\n",
    "print(\"The perplexity of the testing dataset in trigram language model is {:>7.2f}\".format(round(t_perp, 2)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b262c9e3",
   "metadata": {},
   "source": [
    "- `Unigram Model` (Perplexity: 1155.88):\n",
    "  - The unigram model, despite its simplicity, achieves a relatively low perplexity. This suggests that even with minimal context (each word considered independently), the model is able to capture some of the underlying patterns within the Wikipedia text data across the 10 classes. **This could be due to the wide variety of topics covered by Wikipedia, allowing for some generalization even with a unigram model.**\n",
    "- `Bigram Model` (Perplexity: 1587.09):\n",
    "  - The bigram model, which considers the previous word as context, exhibits a higher perplexity compared to the unigram model. However, it still performs reasonably well, indicating that incorporating some contextual information improves prediction accuracy. The performance of the bigram model suggests that **there are significant dependencies between adjacent words within the Wikipedia text data**, contributing to the lower perplexity compared to the trigram model.\n",
    "- `Trigram Model` (Perplexity: 3859.54):\n",
    "  - The trigram model, with the highest perplexity among the three models, struggles more with accurately predicting the next word in the testing dataset. Despite considering two preceding words as context, the model's performance is not as effective as expected. **This could be due to data sparsity issues, especially considering the relatively small training sample size (1000) and the wide range of topics covered by Wikipedia across the 10 classes**. The trigram model might encounter challenges in capturing sufficient instances of trigrams within each class, leading to higher perplexity.\n",
    "- In summary, while the unigram and bigram models demonstrate reasonable performance in capturing language patterns within the Wikipedia text data across the 10 classes, the trigram model's performance is comparatively weaker, possibly due to data sparsity and the complexity of capturing trigram dependencies within each class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd75b35-134d-4629-ac87-ed84fcc9d8e4",
   "metadata": {},
   "source": [
    "> 3) Use each built model to generate five sentences and explain these generated patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea6e1244-2b15-4845-a83b-5708797569b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "For the unigram language model:\n",
      "<s> the of and in to a was that as for with </s> (0.02075)\n",
      "<s> of and in to a was that as for with his </s> (0.01990)\n",
      "<s> and in to a was that as for with his is of he on by it from an at be which had or </s> (0.00895)\n",
      "<s> in to a was that as for with his is he and on by it from an at be which had or are </s> (0.00877)\n",
      "<s> to a was that as for with his is he on in by it from an at be which had or are not </s> (0.00860)\n",
      "------------------------------------------------------------\n",
      "For the bigram language model:\n",
      "<s> the film was a new york city of his own and in which he had been found that it is not be used </s> (0.00987)\n",
      "<s> in the film was a new york city of his own </s> (0.01976)\n",
      "<s> he was a new york city of the film and his own </s> (0.01801)\n",
      "<s> it was a new york city of the film and his own </s> (0.01782)\n",
      "<s> this is a new york city of the film was not be used to his own </s> (0.01341)\n",
      "------------------------------------------------------------\n",
      "For the trigram language model:\n",
      "<s> <s> the film was released on october 31 2014 a new version of windows 8 and 9 to 10 times more likely than </s> (0.00579)\n",
      "<s> <s> in the united states and canada on november 22 1963 he was a member of parliament </s> (0.00868)\n",
      "<s> <s> he was a member of the film and television arts bafta awards for best actor in his own </s> (0.00734)\n",
      "<s> <s> it is not a single day of the film was released on october 31 2014 and in his own </s> (0.00714)\n",
      "<s> <s> this is the most common cause of death in a letter to his own </s> (0.01020)\n"
     ]
    }
   ],
   "source": [
    "num_sentence = 5\n",
    "\n",
    "model_dict = {\"unigram\": unigramModel, \"bigram\": bigramModel, \"trigram\": trigramModel}\n",
    "for _name, _model in model_dict.items():\n",
    "    print(\"-\" * 60)\n",
    "    print(\"For the {} language model:\".format(_name))\n",
    "    for sentence, prob in _model.generate(num_sentence):\n",
    "        print(\"{} ({:.5f})\".format(sentence, prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e287bc9",
   "metadata": {},
   "source": [
    "- `Unigram`:\n",
    "  - Analysis of the generated sentences reveals a dominance of high-frequency words such as \"the,\" \"of,\" and \"as,\" among others.\n",
    "  - These sentences lack coherence and can be considered mere concatenations of single words.\n",
    "  - Minimal semantic relationships exist between words at different positions within each sentence.\n",
    "- `Bigram`:\n",
    "  - Notably, phrases like \"new york city,\" \"the film,\" and \"his own\" recur in all five sentences, indicating the model's ability to learn two-word combinations from the training dataset.\n",
    "  - While there is an improvement from the unigram model, the sentences generated by the bigram model still exhibit some oddness.\n",
    "- `Trigram`:\n",
    "  - Sentences generated by the trigram model demonstrate improved coherence and semblance of meaning.\n",
    "  - However, the average probabilities of these sentences are comparatively lower, reflecting a trade-off between mirroring the training data and generalizing the language model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1b4282-d67c-4543-88cf-1bd0518173c2",
   "metadata": {},
   "source": [
    "## Task3 - Build NB/LR classifiers\n",
    "\n",
    "> 1) Build a Naive Bayes classifier (with Laplace smoothing) and test your model on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f9cdf1d-8147-4033-bbb0-9147f3647c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train score of the Naive Bayes model with laplace smoothing is: 0.998000\n",
      "The  test score of the Naive Bayes model with laplace smoothing is: 0.920000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Actor       1.00      0.80      0.89        10\n",
      "      Animal       1.00      1.00      1.00        10\n",
      "      Artist       1.00      1.00      1.00        10\n",
      "        Book       1.00      0.70      0.82        10\n",
      "     Disease       1.00      1.00      1.00        10\n",
      "        Film       0.90      0.90      0.90        10\n",
      "        Food       1.00      1.00      1.00        10\n",
      "  Politician       0.71      1.00      0.83        10\n",
      "    Software       1.00      1.00      1.00        10\n",
      "      Writer       0.73      0.80      0.76        10\n",
      "\n",
      "    accuracy                           0.92       100\n",
      "   macro avg       0.93      0.92      0.92       100\n",
      "weighted avg       0.93      0.92      0.92       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "import numpy as np\n",
    "\n",
    "class LanguageNaiveBayes(object):\n",
    "    def __init__(self, laplace: int=1) -> None:\n",
    "        self._data_set = None\n",
    "        self._vocab = None\n",
    "        self._features = None\n",
    "        self._labels = None\n",
    "        self._model = None\n",
    "        self.laplace = laplace\n",
    "        self.stopwords = [\n",
    "            'a', 'about', 'above', 'across', 'after', 'afterwards', 'again', 'against', 'all', 'almost', 'alone',\n",
    "             'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'amoungst', 'amount',\n",
    "             'an', 'and', 'another', 'any', 'anyhow', 'anyone', 'anything', 'anyway', 'anywhere', 'are', 'around',\n",
    "             'as', 'at', 'back', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before',\n",
    "             'beforehand', 'behind', 'being', 'below', 'beside', 'besides', 'between', 'beyond', 'bill', 'both',\n",
    "             'bottom', 'but', 'by', 'call', 'can', 'cannot', 'cant', 'co', 'con', 'could', 'couldnt', 'cry', 'de',\n",
    "             'describe', 'detail', 'did', 'do', 'does', 'doing', 'don', 'done', 'down', 'due', 'during', 'each', 'eg',\n",
    "             'eight', 'either', 'eleven', 'else', 'elsewhere', 'empty', 'enough', 'etc', 'even', 'ever', 'every', 'everyone',\n",
    "             'everything', 'everywhere', 'except', 'few', 'fifteen', 'fify', 'fill', 'find', 'fire', 'first', 'five', 'for',\n",
    "             'former', 'formerly', 'forty', 'found', 'four', 'from', 'front', 'full', 'further', 'get', 'give', 'go', 'had',\n",
    "             'has', 'hasnt', 'have', 'having', 'he', 'hence', 'her', 'here', 'hereafter', 'hereby', 'herein', 'hereupon',\n",
    "             'hers', 'herself', 'him', 'himself', 'his', 'how', 'however', 'hundred', 'i', 'ie', 'if', 'in', 'inc', 'indeed',\n",
    "             'interest', 'into', 'is', 'it', 'its', 'itself', 'just', 'keep', 'last', 'latter', 'latterly', 'least', 'less',\n",
    "             'ltd', 'made', 'many', 'may', 'me', 'meanwhile', 'might', 'mill', 'mine', 'more', 'moreover', 'most', 'mostly',\n",
    "             'move', 'much', 'must', 'my', 'myself', 'name', 'namely', 'neither', 'never', 'nevertheless', 'next', 'nine',\n",
    "             'no', 'nobody', 'none', 'noone', 'nor', 'not', 'nothing', 'now', 'nowhere', 'of', 'off', 'often', 'on', 'once',\n",
    "             'one', 'only', 'onto', 'or', 'other', 'others', 'otherwise', 'our', 'ours', 'ourselves', 'out', 'over', 'own',\n",
    "             'part', 'per', 'perhaps', 'please', 'put', 'rather', 're', 's', 'same', 'see', 'seem', 'seemed', 'seeming',\n",
    "             'seems', 'serious', 'several', 'she', 'should', 'show', 'side', 'since', 'sincere', 'six', 'sixty', 'so', \n",
    "             'some', 'somehow', 'someone', 'something', 'sometime', 'sometimes', 'somewhere', 'still', 'such', 'system',\n",
    "             't', 'take', 'ten', 'than', 'that', 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'thence', 'there',\n",
    "             'thereafter', 'thereby', 'therefore', 'therein', 'thereupon', 'these', 'they', 'thickv', 'thin', 'third', 'this',\n",
    "             'those', 'though', 'three', 'through', 'throughout', 'thru', 'thus', 'to', 'together', 'too', 'top', 'toward',\n",
    "             'towards', 'twelve', 'twenty', 'two', 'un', 'under', 'until', 'up', 'upon', 'us', 'very', 'via', 'was', 'we',\n",
    "             'well', 'were', 'what', 'whatever', 'when', 'whence', 'whenever', 'where', 'whereafter', 'whereas', 'whereby',\n",
    "             'wherein', 'whereupon', 'wherever', 'whether', 'which', 'while', 'whither', 'who', 'whoever', 'whole', 'whom',\n",
    "             'whose', 'why', 'will', 'with', 'within', 'without', 'would', 'yet', 'you', 'your', 'yours', 'yourself',\n",
    "             'yourselves'\n",
    "        ]\n",
    "\n",
    "    def train(self, data_list: list, cut_freq: int=5):\n",
    "        self._vocab = self._bulid_vocab(data_list)\n",
    "        self._features = self._extract_features(cut_freq)\n",
    "        self._data_set, self._labels = self._convert_to_dataset(data_list)\n",
    "        self._model = MultinomialNB(alpha=self.laplace)\n",
    "        self._model.fit(self._data_set, self._labels)\n",
    "        return self._model.score(self._data_set, self._labels)\n",
    "\n",
    "    def test(self, data_list: list) -> tuple:\n",
    "        test_dataset, test_labels = self._convert_to_dataset(data_list)\n",
    "        test_pred = self._model.predict(test_dataset)\n",
    "        test_score = self._model.score(test_dataset, test_labels)\n",
    "        report = classification_report(test_labels, test_pred)\n",
    "        return test_score, report\n",
    "\n",
    "    def set_stopwords(self, stopwords: list[str]) -> None:\n",
    "        if isinstance(stopwords, list[str]):\n",
    "            self.stopwords = stopwords\n",
    "        else:\n",
    "            raise TypeError(\"The type of the stopwords should be List[str]\")\n",
    "        \n",
    "    def _bulid_vocab(self, data_list: list) -> dict:\n",
    "        \"\"\"\n",
    "        Build a vocabulary for words which have length greater than 2 and are not in stopwords.\n",
    "\n",
    "        Input:\n",
    "        - data_list: A list with the format of [{\"title\": <title>, \"label\": <label>, \"text\": <text>}, ...]\n",
    "\n",
    "        Return:\n",
    "        - vocab: A dictionary with the format of {\"word\": <count>, ...}, where each word has length greater than 2 and is not in stopwords\n",
    "        \"\"\"\n",
    "        vocab = {}\n",
    "        for i in range(len(data_list)):\n",
    "            sentence = data_list[i][\"text\"]\n",
    "            for word in sentence.split():\n",
    "                if len(word) > 2 and word not in self.stopwords:\n",
    "                    if word not in vocab:\n",
    "                        vocab[word] = 1\n",
    "                    else:\n",
    "                        vocab[word] += 1\n",
    "        return vocab\n",
    "\n",
    "    def _extract_features(self, cut_freq: int) -> dict:\n",
    "        \"\"\"\n",
    "        Extract features from the vocabulary, with the threshold `cut_freq` for frequency of a word.\n",
    "\n",
    "        Input:\n",
    "        - cut_freq: The cutting frequency of the occurrence times of a word\n",
    "\n",
    "        Return:\n",
    "        - features: A dict with (key, value) as the (<extracted_feature>, <index of the feature>) \n",
    "        \"\"\"\n",
    "        features = {}\n",
    "        count = 0\n",
    "        for key, value in self._vocab.items():\n",
    "            if value >= cut_freq:\n",
    "                features[key] = count\n",
    "                count += 1\n",
    "        return features\n",
    "\n",
    "    def _convert_to_dataset(self, data_list: list) -> tuple:\n",
    "        \"\"\"\n",
    "        Convert the `data_list` to `train_dataset` and `labels` which can be accepted by the `MultinomialNB()`\n",
    "\n",
    "        Input:\n",
    "        - data_list: A list with the format of [{\"title\": <title>, \"label\": <label>, \"text\": <text>}, ...]\n",
    "\n",
    "        Return:\n",
    "        - dataset: A 2-d numpy array with rows and columns as the index numbers and feature indexes respectively\n",
    "        - labels: The corresponding y label of the dataset\n",
    "        \"\"\"\n",
    "        data_length = len(data_list)\n",
    "        dataset = np.zeros((data_length, len(self._features)))\n",
    "        labels = [0] * data_length\n",
    "        for i in range(data_length):\n",
    "            word_list = [word for word in data_list[i][\"text\"].split()]\n",
    "            for word in word_list:\n",
    "                if word in self._features:\n",
    "                    dataset[i][self._features[word]] += 1\n",
    "            labels[i] = data_list[i][\"label\"]\n",
    "        return dataset, labels\n",
    "\n",
    "    def _f1_score(self, data_list: list) -> tuple:\n",
    "        \"\"\"\n",
    "        Return the micro-f1 and macro-f1 scores of the model in test dataset\n",
    "        \"\"\"\n",
    "        test_dataset, test_labels = self._convert_to_dataset(data_list)\n",
    "        micro_f1 = f1_score(test_labels, self._model.predict(test_dataset), average=\"micro\")\n",
    "        macro_f1 = f1_score(test_labels, self._model.predict(test_dataset), average=\"macro\")\n",
    "        return micro_f1, macro_f1\n",
    "\n",
    "LMNaiveBayes = LanguageNaiveBayes(1)\n",
    "train_score = LMNaiveBayes.train(cleaned_train_data_list)\n",
    "test_score, report = LMNaiveBayes.test(cleaned_test_data_list)\n",
    "\n",
    "print(\"The train score of the Naive Bayes model with laplace smoothing is: {:7.6f}\".format(train_score))\n",
    "print(\"The  test score of the Naive Bayes model with laplace smoothing is: {:7.6f}\\n\".format(test_score))\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca80a4a-9db5-426f-a45a-b1c2c1e34423",
   "metadata": {},
   "source": [
    "> 2) Build a LR classifier. This question seems to be challenging. We did not directly provide features for samples. But just use your own method to build useful features. You may need to split the training dataset into train and validation so that some involved parameters can be tuned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae83301a-0529-4dc9-899b-504324f9a547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "[CV 1/5] END ................tfidf__max_df=0.25;, score=0.900 total time=  13.0s\n",
      "[CV 2/5] END ................tfidf__max_df=0.25;, score=0.925 total time=  12.5s\n",
      "[CV 3/5] END ................tfidf__max_df=0.25;, score=0.885 total time=  13.0s\n",
      "[CV 4/5] END ................tfidf__max_df=0.25;, score=0.890 total time=  12.2s\n",
      "[CV 5/5] END ................tfidf__max_df=0.25;, score=0.895 total time=  12.0s\n",
      "[CV 1/5] END .................tfidf__max_df=0.5;, score=0.895 total time=  13.8s\n",
      "[CV 2/5] END .................tfidf__max_df=0.5;, score=0.955 total time=  12.6s\n",
      "[CV 3/5] END .................tfidf__max_df=0.5;, score=0.910 total time=  13.4s\n",
      "[CV 4/5] END .................tfidf__max_df=0.5;, score=0.900 total time=  12.8s\n",
      "[CV 5/5] END .................tfidf__max_df=0.5;, score=0.945 total time=  14.2s\n",
      "[CV 1/5] END ................tfidf__max_df=0.75;, score=0.890 total time=  12.4s\n",
      "[CV 2/5] END ................tfidf__max_df=0.75;, score=0.955 total time=  13.8s\n",
      "[CV 3/5] END ................tfidf__max_df=0.75;, score=0.920 total time=  13.6s\n",
      "[CV 4/5] END ................tfidf__max_df=0.75;, score=0.910 total time=  12.8s\n",
      "[CV 5/5] END ................tfidf__max_df=0.75;, score=0.930 total time=  14.0s\n",
      "\n",
      "The train score of the Logistic Regression model is: 0.996000\n",
      "The  test score of the Logistic Regression model is: 0.940000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Actor       1.00      0.90      0.95        10\n",
      "      Animal       1.00      1.00      1.00        10\n",
      "      Artist       1.00      1.00      1.00        10\n",
      "        Book       1.00      0.80      0.89        10\n",
      "     Disease       1.00      1.00      1.00        10\n",
      "        Film       0.90      0.90      0.90        10\n",
      "        Food       1.00      1.00      1.00        10\n",
      "  Politician       0.71      1.00      0.83        10\n",
      "    Software       1.00      1.00      1.00        10\n",
      "      Writer       0.89      0.80      0.84        10\n",
      "\n",
      "    accuracy                           0.94       100\n",
      "   macro avg       0.95      0.94      0.94       100\n",
      "weighted avg       0.95      0.94      0.94       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "class LanguageLogisticRegression(object):\n",
    "    def __init__(self) -> None:\n",
    "        self._data_set = None\n",
    "        self._labels = None\n",
    "        self._model = None\n",
    "\n",
    "    def train(self, data_list: list) -> None:\n",
    "        self._data_set, self._labels = self._convert_to_dataset(data_list)\n",
    "        # create the tfidf-lr pipeline, use cv to choose the best parameter\n",
    "        pipeline = Pipeline([\n",
    "            ('tfidf', TfidfVectorizer()),  \n",
    "            ('clf', LogisticRegression(max_iter=1000))  \n",
    "        ])\n",
    "        parameters = {\n",
    "            'tfidf__max_df': (0.25, 0.5, 0.75),\n",
    "        }\n",
    "        self._model = GridSearchCV(pipeline, parameters, cv=5, verbose=3)\n",
    "\n",
    "        self._model.fit(self._data_set, self._labels)\n",
    "        return self._model.score(self._data_set, self._labels)\n",
    "\n",
    "    def test(self, data_list: list) -> None:\n",
    "        test_dataset, test_labels = self._convert_to_dataset(data_list)\n",
    "        test_pred = self._model.predict(test_dataset)\n",
    "        test_score = self._model.score(test_dataset, test_labels)\n",
    "        report = classification_report(test_labels, test_pred)\n",
    "        return test_score, report\n",
    "\n",
    "    def _convert_to_dataset(self, data_list: list) -> tuple:\n",
    "        \"\"\"\n",
    "        Convert the `data_list` to `train_dataset` and `labels` which can be accepted by the `LogisticRegression()`\n",
    "\n",
    "        Input:\n",
    "        - data_list: A list with the format of [{\"title\": <title>, \"label\": <label>, \"text\": <text>}, ...]\n",
    "\n",
    "        Return:\n",
    "        - texts: A list with each element as <text>\n",
    "        - labels: The corresponding y label of the dataset\n",
    "        \"\"\"\n",
    "        texts, labels = [], []\n",
    "        for i in range(len(data_list)):\n",
    "            texts.append(data_list[i][\"text\"])\n",
    "            labels.append(data_list[i][\"label\"])\n",
    "        return texts, labels\n",
    "    \n",
    "    def _f1_score(self, data_list: list) -> tuple:\n",
    "        \"\"\"\n",
    "        Return the micro-f1 and macro-f1 scores of the model in test dataset\n",
    "        \"\"\"\n",
    "        test_dataset, test_labels = self._convert_to_dataset(data_list)\n",
    "        micro_f1 = f1_score(test_labels, self._model.predict(test_dataset), average=\"micro\")\n",
    "        macro_f1 = f1_score(test_labels, self._model.predict(test_dataset), average=\"macro\")\n",
    "        return micro_f1, macro_f1\n",
    "    \n",
    "\n",
    "LMLogisticRegression = LanguageLogisticRegression()\n",
    "train_score = LMLogisticRegression.train(cleaned_train_data_list)\n",
    "test_score, report = LMLogisticRegression.test(cleaned_test_data_list)\n",
    "\n",
    "print(\"\\nThe train score of the Logistic Regression model is: {:7.6f}\".format(train_score))\n",
    "print(\"The  test score of the Logistic Regression model is: {:7.6f}\\n\".format(test_score))\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53a8b62-1ef4-4c2f-934d-f78551ad039e",
   "metadata": {},
   "source": [
    "> 3) Report Micro-F1 score and Macro-F1 score for these classifiers on testing dataset explain our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "500abc8d-0e08-4a84-af6c-9d0e1e0fa035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the    Naive      Bayes Classifier: The Micro-F1 score is 0.920000, the Macro-F1 score is 0.920766\n",
      "For the Logistic Regression Classifier: The Micro-F1 score is 0.940000, the Macro-F1 score is 0.941170\n"
     ]
    }
   ],
   "source": [
    "nb_micro_f1, nb_macro_f1 = LMNaiveBayes._f1_score(cleaned_test_data_list)\n",
    "print(\"For the    Naive      Bayes Classifier: The Micro-F1 score is {:>7.6f}, the Macro-F1 score is {:>7.6f}\".format(nb_micro_f1, nb_macro_f1))\n",
    "lr_micro_f1, lr_macro_f1 = LMLogisticRegression._f1_score(cleaned_test_data_list)\n",
    "print(\"For the Logistic Regression Classifier: The Micro-F1 score is {:>7.6f}, the Macro-F1 score is {:>7.6f}\".format(lr_micro_f1, lr_macro_f1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc635dc7",
   "metadata": {},
   "source": [
    "- Micro-F1 vs. Macro-F1:\n",
    "  - Both the `Naive Bayes` and `Logistic Regression` classifiers exhibit **Micro-F1** scores slightly lower than their **Macro-F1** counterparts. This discrepancy suggests that the classifiers tend to perform better on average across individual classes (as indicated by **Macro-F1**) than when considering the overall performance across all classes equally (**Micro-F1**).\n",
    "- Classifier Comparison:\n",
    "  - The scores of the `Logistic Regression` classifier, obtained through **cross-validation**, surpass those of the `Naive Bayes` classifier. This suggests a superior performance of the `Logistic Regression` classifier on the given dataset.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
