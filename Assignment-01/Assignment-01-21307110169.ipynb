{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7186371d-a854-4609-8158-8537db05c369",
   "metadata": {},
   "source": [
    "## Instruction\n",
    "\n",
    "> 1. Rename Assignment-01-###.ipynb where ### is your student ID.\n",
    "> 2. The deadline of Assignment-01 is 23:59pm, 03-31-2024\n",
    ">\n",
    "> 3. In this assignment, you will\n",
    ">    1) explore Wikipedia text data\n",
    ">    2) build language models\n",
    ">    3) build NB and LR classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c23f2dd-e01e-4b02-9b17-c92885f8a428",
   "metadata": {},
   "source": [
    "## Task0 - Download datasets\n",
    "> Download the preprocessed data, enwiki-train.json and enwiki-test.json from the Assignment-01 folder. In the data file, each line contains a Wikipedia page with attributes, title, label, and text. There are 1000 records in the train file and 100 records in test file with ten categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cc7cb5-dd64-4886-8cd5-24f147288941",
   "metadata": {},
   "source": [
    "## Task1 - Data exploring and preprocessing\n",
    "\n",
    "> 1) Print out how many documents are in each class  (for both train and test dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18ba34ae-48ae-438a-b3dd-c9d4cb3ee416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of documents in each class for train dataset is: \n",
      "\n",
      "There are 100 documents in class       Film\n",
      "There are 100 documents in class       Book\n",
      "There are 100 documents in class Politician\n",
      "There are 100 documents in class     Writer\n",
      "There are 100 documents in class       Food\n",
      "There are  70 documents in class      Actor\n",
      "There are  80 documents in class     Animal\n",
      "There are 130 documents in class   Software\n",
      "There are 100 documents in class     Artist\n",
      "There are 120 documents in class    Disease\n",
      "--------------------------------------------------\n",
      "The number of documents in each class for test dataset is: \n",
      "\n",
      "There are  10 documents in class       Film\n",
      "There are  10 documents in class       Book\n",
      "There are  10 documents in class Politician\n",
      "There are  10 documents in class     Writer\n",
      "There are  10 documents in class       Food\n",
      "There are  10 documents in class      Actor\n",
      "There are  10 documents in class     Animal\n",
      "There are  10 documents in class   Software\n",
      "There are  10 documents in class     Artist\n",
      "There are  10 documents in class    Disease\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from typing import Callable\n",
    "\n",
    "################################################################ \n",
    "###         define the function we need for later use        ###\n",
    "################################################################\n",
    "\n",
    "def load_json(file_path: str) -> list:\n",
    "    \"\"\"\n",
    "    Fetch the data from `.json` file and concat them into a list.\n",
    "\n",
    "    Input:\n",
    "    - file_path: The relative file path of the `.json` file\n",
    "\n",
    "    Returns:\n",
    "    - join_data_list: A list containing the data, with the format of [{'title':<>, 'label':<>, 'text':<>}, {}, ...]\n",
    "    \"\"\"\n",
    "    join_data_list = []\n",
    "    with open(file_path, \"r\") as json_file:\n",
    "        for line in json_file:\n",
    "            line = line.strip()\n",
    "            # guaranteen the line is not empty\n",
    "            if line: \n",
    "                join_data_list.append(json.loads(line))\n",
    "    return join_data_list\n",
    "\n",
    "def iterate_line_in_list(data_list: list, f: Callable) -> dict:\n",
    "    \"\"\"\n",
    "    Iterate the `data_list` while recording the class.\n",
    "\n",
    "    Input:\n",
    "    - data_list: A list containing (train/test) data, with the format of [{'title':<>, 'label':<>, 'text':<>}, {}, ...]\n",
    "    - type: The type of the data, default is \"train\". Can take the value of \"train\" or \"test\"\n",
    "    - f: A function to compute the number of documents, sentences e.t.c. in each `line`\n",
    "\n",
    "    Output:\n",
    "    - class_dict: A list containing dictionaries with (key, value) as (<class>, <number_of_documents>)\n",
    "    \"\"\"\n",
    "    class_dict = {}\n",
    "    for line in data_list:\n",
    "        line_class = line['label']\n",
    "        class_dict[line_class] = class_dict.get(line_class, 0) + f(line['text'])  # if the class doesn't exist, set the value as 0\n",
    "    return class_dict\n",
    "\n",
    "################################################################ \n",
    "###                        end define                        ###\n",
    "################################################################\n",
    "\n",
    "def count_docs(text):\n",
    "    return 1\n",
    "\n",
    "def print_docs_in_class(class_dict: dict, type: str = \"train\") -> None:\n",
    "    print(\"The number of documents in each class for \" + type + \" dataset is: \\n\")\n",
    "    for _class, _times in class_dict.items():\n",
    "        print(\"There are {:>3} documents in class {:>10}\".format(_times, _class))\n",
    "    print('-'*50)\n",
    "\n",
    "\n",
    "# Fetch data from the json file\n",
    "    \n",
    "train_file_path, test_file_path = \"enwiki-train.json\", \"enwiki-test.json\"\n",
    "train_data_list, test_data_list = map(load_json, [train_file_path, test_file_path])\n",
    "\n",
    "# print out the number of documents of each class in train and test dataset\n",
    "\n",
    "train_docs_num = iterate_line_in_list(train_data_list, count_docs)\n",
    "test_docs_num = iterate_line_in_list(test_data_list, count_docs)\n",
    "\n",
    "print_docs_in_class(train_docs_num)\n",
    "print_docs_in_class(test_docs_num, \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1a2d5c-b719-41ec-8f14-04e092517eb9",
   "metadata": {},
   "source": [
    "> 2) Print out the average number of sentences in each class.\n",
    ">    You may need to use sentence tokenization of NLTK.\n",
    ">    (for both train and test dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37e85dc7-d50b-406a-9550-b2063f236ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average number of sentences in each class for train dataset is: \n",
      "\n",
      "There are average  438.56 sentences in class       Film\n",
      "There are average  400.36 sentences in class       Book\n",
      "There are average  706.20 sentences in class Politician\n",
      "There are average  420.32 sentences in class     Writer\n",
      "There are average  175.24 sentences in class       Food\n",
      "There are average   76.70 sentences in class      Actor\n",
      "There are average   70.38 sentences in class     Animal\n",
      "There are average  260.95 sentences in class   Software\n",
      "There are average  306.47 sentences in class     Artist\n",
      "There are average  404.90 sentences in class    Disease\n",
      "--------------------------------------------------\n",
      "The average number of sentences in each class for test dataset is: \n",
      "\n",
      "There are average  364.70 sentences in class       Film\n",
      "There are average  295.90 sentences in class       Book\n",
      "There are average  597.60 sentences in class Politician\n",
      "There are average  294.90 sentences in class     Writer\n",
      "There are average  107.60 sentences in class       Food\n",
      "There are average   30.70 sentences in class      Actor\n",
      "There are average   46.80 sentences in class     Animal\n",
      "There are average  160.10 sentences in class   Software\n",
      "There are average  234.00 sentences in class     Artist\n",
      "There are average  311.70 sentences in class    Disease\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def count_sents(text):\n",
    "    return len(sent_tokenize(text))\n",
    "\n",
    "def print_ave_sents_in_class(class_dict: dict, type: str = \"train\"):\n",
    "    # get the dict of number of documents in each class based on the input type\n",
    "    if type == \"train\":\n",
    "        docs_num_class = train_docs_num\n",
    "    elif type == \"test\":\n",
    "        docs_num_class = test_docs_num\n",
    "    else:\n",
    "        raise TypeError\n",
    "    \n",
    "    # print the result\n",
    "    print(\"The average number of sentences in each class for \" + type + \" dataset is: \\n\")\n",
    "    for _class, _times in class_dict.items():\n",
    "        print(\"There are average {:>7.2f} sentences in class {:>10}\".format(_times / docs_num_class[_class], _class))\n",
    "    print('-'*50)\n",
    "\n",
    "\n",
    "train_ave_sents = iterate_line_in_list(train_data_list, count_sents)\n",
    "test_ave_sents = iterate_line_in_list(test_data_list, count_sents)\n",
    "\n",
    "print_ave_sents_in_class(train_ave_sents)\n",
    "print_ave_sents_in_class(test_ave_sents, \"test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502ef409-65a5-4d07-9b69-c5986569970a",
   "metadata": {},
   "source": [
    "> 3) Print out the average number of tokens in each class\n",
    ">    (for both train and test dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4d7628e-762c-49fe-804a-796fa0265af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average number of tokens in each class for train dataset is: \n",
      "\n",
      "There are average 11895.28 tokens in class       Film\n",
      "There are average 10540.51 tokens in class       Book\n",
      "There are average 18644.30 tokens in class Politician\n",
      "There are average 11849.91 tokens in class     Writer\n",
      "There are average  3904.15 tokens in class       Food\n",
      "There are average  1868.84 tokens in class      Actor\n",
      "There are average  1521.92 tokens in class     Animal\n",
      "There are average  6302.30 tokens in class   Software\n",
      "There are average  8212.91 tokens in class     Artist\n",
      "There are average  9322.96 tokens in class    Disease\n",
      "--------------------------------------------------\n",
      "The average number of tokens in each class for test dataset is: \n",
      "\n",
      "There are average  9292.90 tokens in class       Film\n",
      "There are average  7711.10 tokens in class       Book\n",
      "There are average 15204.30 tokens in class Politician\n",
      "There are average  8499.40 tokens in class     Writer\n",
      "There are average  2445.50 tokens in class       Food\n",
      "There are average   677.50 tokens in class      Actor\n",
      "There are average   885.60 tokens in class     Animal\n",
      "There are average  3972.80 tokens in class   Software\n",
      "There are average  5706.40 tokens in class     Artist\n",
      "There are average  6988.80 tokens in class    Disease\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def count_tokens(text):\n",
    "    return len(word_tokenize(text))\n",
    "\n",
    "def print_ave_tokens_in_class(class_dict: dict, type: str = \"train\"):\n",
    "    # get the dict of number of documents in each class based on the input type\n",
    "    if type == \"train\":\n",
    "        docs_num_class = train_docs_num\n",
    "    elif type == \"test\":\n",
    "        docs_num_class = test_docs_num\n",
    "    else:\n",
    "        raise TypeError\n",
    "    \n",
    "    # print the result\n",
    "    print(\"The average number of tokens in each class for \" + type + \" dataset is: \\n\")\n",
    "    for _class, _times in class_dict.items():\n",
    "        print(\"There are average {:>8.2f} tokens in class {:>10}\".format(_times / docs_num_class[_class], _class))\n",
    "    print('-'*50)\n",
    "\n",
    "train_ave_tokens = iterate_line_in_list(train_data_list, count_tokens)\n",
    "test_ave_tokens = iterate_line_in_list(test_data_list, count_tokens)\n",
    "\n",
    "print_ave_tokens_in_class(train_ave_tokens)\n",
    "print_ave_tokens_in_class(test_ave_tokens, \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d999a66-977c-469b-939c-c3934210972e",
   "metadata": {},
   "source": [
    "> 4) For each sentence in the document, remove punctuations and other special characters so that each sentence only contains English words and numbers. To make your life easier, you can make all words as lower cases. For each class, print out the first article's name and the processed first 40 words. (for both train and test dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "819a28ef-0821-4ace-be70-fb9b249d355b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result of the train data list:\n",
      "\n",
      "The first article's name of class       Film is         Citizen_Kane\n",
      "The cleaned text is: [Citizen Kane is a 1941 American drama fi] ==> [citizen kane is a 1941 american drama fi]\n",
      "\n",
      "The first article's name of class       Book is The_Spirit_of_the_Age\n",
      "The cleaned text is: [The Spirit of the Age (full title \"The S] ==> [the spirit of the age full title the spi]\n",
      "\n",
      "The first article's name of class Politician is    Charles_de_Gaulle\n",
      "The cleaned text is: [Charles André Joseph Marie de Gaulle (; ] ==> [charles andr joseph marie de gaulle 22 n]\n",
      "\n",
      "The first article's name of class     Writer is        Mircea_Eliade\n",
      "The cleaned text is: [Mircea Eliade (; – April 22, 1986) was a] ==> [mircea eliade april 22 1986 was a romani]\n",
      "\n",
      "The first article's name of class       Food is       Korean_cuisine\n",
      "The cleaned text is: [ \n",
      "Korean cuisine has evolved through cen] ==> [korean cuisine has evolved through centu]\n",
      "\n",
      "The first article's name of class      Actor is       Roman_Polanski\n",
      "The cleaned text is: [Roman Polanski ( ; ; born Raymond Thierr] ==> [roman polanski born raymond thierry lieb]\n",
      "\n",
      "The first article's name of class     Animal is      Oesophagostomum\n",
      "The cleaned text is: [Oesophagostomum is a genus of parasitic ] ==> [oesophagostomum is a genus of parasitic ]\n",
      "\n",
      "The first article's name of class   Software is Android_(operating_system)\n",
      "The cleaned text is: [Android is a mobile operating system bas] ==> [android is a mobile operating system bas]\n",
      "\n",
      "The first article's name of class     Artist is           Mihai_Olos\n",
      "The cleaned text is: [Mihai Olos (born 26 February 1940 in Ari] ==> [mihai olos born 26 february 1940 in arin]\n",
      "\n",
      "The first article's name of class    Disease is    Domestic_violence\n",
      "The cleaned text is: [Domestic violence (also called domestic ] ==> [domestic violence also called domestic a]\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "The result of the test data list:\n",
      "\n",
      "The first article's name of class       Film is Monty_Python's_Life_of_Brian\n",
      "The cleaned text is: [Monty Python's Life of Brian, also known] ==> [monty pythons life of brian also known a]\n",
      "\n",
      "The first article's name of class       Book is         Cousin_Bette\n",
      "The cleaned text is: [La Cousine Bette (, \"Cousin Bette\") is a] ==> [la cousine bette cousin bette is an 1846]\n",
      "\n",
      "The first article's name of class Politician is    Olusegun_Obasanjo\n",
      "The cleaned text is: [Chief Olusegun Matthew Okikiola Aremu Ob] ==> [chief olusegun matthew okikiola aremu ob]\n",
      "\n",
      "The first article's name of class     Writer is         Horia_Gârbea\n",
      "The cleaned text is: [Horia-Răzvan Gârbea or Gîrbea (; born Au] ==> [horiarzvan grbea or grbea born august 10]\n",
      "\n",
      "The first article's name of class       Food is          Sponge_cake\n",
      "The cleaned text is: [Sponge cake is a light cake made with eg] ==> [sponge cake is a light cake made with eg]\n",
      "\n",
      "The first article's name of class      Actor is       Kom_Chuanchuen\n",
      "The cleaned text is: [Akom Preedakul (, , ; 5 January 1958 – 3] ==> [akom preedakul 5 january 1958 30 april 2]\n",
      "\n",
      "The first article's name of class     Animal is Articulata_hypothesis\n",
      "The cleaned text is: [The Articulata hypothesis is the groupin] ==> [the articulata hypothesis is the groupin]\n",
      "\n",
      "The first article's name of class   Software is                 Unix\n",
      "The cleaned text is: [Unix (; trademarked as UNIX) is a family] ==> [unix trademarked as unix is a family of ]\n",
      "\n",
      "The first article's name of class     Artist is     Camille_Pissarro\n",
      "The cleaned text is: [Camille Pissarro ( , ; 10 July 1830 – 13] ==> [camille pissarro 10 july 1830 13 novembe]\n",
      "\n",
      "The first article's name of class    Disease is Staphylococcus_aureus\n",
      "The cleaned text is: [Staphylococcus aureus is a Gram-positive] ==> [staphylococcus aureus is a grampositive ]\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from copy import deepcopy\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def clean_doc(document: str) -> list:\n",
    "    document = document.lower()\n",
    "    cleaned_document = []\n",
    "    sentences = sent_tokenize(document)\n",
    "    for sentence in sentences:\n",
    "        # remove punctuations and special characters\n",
    "        sentence = re.sub(r'[^a-zA-Z0-9\\s]', '', sentence)\n",
    "        # remove extra whitespaces\n",
    "        sentence = re.sub(r'\\s+', ' ', sentence).strip()\n",
    "        cleaned_document.append(sentence)\n",
    "    return cleaned_document\n",
    "\n",
    "def process_data_list(data_list: list, type: str = \"train\") -> list:\n",
    "    explored = []\n",
    "    print(\"The result of the \" + type + \" data list:\")\n",
    "    # process the data_list\n",
    "    for line in data_list:\n",
    "        class_label = line[\"label\"]\n",
    "        former_line_text = line[\"text\"]                         # former text\n",
    "        line[\"sentences\"] = clean_doc(line[\"text\"])             # cleaned sentences list\n",
    "        line[\"text\"] = \". \".join(line[\"sentences\"]) + \".\"       # join the sentence list with \". \" to generate the processed text\n",
    "        if class_label not in explored:\n",
    "            explored.append(class_label)\n",
    "            # print the result\n",
    "            print()\n",
    "            print(\"The first article's name of class {:>10} is {:>20}\".format(class_label, line[\"title\"]))\n",
    "            print(\"The cleaned text is: [{}] ==> [{}]\".format(former_line_text[:40], line[\"text\"][:40]))\n",
    "    print(\"-\"*120)\n",
    "    return data_list\n",
    "\n",
    "# make a deepcopy of the origin data list to avoid over-write\n",
    "cleaned_train_data_list = deepcopy(train_data_list)\n",
    "cleaned_test_data_list = deepcopy(test_data_list)\n",
    "\n",
    "# process the copyed data list in place by `process_data_list`\n",
    "cleaned_train_data_list = process_data_list(cleaned_train_data_list)\n",
    "cleaned_test_data_list = process_data_list(cleaned_test_data_list, \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6c7ea9-18fa-43f1-bded-625c09a947b1",
   "metadata": {},
   "source": [
    "## Task2 - Build language models\n",
    "\n",
    "> 1) Based on the training dataset, build unigram, bigram, and trigram language models using Add-one smoothing technique. It is encouraged to implement models by yourself. If you use public code, please cite it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0e979b1-235f-4663-8acc-00257735e209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The unigram language model has been successfully built!\n",
      "The  bigram language model has been successfully built!\n",
      "The trigram language model has been successfully built!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from itertools import product\n",
    "import math\n",
    "\n",
    "'''\n",
    "The framework of the class `NGramModels` follows from the repo \"https://github.com/joshualoehr/ngram-language-model\" with some \\\n",
    "modification to fit into the task 1.\n",
    "'''\n",
    "\n",
    "class NGramModels(object):\n",
    "    def __init__(self, n, laplace=1) -> None:\n",
    "        self.n = n\n",
    "        self.laplace = laplace\n",
    "        self._model = None\n",
    "        self._tokens = None\n",
    "        self._vocab = None\n",
    "        self._masks = list(reversed(list(product((0,1), repeat=n))))\n",
    "    \n",
    "    def _preprocess(self, sentences: list) -> list:\n",
    "        \"\"\"\n",
    "        Preprocess the raw text by adding (n-1)*\"<s>\" (or one single <s>) on the front of the sentence \n",
    "        and replacing the tokens which occur only once with \"<UNK>\".\n",
    "\n",
    "        Input:\n",
    "        - sentences: A list with each element as a `sent_tokenized` sentence.\n",
    "\n",
    "        Return:\n",
    "        - tokens: A list containing the processed tokens\n",
    "        \"\"\"\n",
    "        sos = \"<s> \" * (self.n - 1) if self.n > 1 else \"<s> \"\n",
    "        tokenized_sentences = ['{}{} {}'.format(sos, sent, \"</s>\").split() for sent in sentences]\n",
    "        tokenized_sentences = [token for sublist in tokenized_sentences for token in sublist]  # flatten\n",
    "        # Replace tokens which appear only once in the corpus with <UNK>\n",
    "        vocab = nltk.FreqDist(tokenized_sentences)\n",
    "        tokens = [token if vocab[token] > 1 else \"<UNK>\" for token in tokenized_sentences]\n",
    "        return tokens\n",
    "    \n",
    "    def _smooth(self) -> dict:\n",
    "        \"\"\"\n",
    "        Smooth the frequency distribution based on Laplace smoothing.\n",
    "\n",
    "        Return:\n",
    "        - A dictionary {<ngram>: <count>, ...} containing the information of the frequency distribution\n",
    "        \"\"\"\n",
    "        vocab_size = len(self._vocab)\n",
    "        if self.n == 1:         # if n equals 1, we don't need to smooth it\n",
    "            num_tokens = len(self._tokens)\n",
    "            return {(unigram,): count / num_tokens for unigram, count in self._vocab.items()}\n",
    "        else:\n",
    "            n_grams = nltk.ngrams(self._tokens, self.n)\n",
    "            n_vocab = nltk.FreqDist(n_grams)\n",
    "            n_minus_one_grams = nltk.ngrams(self._tokens, self.n-1)\n",
    "            n_minus_one_vocab = nltk.FreqDist(n_minus_one_grams)\n",
    "            return {ngram: (n_freq + self.laplace) / (n_minus_one_vocab[ngram[:-1]] + self.laplace * vocab_size) for ngram, n_freq in n_vocab.items()}\n",
    "\n",
    "    def train(self, sentences: list) -> None:\n",
    "        \"\"\"\n",
    "        Train the model based on the given raw text.\n",
    "\n",
    "        Input:\n",
    "        - sentences: A list with each element as a `sent_tokenized` sentence.\n",
    "        \"\"\"\n",
    "        tokens = self._preprocess(sentences)\n",
    "        self._tokens = tokens\n",
    "        self._vocab = nltk.FreqDist(self._tokens)\n",
    "        self._model = self._smooth()\n",
    "\n",
    "    def _find_match(self, ngram: tuple) -> str:\n",
    "        \"\"\"\n",
    "        Find the best match of the given ngram token in the trained model by masking the ngram in iteration\n",
    "\n",
    "        Input: \n",
    "        - ngram: A tuple representing a test ngram token\n",
    "\n",
    "        Return:\n",
    "        - tokens: The best match of the ngram in the trained model\n",
    "        \"\"\"\n",
    "        mask = lambda ngram, bitmask: tuple((token if flag == 1 else \"<UNK>\" for token, flag in zip(ngram, bitmask)))\n",
    "        possible_tokens = [mask(ngram, bitmask) for bitmask in self._masks]\n",
    "        for tokens in possible_tokens:\n",
    "            if tokens in self._model:\n",
    "                return tokens\n",
    "\n",
    "    def perplexity(self, test_sentences: list) -> float:\n",
    "        \"\"\"\n",
    "        Compute the perplexity of the given `test_sentences` based on the train tokens.\n",
    "\n",
    "        Input:\n",
    "        - test_sentences: A list containing the test sentences\n",
    "        \n",
    "        Return:\n",
    "        - perplexity: The perplexity of the test material, computed by the geomteric mean of the\n",
    "                      log probabilities. \n",
    "        \"\"\"\n",
    "        test_tokens = self._preprocess(test_sentences)\n",
    "        test_ngrams = nltk.ngrams(test_tokens, self.n)\n",
    "        known_ngrams  = (self._find_match((ngram,)) if isinstance(ngram, str) else self._find_match(ngram) for ngram in test_ngrams)\n",
    "        probabilities = [self._model[ngram] for ngram in known_ngrams]\n",
    "\n",
    "        return math.exp((-1 / len(test_tokens)) * sum(map(math.log, probabilities)))\n",
    "\n",
    "    def _best_candidate(self, prev: tuple, i: int, blacklist: list=[]) -> tuple:\n",
    "        \"\"\"\n",
    "        Find the best candidate from the trained model based on the previous text and blacklist.\n",
    "\n",
    "        Input:\n",
    "        - prev: A tuple containing the information of the previous text \n",
    "        - i: current index\n",
    "        - blacklist: A list of values that can't be taken\n",
    "\n",
    "        Return:\n",
    "        - candidate: A tuple with the format (<candidate_token>, <prob>)\n",
    "        \"\"\"\n",
    "        blacklist += [\"<UNK>\"]\n",
    "        candidates = [(ngram[-1], prob) for ngram, prob in self._model.items() if ngram[:-1] == prev] # find the candidates based on the trained moel\n",
    "        candidates = [candidate for candidate in candidates if candidate[0] not in blacklist]         # filter out the candidate in blacklist\n",
    "        candidates = sorted(candidates, key=lambda candidate: candidate[1], reverse=True)             # sort the candidates based on the prob\n",
    "        if len(candidates) == 0:\n",
    "            return (\"</s>\", 1)\n",
    "        return candidates[0 if prev != () and prev[-1] != \"<s>\" else i]\n",
    "\n",
    "    def generate(self, num: int, min_len: int=12, max_len: int=24):\n",
    "        \"\"\"\n",
    "        Generate sentences based on the trained model for given number of sentences, minimum length and maximun length\n",
    "\n",
    "        Input:\n",
    "        - num: The number of sentences we need to generate\n",
    "        - min_len: The minmum length of the generated sentence\n",
    "        - max_len: The maximum length of the generated sentence\n",
    "\n",
    "        Return (Yield):\n",
    "        - The generated sentence one by one\n",
    "        \"\"\"\n",
    "        for i in range(num):\n",
    "            sent, prob = [\"<s>\"] * max(1, self.n - 1), 1\n",
    "            while sent[-1] != \"</s>\":\n",
    "                prev = () if self.n == 1 else tuple(sent[-(self.n-1):])\n",
    "                blacklist = sent + ([\"</s>\"] if len(sent) < min_len else [])\n",
    "                next_token, next_prob = self._best_candidate(prev, i, blacklist)\n",
    "                sent.append(next_token)\n",
    "                prob *= next_prob\n",
    "                \n",
    "                if len(sent) >= max_len:\n",
    "                    sent.append(\"</s>\")\n",
    "\n",
    "            yield ' '.join(sent), -1/math.log(prob)\n",
    "\n",
    "# generate the train sentences from `cleaned_train_data_list`\n",
    "train_sentences = []\n",
    "for each in cleaned_train_data_list:\n",
    "    train_sentences.extend(each[\"sentences\"]) \n",
    "\n",
    "# unigram language model\n",
    "unigramModel = NGramModels(1)\n",
    "unigramModel.train(train_sentences)\n",
    "print(\"The unigram language model has been successfully built!\")\n",
    "# bigram language model\n",
    "bigramModel = NGramModels(2)\n",
    "bigramModel.train(train_sentences)\n",
    "print(\"The  bigram language model has been successfully built!\")\n",
    "# trigram language model\n",
    "trigramModel = NGramModels(3)\n",
    "trigramModel.train(train_sentences)\n",
    "print(\"The trigram language model has been successfully built!\")\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb6edc5-78c5-4de0-8855-2a455d625c97",
   "metadata": {},
   "source": [
    "> 2) Report the perplexity of these 3 trained models on the testing dataset and explain your findings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3acda9a7-606c-4fa6-918a-9ebe7745ad42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The perplexity of the testing dataset in unigram language model is 1155.88\n",
      "The perplexity of the testing dataset in  bigram language model is 1587.09\n",
      "The perplexity of the testing dataset in trigram language model is 3859.54\n"
     ]
    }
   ],
   "source": [
    "# generate the test sentences from `cleaned_test_data_list`\n",
    "test_sentences = []\n",
    "for each in cleaned_test_data_list:\n",
    "    test_sentences.extend(each[\"sentences\"]) \n",
    "\n",
    "# compute the perplexity of the test dataset\n",
    "u_perp = unigramModel.perplexity(test_sentences)\n",
    "b_perp = bigramModel.perplexity(test_sentences)\n",
    "t_perp = trigramModel.perplexity(test_sentences)\n",
    "\n",
    "print(\"The perplexity of the testing dataset in unigram language model is {:>7.2f}\".format(round(u_perp, 2)))\n",
    "print(\"The perplexity of the testing dataset in  bigram language model is {:>7.2f}\".format(round(b_perp, 2)))\n",
    "print(\"The perplexity of the testing dataset in trigram language model is {:>7.2f}\".format(round(t_perp, 2)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b262c9e3",
   "metadata": {},
   "source": [
    "- From the results in the above cell, we can find that the order of perplexity between different `NGram` language models is `unigramModel` $<$ `bigramModel` $<$ `trigramModel`.\n",
    "- The result seems counterintuitive, and below I will give the explanations for different language models:\n",
    "  - `unigramModel`: the perplexity of the LM is the smallest, and it may due to the fact that the vocabulary of the `train_sentences` and `test_sentences` are similar.\n",
    "  - `bigramModel`: the perplexity of this model is slighly higher than the `unigramModel`, which may resulting from the commonly used two-words phase in the text from wikipedia.\n",
    "  - `trigramModel`: the perplexity of this model is the largest among all, indicating the very low similarity of three-words phase between train data and test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd75b35-134d-4629-ac87-ed84fcc9d8e4",
   "metadata": {},
   "source": [
    "> 3) Use each built model to generate five sentences and explain these generated patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea6e1244-2b15-4845-a83b-5708797569b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "For the unigram language model:\n",
      "<s> the of and in to a was that as for with </s> (0.02075)\n",
      "<s> of and in to a was that as for with his </s> (0.01990)\n",
      "<s> and in to a was that as for with his is of he on by it from an at be which had or </s> (0.00895)\n",
      "<s> in to a was that as for with his is he and on by it from an at be which had or are </s> (0.00877)\n",
      "<s> to a was that as for with his is he on in by it from an at be which had or are not </s> (0.00860)\n",
      "--------------------------------------------------\n",
      "For the bigram language model:\n",
      "<s> the film was a new york city of his own and in which he had been found that it is not be used </s> (0.00987)\n",
      "<s> in the film was a new york city of his own </s> (0.01976)\n",
      "<s> he was a new york city of the film and his own </s> (0.01801)\n",
      "<s> it was a new york city of the film and his own </s> (0.01782)\n",
      "<s> this is a new york city of the film was not be used to his own </s> (0.01341)\n",
      "--------------------------------------------------\n",
      "For the trigram language model:\n",
      "<s> <s> the film was released on october 31 2014 a new version of windows 8 and 9 to 10 times more likely than </s> (0.00579)\n",
      "<s> <s> in the united states and canada on november 22 1963 he was a member of parliament </s> (0.00868)\n",
      "<s> <s> he was a member of the film and television arts bafta awards for best actor in his own </s> (0.00734)\n",
      "<s> <s> it is not a single day of the film was released on october 31 2014 and in his own </s> (0.00714)\n",
      "<s> <s> this is the most common cause of death in a letter to his own </s> (0.01020)\n"
     ]
    }
   ],
   "source": [
    "num_sentence = 5\n",
    "\n",
    "model_dict = {\"unigram\": unigramModel, \"bigram\": bigramModel, \"trigram\": trigramModel}\n",
    "for _name, _model in model_dict.items():\n",
    "    print(\"-\" * 50)\n",
    "    print(\"For the {} language model:\".format(_name))\n",
    "    for sentence, prob in _model.generate(num_sentence):\n",
    "        print(\"{} ({:.5f})\".format(sentence, prob))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e287bc9",
   "metadata": {},
   "source": [
    "- `unigram`\n",
    "  - From the generated sentences above, we can find that these sentences are composed of words which have high occurrence, such as [\"the\", \"of\", \"as\", \"for\", \"in\", ...]\n",
    "  - All the five sentences don't make sence, and can be regarded as just the combination of \"one single word\".\n",
    "  - There is almost no relationship between words in different positions of each sentence.\n",
    "- `bigram`\n",
    "  - The phases [\"new york city\", \"the film\", \"his own\"] occur in all five sentences, indicating that the language model \"learned\" several two-words phases from the train dataset.\n",
    "  - The sentences generated by the LM still look like some weird, with some improvement from the `unigram` LM.\n",
    "- `trigram`\n",
    "  - The sentences generated by `trigram` LM make some sense now and they (or the sub-sentence of them) seem to be meaningful.\n",
    "  - The probabilities of the generated sentences are the lowest in average among these three LMs, which can be seen as a trade-off between the similarity of the train data and generalization of the LM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1b4282-d67c-4543-88cf-1bd0518173c2",
   "metadata": {},
   "source": [
    "## Task3 - Build NB/LR classifiers\n",
    "\n",
    "> 1) Build a Naive Bayes classifier (with Laplace smoothing) and test your model on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f9cdf1d-8147-4033-bbb0-9147f3647c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train score of the Naive Bayes model with laplace smoothing is: 0.998000\n",
      "The  test score of the Naive Bayes model with laplace smoothing is: 0.920000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Actor       1.00      0.80      0.89        10\n",
      "      Animal       1.00      1.00      1.00        10\n",
      "      Artist       1.00      1.00      1.00        10\n",
      "        Book       1.00      0.70      0.82        10\n",
      "     Disease       1.00      1.00      1.00        10\n",
      "        Film       0.90      0.90      0.90        10\n",
      "        Food       1.00      1.00      1.00        10\n",
      "  Politician       0.71      1.00      0.83        10\n",
      "    Software       1.00      1.00      1.00        10\n",
      "      Writer       0.73      0.80      0.76        10\n",
      "\n",
      "    accuracy                           0.92       100\n",
      "   macro avg       0.93      0.92      0.92       100\n",
      "weighted avg       0.93      0.92      0.92       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "import numpy as np\n",
    "\n",
    "class LanguageNaiveBayes(object):\n",
    "    def __init__(self, laplace: int=1) -> None:\n",
    "        self._data_set = None\n",
    "        self._vocab = None\n",
    "        self._features = None\n",
    "        self._labels = None\n",
    "        self._model = None\n",
    "        self.laplace = laplace\n",
    "        self.stopwords = [\n",
    "            'a', 'about', 'above', 'across', 'after', 'afterwards', 'again', 'against', 'all', 'almost', 'alone',\n",
    "             'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'amoungst', 'amount',\n",
    "             'an', 'and', 'another', 'any', 'anyhow', 'anyone', 'anything', 'anyway', 'anywhere', 'are', 'around',\n",
    "             'as', 'at', 'back', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before',\n",
    "             'beforehand', 'behind', 'being', 'below', 'beside', 'besides', 'between', 'beyond', 'bill', 'both',\n",
    "             'bottom', 'but', 'by', 'call', 'can', 'cannot', 'cant', 'co', 'con', 'could', 'couldnt', 'cry', 'de',\n",
    "             'describe', 'detail', 'did', 'do', 'does', 'doing', 'don', 'done', 'down', 'due', 'during', 'each', 'eg',\n",
    "             'eight', 'either', 'eleven', 'else', 'elsewhere', 'empty', 'enough', 'etc', 'even', 'ever', 'every', 'everyone',\n",
    "             'everything', 'everywhere', 'except', 'few', 'fifteen', 'fify', 'fill', 'find', 'fire', 'first', 'five', 'for',\n",
    "             'former', 'formerly', 'forty', 'found', 'four', 'from', 'front', 'full', 'further', 'get', 'give', 'go', 'had',\n",
    "             'has', 'hasnt', 'have', 'having', 'he', 'hence', 'her', 'here', 'hereafter', 'hereby', 'herein', 'hereupon',\n",
    "             'hers', 'herself', 'him', 'himself', 'his', 'how', 'however', 'hundred', 'i', 'ie', 'if', 'in', 'inc', 'indeed',\n",
    "             'interest', 'into', 'is', 'it', 'its', 'itself', 'just', 'keep', 'last', 'latter', 'latterly', 'least', 'less',\n",
    "             'ltd', 'made', 'many', 'may', 'me', 'meanwhile', 'might', 'mill', 'mine', 'more', 'moreover', 'most', 'mostly',\n",
    "             'move', 'much', 'must', 'my', 'myself', 'name', 'namely', 'neither', 'never', 'nevertheless', 'next', 'nine',\n",
    "             'no', 'nobody', 'none', 'noone', 'nor', 'not', 'nothing', 'now', 'nowhere', 'of', 'off', 'often', 'on', 'once',\n",
    "             'one', 'only', 'onto', 'or', 'other', 'others', 'otherwise', 'our', 'ours', 'ourselves', 'out', 'over', 'own',\n",
    "             'part', 'per', 'perhaps', 'please', 'put', 'rather', 're', 's', 'same', 'see', 'seem', 'seemed', 'seeming',\n",
    "             'seems', 'serious', 'several', 'she', 'should', 'show', 'side', 'since', 'sincere', 'six', 'sixty', 'so', \n",
    "             'some', 'somehow', 'someone', 'something', 'sometime', 'sometimes', 'somewhere', 'still', 'such', 'system',\n",
    "             't', 'take', 'ten', 'than', 'that', 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'thence', 'there',\n",
    "             'thereafter', 'thereby', 'therefore', 'therein', 'thereupon', 'these', 'they', 'thickv', 'thin', 'third', 'this',\n",
    "             'those', 'though', 'three', 'through', 'throughout', 'thru', 'thus', 'to', 'together', 'too', 'top', 'toward',\n",
    "             'towards', 'twelve', 'twenty', 'two', 'un', 'under', 'until', 'up', 'upon', 'us', 'very', 'via', 'was', 'we',\n",
    "             'well', 'were', 'what', 'whatever', 'when', 'whence', 'whenever', 'where', 'whereafter', 'whereas', 'whereby',\n",
    "             'wherein', 'whereupon', 'wherever', 'whether', 'which', 'while', 'whither', 'who', 'whoever', 'whole', 'whom',\n",
    "             'whose', 'why', 'will', 'with', 'within', 'without', 'would', 'yet', 'you', 'your', 'yours', 'yourself',\n",
    "             'yourselves'\n",
    "        ]\n",
    "\n",
    "    def train(self, data_list: list, cut_freq: int=5):\n",
    "        self._vocab = self._bulid_vocab(data_list)\n",
    "        self._features = self._extract_features(cut_freq)\n",
    "        self._data_set, self._labels = self._convert_to_dataset(data_list)\n",
    "        self._model = MultinomialNB(alpha=self.laplace)\n",
    "        self._model.fit(self._data_set, self._labels)\n",
    "        return self._model.score(self._data_set, self._labels)\n",
    "\n",
    "    def test(self, data_list: list) -> tuple:\n",
    "        test_dataset, test_labels = self._convert_to_dataset(data_list)\n",
    "        test_pred = self._model.predict(test_dataset)\n",
    "        test_score = self._model.score(test_dataset, test_labels)\n",
    "        report = classification_report(test_labels, test_pred)\n",
    "        return test_score, report\n",
    "\n",
    "    def set_stopwords(self, stopwords: list[str]) -> None:\n",
    "        if isinstance(stopwords, list[str]):\n",
    "            self.stopwords = stopwords\n",
    "        else:\n",
    "            raise TypeError(\"The type of the stopwords should be List[str]\")\n",
    "        \n",
    "    def _bulid_vocab(self, data_list: list) -> dict:\n",
    "        \"\"\"\n",
    "        Build a vocabulary for words which have length greater than 2 and are not in stopwords.\n",
    "\n",
    "        Input:\n",
    "        - data_list: A list with the format of [{\"title\": <title>, \"label\": <label>, \"text\": <text>}, ...]\n",
    "\n",
    "        Return:\n",
    "        - vocab: A dictionary with the format of {\"word\": <count>, ...}, where each word has length greater than 2 and is not in stopwords\n",
    "        \"\"\"\n",
    "        vocab = {}\n",
    "        for i in range(len(data_list)):\n",
    "            sentence = data_list[i][\"text\"]\n",
    "            for word in sentence.split():\n",
    "                if len(word) > 2 and word not in self.stopwords:\n",
    "                    if word not in vocab:\n",
    "                        vocab[word] = 1\n",
    "                    else:\n",
    "                        vocab[word] += 1\n",
    "        return vocab\n",
    "\n",
    "    def _extract_features(self, cut_freq: int) -> dict:\n",
    "        \"\"\"\n",
    "        Extract features from the vocabulary, with the threshold `cut_freq` for frequency of a word.\n",
    "\n",
    "        Input:\n",
    "        - cut_freq: The cutting frequency of the occurrence times of a word\n",
    "\n",
    "        Return:\n",
    "        - features: A dict with (key, value) as the (<extracted_feature>, <index of the feature>) \n",
    "        \"\"\"\n",
    "        features = {}\n",
    "        count = 0\n",
    "        for key, value in self._vocab.items():\n",
    "            if value >= cut_freq:\n",
    "                features[key] = count\n",
    "                count += 1\n",
    "        return features\n",
    "\n",
    "    def _convert_to_dataset(self, data_list: list) -> tuple:\n",
    "        \"\"\"\n",
    "        Convert the `data_list` to `train_dataset` and `labels` which can be accepted by the `MultinomialNB()`\n",
    "\n",
    "        Input:\n",
    "        - data_list: A list with the format of [{\"title\": <title>, \"label\": <label>, \"text\": <text>}, ...]\n",
    "\n",
    "        Return:\n",
    "        - dataset: A 2-d numpy array with rows and columns as the index numbers and feature indexes respectively\n",
    "        - labels: The corresponding y label of the dataset\n",
    "        \"\"\"\n",
    "        data_length = len(data_list)\n",
    "        dataset = np.zeros((data_length, len(self._features)))\n",
    "        labels = [0] * data_length\n",
    "        for i in range(data_length):\n",
    "            word_list = [word for word in data_list[i][\"text\"].split()]\n",
    "            for word in word_list:\n",
    "                if word in self._features:\n",
    "                    dataset[i][self._features[word]] += 1\n",
    "            labels[i] = data_list[i][\"label\"]\n",
    "        return dataset, labels\n",
    "\n",
    "    def _f1_score(self, data_list: list) -> tuple:\n",
    "        \"\"\"\n",
    "        Return the micro-f1 and macro-f1 scores of the model in test dataset\n",
    "        \"\"\"\n",
    "        test_dataset, test_labels = self._convert_to_dataset(data_list)\n",
    "        micro_f1 = f1_score(test_labels, self._model.predict(test_dataset), average=\"micro\")\n",
    "        macro_f1 = f1_score(test_labels, self._model.predict(test_dataset), average=\"macro\")\n",
    "        return micro_f1, macro_f1\n",
    "\n",
    "LMNaiveBayes = LanguageNaiveBayes(1)\n",
    "train_score = LMNaiveBayes.train(cleaned_train_data_list)\n",
    "test_score, report = LMNaiveBayes.test(cleaned_test_data_list)\n",
    "\n",
    "print(\"The train score of the Naive Bayes model with laplace smoothing is: {:7.6f}\".format(train_score))\n",
    "print(\"The  test score of the Naive Bayes model with laplace smoothing is: {:7.6f}\".format(test_score))\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca80a4a-9db5-426f-a45a-b1c2c1e34423",
   "metadata": {},
   "source": [
    "> 2) Build a LR classifier. This question seems to be challenging. We did not directly provide features for samples. But just use your own method to build useful features. You may need to split the training dataset into train and validation so that some involved parameters can be tuned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ae83301a-0529-4dc9-899b-504324f9a547",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 117\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m dataset, labels\n\u001b[0;32m    116\u001b[0m LMLogisticRegression \u001b[38;5;241m=\u001b[39m LanguageLogisticRegression()\n\u001b[1;32m--> 117\u001b[0m best_params \u001b[38;5;241m=\u001b[39m \u001b[43mLMLogisticRegression\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcleaned_train_data_list\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[19], line 50\u001b[0m, in \u001b[0;36mLanguageLogisticRegression.train\u001b[1;34m(self, data_list)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_set, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_to_dataset(data_list)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model \u001b[38;5;241m=\u001b[39m RFE(LogisticRegression(), n_features_to_select\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m---> 50\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39mbest_params_\n",
      "File \u001b[1;32mc:\\Users\\雪浪\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\雪浪\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_selection\\_rfe.py:264\u001b[0m, in \u001b[0;36mRFE.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit the RFE model and then the underlying estimator on the selected features.\u001b[39;00m\n\u001b[0;32m    245\u001b[0m \n\u001b[0;32m    246\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;124;03m    Fitted estimator.\u001b[39;00m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    263\u001b[0m _raise_for_unsupported_routing(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m--> 264\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\雪浪\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_selection\\_rfe.py:311\u001b[0m, in \u001b[0;36mRFE._fit\u001b[1;34m(self, X, y, step_score, **fit_params)\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    309\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting estimator with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m features.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m np\u001b[38;5;241m.\u001b[39msum(support_))\n\u001b[1;32m--> 311\u001b[0m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;66;03m# Get importance and rank them\u001b[39;00m\n\u001b[0;32m    314\u001b[0m importances \u001b[38;5;241m=\u001b[39m _get_feature_importances(\n\u001b[0;32m    315\u001b[0m     estimator,\n\u001b[0;32m    316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimportance_getter,\n\u001b[0;32m    317\u001b[0m     transform_func\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msquare\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    318\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\雪浪\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\雪浪\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1293\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1294\u001b[0m     n_threads \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1296\u001b[0m fold_coefs_ \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1298\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1299\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1300\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1301\u001b[0m \u001b[43m        \u001b[49m\u001b[43mCs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mC_\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1302\u001b[0m \u001b[43m        \u001b[49m\u001b[43ml1_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ml1_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1303\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_intercept\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_intercept\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1304\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1305\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1306\u001b[0m \u001b[43m        \u001b[49m\u001b[43msolver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msolver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1307\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmulti_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmulti_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1308\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcoef\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwarm_start_coef_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpenalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpenalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_squared_sum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_squared_sum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1315\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1317\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1318\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclass_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarm_start_coef_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclasses_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarm_start_coef\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1319\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1321\u001b[0m fold_coefs_, _, n_iter_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mfold_coefs_)\n\u001b[0;32m   1322\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(n_iter_, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint32)[:, \u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\雪浪\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     66\u001b[0m )\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\雪浪\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1861\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1862\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n\u001b[0;32m   1865\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1866\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1867\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1868\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1869\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1870\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32mc:\\Users\\雪浪\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1790\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1791\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1792\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1793\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1794\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32mc:\\Users\\雪浪\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:129\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    127\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\雪浪\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:455\u001b[0m, in \u001b[0;36m_logistic_regression_path\u001b[1;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio, n_threads)\u001b[0m\n\u001b[0;32m    451\u001b[0m l2_reg_strength \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m (C \u001b[38;5;241m*\u001b[39m sw_sum)\n\u001b[0;32m    452\u001b[0m iprint \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m101\u001b[39m][\n\u001b[0;32m    453\u001b[0m     np\u001b[38;5;241m.\u001b[39msearchsorted(np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m]), verbose)\n\u001b[0;32m    454\u001b[0m ]\n\u001b[1;32m--> 455\u001b[0m opt_res \u001b[38;5;241m=\u001b[39m \u001b[43moptimize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    456\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m    \u001b[49m\u001b[43mw0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mL-BFGS-B\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml2_reg_strength\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[0;32m    462\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaxiter\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    463\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaxls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# default is 20\u001b[39;49;00m\n\u001b[0;32m    464\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43miprint\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43miprint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    465\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgtol\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    466\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mftol\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    468\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    469\u001b[0m n_iter_i \u001b[38;5;241m=\u001b[39m _check_optimize_result(\n\u001b[0;32m    470\u001b[0m     solver,\n\u001b[0;32m    471\u001b[0m     opt_res,\n\u001b[0;32m    472\u001b[0m     max_iter,\n\u001b[0;32m    473\u001b[0m     extra_warning_msg\u001b[38;5;241m=\u001b[39m_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n\u001b[0;32m    474\u001b[0m )\n\u001b[0;32m    475\u001b[0m w0, loss \u001b[38;5;241m=\u001b[39m opt_res\u001b[38;5;241m.\u001b[39mx, opt_res\u001b[38;5;241m.\u001b[39mfun\n",
      "File \u001b[1;32mc:\\Users\\雪浪\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\optimize\\_minimize.py:710\u001b[0m, in \u001b[0;36mminimize\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    707\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[0;32m    708\u001b[0m                              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml-bfgs-b\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 710\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43m_minimize_lbfgsb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    711\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    712\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtnc\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    713\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_tnc(fun, x0, args, jac, bounds, callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[0;32m    714\u001b[0m                         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "File \u001b[1;32mc:\\Users\\雪浪\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\optimize\\_lbfgsb_py.py:365\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[1;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[0;32m    359\u001b[0m task_str \u001b[38;5;241m=\u001b[39m task\u001b[38;5;241m.\u001b[39mtobytes()\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m task_str\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFG\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    361\u001b[0m     \u001b[38;5;66;03m# The minimization routine wants f and g at the current x.\u001b[39;00m\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;66;03m# Note that interruptions due to maxfun are postponed\u001b[39;00m\n\u001b[0;32m    363\u001b[0m     \u001b[38;5;66;03m# until the completion of the current minimization iteration.\u001b[39;00m\n\u001b[0;32m    364\u001b[0m     \u001b[38;5;66;03m# Overwrite f and g:\u001b[39;00m\n\u001b[1;32m--> 365\u001b[0m     f, g \u001b[38;5;241m=\u001b[39m \u001b[43mfunc_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m task_str\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNEW_X\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    367\u001b[0m     \u001b[38;5;66;03m# new iteration\u001b[39;00m\n\u001b[0;32m    368\u001b[0m     n_iterations \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\雪浪\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:283\u001b[0m, in \u001b[0;36mScalarFunction.fun_and_grad\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    282\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfun_and_grad\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m--> 283\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray_equal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    284\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_x_impl(x)\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_fun()\n",
      "File \u001b[1;32mc:\\Users\\雪浪\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\numeric.py:2439\u001b[0m, in \u001b[0;36marray_equal\u001b[1;34m(a1, a2, equal_nan)\u001b[0m\n\u001b[0;32m   2437\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   2438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m equal_nan:\n\u001b[1;32m-> 2439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(asarray(a1 \u001b[38;5;241m==\u001b[39m a2)\u001b[38;5;241m.\u001b[39mall())\n\u001b[0;32m   2440\u001b[0m \u001b[38;5;66;03m# Handling NaN values if equal_nan is True\u001b[39;00m\n\u001b[0;32m   2441\u001b[0m a1nan, a2nan \u001b[38;5;241m=\u001b[39m isnan(a1), isnan(a2)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "class LanguageLogisticRegression(object):\n",
    "    def __init__(self) -> None:\n",
    "        self._data_set = None\n",
    "        self._vocab = None\n",
    "        self._features = None\n",
    "        self._labels = None\n",
    "        self._model = None\n",
    "        self.stopwords = [\n",
    "            'a', 'about', 'above', 'across', 'after', 'afterwards', 'again', 'against', 'all', 'almost', 'alone',\n",
    "             'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'amoungst', 'amount',\n",
    "             'an', 'and', 'another', 'any', 'anyhow', 'anyone', 'anything', 'anyway', 'anywhere', 'are', 'around',\n",
    "             'as', 'at', 'back', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before',\n",
    "             'beforehand', 'behind', 'being', 'below', 'beside', 'besides', 'between', 'beyond', 'bill', 'both',\n",
    "             'bottom', 'but', 'by', 'call', 'can', 'cannot', 'cant', 'co', 'con', 'could', 'couldnt', 'cry', 'de',\n",
    "             'describe', 'detail', 'did', 'do', 'does', 'doing', 'don', 'done', 'down', 'due', 'during', 'each', 'eg',\n",
    "             'eight', 'either', 'eleven', 'else', 'elsewhere', 'empty', 'enough', 'etc', 'even', 'ever', 'every', 'everyone',\n",
    "             'everything', 'everywhere', 'except', 'few', 'fifteen', 'fify', 'fill', 'find', 'fire', 'first', 'five', 'for',\n",
    "             'former', 'formerly', 'forty', 'found', 'four', 'from', 'front', 'full', 'further', 'get', 'give', 'go', 'had',\n",
    "             'has', 'hasnt', 'have', 'having', 'he', 'hence', 'her', 'here', 'hereafter', 'hereby', 'herein', 'hereupon',\n",
    "             'hers', 'herself', 'him', 'himself', 'his', 'how', 'however', 'hundred', 'i', 'ie', 'if', 'in', 'inc', 'indeed',\n",
    "             'interest', 'into', 'is', 'it', 'its', 'itself', 'just', 'keep', 'last', 'latter', 'latterly', 'least', 'less',\n",
    "             'ltd', 'made', 'many', 'may', 'me', 'meanwhile', 'might', 'mill', 'mine', 'more', 'moreover', 'most', 'mostly',\n",
    "             'move', 'much', 'must', 'my', 'myself', 'name', 'namely', 'neither', 'never', 'nevertheless', 'next', 'nine',\n",
    "             'no', 'nobody', 'none', 'noone', 'nor', 'not', 'nothing', 'now', 'nowhere', 'of', 'off', 'often', 'on', 'once',\n",
    "             'one', 'only', 'onto', 'or', 'other', 'others', 'otherwise', 'our', 'ours', 'ourselves', 'out', 'over', 'own',\n",
    "             'part', 'per', 'perhaps', 'please', 'put', 'rather', 're', 's', 'same', 'see', 'seem', 'seemed', 'seeming',\n",
    "             'seems', 'serious', 'several', 'she', 'should', 'show', 'side', 'since', 'sincere', 'six', 'sixty', 'so', \n",
    "             'some', 'somehow', 'someone', 'something', 'sometime', 'sometimes', 'somewhere', 'still', 'such', 'system',\n",
    "             't', 'take', 'ten', 'than', 'that', 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'thence', 'there',\n",
    "             'thereafter', 'thereby', 'therefore', 'therein', 'thereupon', 'these', 'they', 'thickv', 'thin', 'third', 'this',\n",
    "             'those', 'though', 'three', 'through', 'throughout', 'thru', 'thus', 'to', 'together', 'too', 'top', 'toward',\n",
    "             'towards', 'twelve', 'twenty', 'two', 'un', 'under', 'until', 'up', 'upon', 'us', 'very', 'via', 'was', 'we',\n",
    "             'well', 'were', 'what', 'whatever', 'when', 'whence', 'whenever', 'where', 'whereafter', 'whereas', 'whereby',\n",
    "             'wherein', 'whereupon', 'wherever', 'whether', 'which', 'while', 'whither', 'who', 'whoever', 'whole', 'whom',\n",
    "             'whose', 'why', 'will', 'with', 'within', 'without', 'would', 'yet', 'you', 'your', 'yours', 'yourself',\n",
    "             'yourselves'\n",
    "        ]\n",
    "\n",
    "    def train(self, data_list: list) -> dict:\n",
    "        self._vocab = self._bulid_vocab(data_list)\n",
    "        self._features = self._extract_features(10)\n",
    "        self._data_set, self._labels = self._convert_to_dataset(data_list)\n",
    "        self._model = RFE(LogisticRegression(), n_features_to_select=10)\n",
    "        self._model.fit(self._data_set, self._labels)\n",
    "        return self._model.best_params_\n",
    "\n",
    "\n",
    "    def _bulid_vocab(self, data_list: list) -> dict:\n",
    "        \"\"\"\n",
    "        Build a vocabulary for words which have length greater than 2 and are not in stopwords.\n",
    "\n",
    "        Input:\n",
    "        - data_list: A list with the format of [{\"title\": <title>, \"label\": <label>, \"text\": <text>}, ...]\n",
    "\n",
    "        Return:\n",
    "        - vocab: A dictionary with the format of {\"word\": <count>, ...}, where each word has length greater than 2 and is not in stopwords\n",
    "        \"\"\"\n",
    "        vocab = {}\n",
    "        for i in range(len(data_list)):\n",
    "            sentence = data_list[i][\"text\"]\n",
    "            for word in sentence.split():\n",
    "                if len(word) > 2 and word not in self.stopwords:\n",
    "                    if word not in vocab:\n",
    "                        vocab[word] = 1\n",
    "                    else:\n",
    "                        vocab[word] += 1\n",
    "        return vocab\n",
    "    \n",
    "    def _extract_features(self, cut_freq: int) -> dict:\n",
    "        \"\"\"\n",
    "        Extract features from the vocabulary, with the threshold `cut_freq` for frequency of a word.\n",
    "\n",
    "        Input:\n",
    "        - cut_freq: The cutting frequency of the occurrence times of a word\n",
    "\n",
    "        Return:\n",
    "        - features: A dict with (key, value) as the (<extracted_feature>, <index of the feature>) \n",
    "        \"\"\"\n",
    "        features = {}\n",
    "        count = 0\n",
    "        for key, value in self._vocab.items():\n",
    "            if value >= cut_freq:\n",
    "                features[key] = count\n",
    "                count += 1\n",
    "        return features\n",
    "    \n",
    "    def _convert_to_dataset(self, data_list: list) -> tuple:\n",
    "        \"\"\"\n",
    "        Convert the `data_list` to `train_dataset` and `labels` which can be accepted by the `MultinomialNB()`\n",
    "\n",
    "        Input:\n",
    "        - data_list: A list with the format of [{\"title\": <title>, \"label\": <label>, \"text\": <text>}, ...]\n",
    "\n",
    "        Return:\n",
    "        - dataset: A 2-d numpy array with rows and columns as the index numbers and feature indexes respectively\n",
    "        - labels: The corresponding y label of the dataset\n",
    "        \"\"\"\n",
    "        data_length = len(data_list)\n",
    "        dataset = np.zeros((data_length, len(self._features)))\n",
    "        labels = [0] * data_length\n",
    "        for i in range(data_length):\n",
    "            word_list = [word for word in data_list[i][\"text\"].split()]\n",
    "            for word in word_list:\n",
    "                if word in self._features:\n",
    "                    dataset[i][self._features[word]] += 1\n",
    "            labels[i] = data_list[i][\"label\"]\n",
    "        return dataset, labels\n",
    "    \n",
    "\n",
    "LMLogisticRegression = LanguageLogisticRegression()\n",
    "best_params = LMLogisticRegression.train(cleaned_train_data_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53a8b62-1ef4-4c2f-934d-f78551ad039e",
   "metadata": {},
   "source": [
    "> 3) Report Micro-F1 score and Macro-F1 score for these classifiers on testing dataset explain our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "500abc8d-0e08-4a84-af6c-9d0e1e0fa035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the Naive Bayes Classifier: The Micro-F1 score is 0.920000, the Macro-F1 score is 0.920766\n"
     ]
    }
   ],
   "source": [
    "nb_micro_f1, nb_macro_f1 = LMNaiveBayes._f1_score(cleaned_test_data_list)\n",
    "print(\"For the Naive Bayes Classifier: The Micro-F1 score is {:>7.6f}, the Macro-F1 score is {:>7.6f}\".format(nb_micro_f1, nb_macro_f1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
