{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7186371d-a854-4609-8158-8537db05c369",
   "metadata": {},
   "source": [
    "## Instruction\n",
    "\n",
    "> 1. Rename Assignment-01-###.ipynb where ### is your student ID.\n",
    "> 2. The deadline of Assignment-01 is 23:59pm, 03-31-2024\n",
    ">\n",
    "> 3. In this assignment, you will\n",
    ">    1) explore Wikipedia text data\n",
    ">    2) build language models\n",
    ">    3) build NB and LR classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c23f2dd-e01e-4b02-9b17-c92885f8a428",
   "metadata": {},
   "source": [
    "## Task0 - Download datasets\n",
    "> Download the preprocessed data, enwiki-train.json and enwiki-test.json from the Assignment-01 folder. In the data file, each line contains a Wikipedia page with attributes, title, label, and text. There are 1000 records in the train file and 100 records in test file with ten categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cc7cb5-dd64-4886-8cd5-24f147288941",
   "metadata": {},
   "source": [
    "## Task1 - Data exploring and preprocessing\n",
    "\n",
    "> 1) Print out how many documents are in each class  (for both train and test dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "18ba34ae-48ae-438a-b3dd-c9d4cb3ee416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of documents in each class for train dataset is: \n",
      "\n",
      "There are 100 documents in class       Film\n",
      "There are 100 documents in class       Book\n",
      "There are 100 documents in class Politician\n",
      "There are 100 documents in class     Writer\n",
      "There are 100 documents in class       Food\n",
      "There are  70 documents in class      Actor\n",
      "There are  80 documents in class     Animal\n",
      "There are 130 documents in class   Software\n",
      "There are 100 documents in class     Artist\n",
      "There are 120 documents in class    Disease\n",
      "--------------------------------------------------\n",
      "The number of documents in each class for test dataset is: \n",
      "\n",
      "There are  10 documents in class       Film\n",
      "There are  10 documents in class       Book\n",
      "There are  10 documents in class Politician\n",
      "There are  10 documents in class     Writer\n",
      "There are  10 documents in class       Food\n",
      "There are  10 documents in class      Actor\n",
      "There are  10 documents in class     Animal\n",
      "There are  10 documents in class   Software\n",
      "There are  10 documents in class     Artist\n",
      "There are  10 documents in class    Disease\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from typing import Callable\n",
    "\n",
    "################################################################ \n",
    "###         define the function we need for later use        ###\n",
    "################################################################\n",
    "\n",
    "def load_json(file_path: str) -> list:\n",
    "    \"\"\"\n",
    "    Fetch the data from `.json` file and concat them into a list.\n",
    "\n",
    "    Input:\n",
    "    - file_path: The relative file path of the `.json` file\n",
    "\n",
    "    Returns:\n",
    "    - join_data_list: A list containing the data, with the format of [{'title':<>, 'label':<>, 'text':<>}, {}, ...]\n",
    "    \"\"\"\n",
    "    join_data_list = []\n",
    "    with open(file_path, \"r\") as json_file:\n",
    "        for line in json_file:\n",
    "            line = line.strip()\n",
    "            # guaranteen the line is not empty\n",
    "            if line: \n",
    "                join_data_list.append(json.loads(line))\n",
    "    return join_data_list\n",
    "\n",
    "def iterate_line_in_list(data_list: list, f: Callable) -> dict:\n",
    "    \"\"\"\n",
    "    Iterate the `data_list` while recording the class.\n",
    "\n",
    "    Input:\n",
    "    - data_list: A list containing (train/test) data, with the format of [{'title':<>, 'label':<>, 'text':<>}, {}, ...]\n",
    "    - type: The type of the data, default is \"train\". Can take the value of \"train\" or \"test\"\n",
    "    - f: A function to compute the number of documents, sentences e.t.c. in each `line`\n",
    "\n",
    "    Output:\n",
    "    - class_dict: A list containing dictionaries with (key, value) as (<class>, <number_of_documents>)\n",
    "    \"\"\"\n",
    "    class_dict = {}\n",
    "    for line in data_list:\n",
    "        line_class = line['label']\n",
    "        class_dict[line_class] = class_dict.get(line_class, 0) + f(line['text'])  # if the class doesn't exist, set the value as 0\n",
    "    return class_dict\n",
    "\n",
    "################################################################ \n",
    "###                        end define                        ###\n",
    "################################################################\n",
    "\n",
    "def count_docs(text):\n",
    "    return 1\n",
    "\n",
    "def print_docs_in_class(class_dict: dict, type: str = \"train\") -> None:\n",
    "    print(\"The number of documents in each class for \" + type + \" dataset is: \\n\")\n",
    "    for _class, _times in class_dict.items():\n",
    "        print(\"There are {:>3} documents in class {:>10}\".format(_times, _class))\n",
    "    print('-'*50)\n",
    "\n",
    "\n",
    "# Fetch data from the json file\n",
    "    \n",
    "train_file_path, test_file_path = \"enwiki-train.json\", \"enwiki-test.json\"\n",
    "train_data_list, test_data_list = map(load_json, [train_file_path, test_file_path])\n",
    "\n",
    "# print out the number of documents of each class in train and test dataset\n",
    "\n",
    "train_docs_num = iterate_line_in_list(train_data_list, count_docs)\n",
    "test_docs_num = iterate_line_in_list(test_data_list, count_docs)\n",
    "\n",
    "print_docs_in_class(train_docs_num), print_docs_in_class(test_docs_num, \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1a2d5c-b719-41ec-8f14-04e092517eb9",
   "metadata": {},
   "source": [
    "> 2) Print out the average number of sentences in each class.\n",
    ">    You may need to use sentence tokenization of NLTK.\n",
    ">    (for both train and test dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "37e85dc7-d50b-406a-9550-b2063f236ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average number of sentences in each class for train dataset is: \n",
      "\n",
      "There are average  438.56 sentences in class       Film\n",
      "There are average  400.36 sentences in class       Book\n",
      "There are average  706.20 sentences in class Politician\n",
      "There are average  420.32 sentences in class     Writer\n",
      "There are average  175.24 sentences in class       Food\n",
      "There are average   76.70 sentences in class      Actor\n",
      "There are average   70.38 sentences in class     Animal\n",
      "There are average  260.95 sentences in class   Software\n",
      "There are average  306.47 sentences in class     Artist\n",
      "There are average  404.90 sentences in class    Disease\n",
      "--------------------------------------------------\n",
      "The average number of sentences in each class for test dataset is: \n",
      "\n",
      "There are average  364.70 sentences in class       Film\n",
      "There are average  295.90 sentences in class       Book\n",
      "There are average  597.60 sentences in class Politician\n",
      "There are average  294.90 sentences in class     Writer\n",
      "There are average  107.60 sentences in class       Food\n",
      "There are average   30.70 sentences in class      Actor\n",
      "There are average   46.80 sentences in class     Animal\n",
      "There are average  160.10 sentences in class   Software\n",
      "There are average  234.00 sentences in class     Artist\n",
      "There are average  311.70 sentences in class    Disease\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def count_sents(text):\n",
    "    return len(sent_tokenize(text))\n",
    "\n",
    "def print_ave_sents_in_class(class_dict: dict, type: str = \"train\"):\n",
    "    # get the dict of number of documents in each class based on the input type\n",
    "    if type == \"train\":\n",
    "        docs_num_class = train_docs_num\n",
    "    elif type == \"test\":\n",
    "        docs_num_class = test_docs_num\n",
    "    else:\n",
    "        raise TypeError\n",
    "    \n",
    "    # print the result\n",
    "    print(\"The average number of sentences in each class for \" + type + \" dataset is: \\n\")\n",
    "    for _class, _times in class_dict.items():\n",
    "        print(\"There are average {:>7.2f} sentences in class {:>10}\".format(_times / docs_num_class[_class], _class))\n",
    "    print('-'*50)\n",
    "\n",
    "\n",
    "train_ave_sents = iterate_line_in_list(train_data_list, count_sents)\n",
    "test_ave_sents = iterate_line_in_list(test_data_list, count_sents)\n",
    "\n",
    "print_ave_sents_in_class(train_ave_sents), print_ave_sents_in_class(test_ave_sents, \"test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502ef409-65a5-4d07-9b69-c5986569970a",
   "metadata": {},
   "source": [
    "> 3) Print out the average number of tokens in each class\n",
    ">    (for both train and test dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f4d7628e-762c-49fe-804a-796fa0265af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average number of tokens in each class for train dataset is: \n",
      "\n",
      "There are average 11895.28 tokens in class       Film\n",
      "There are average 10540.51 tokens in class       Book\n",
      "There are average 18644.30 tokens in class Politician\n",
      "There are average 11849.91 tokens in class     Writer\n",
      "There are average  3904.15 tokens in class       Food\n",
      "There are average  1868.84 tokens in class      Actor\n",
      "There are average  1521.92 tokens in class     Animal\n",
      "There are average  6302.30 tokens in class   Software\n",
      "There are average  8212.91 tokens in class     Artist\n",
      "There are average  9322.96 tokens in class    Disease\n",
      "--------------------------------------------------\n",
      "The average number of tokens in each class for test dataset is: \n",
      "\n",
      "There are average  9292.90 tokens in class       Film\n",
      "There are average  7711.10 tokens in class       Book\n",
      "There are average 15204.30 tokens in class Politician\n",
      "There are average  8499.40 tokens in class     Writer\n",
      "There are average  2445.50 tokens in class       Food\n",
      "There are average   677.50 tokens in class      Actor\n",
      "There are average   885.60 tokens in class     Animal\n",
      "There are average  3972.80 tokens in class   Software\n",
      "There are average  5706.40 tokens in class     Artist\n",
      "There are average  6988.80 tokens in class    Disease\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def count_tokens(text):\n",
    "    return len(word_tokenize(text))\n",
    "\n",
    "def print_ave_tokens_in_class(class_dict: dict, type: str = \"train\"):\n",
    "    # get the dict of number of documents in each class based on the input type\n",
    "    if type == \"train\":\n",
    "        docs_num_class = train_docs_num\n",
    "    elif type == \"test\":\n",
    "        docs_num_class = test_docs_num\n",
    "    else:\n",
    "        raise TypeError\n",
    "    \n",
    "    # print the result\n",
    "    print(\"The average number of tokens in each class for \" + type + \" dataset is: \\n\")\n",
    "    for _class, _times in class_dict.items():\n",
    "        print(\"There are average {:>8.2f} tokens in class {:>10}\".format(_times / docs_num_class[_class], _class))\n",
    "    print('-'*50)\n",
    "\n",
    "train_ave_tokens = iterate_line_in_list(train_data_list, count_tokens)\n",
    "test_ave_tokens = iterate_line_in_list(test_data_list, count_tokens)\n",
    "\n",
    "print_ave_tokens_in_class(train_ave_tokens), print_ave_tokens_in_class(test_ave_tokens, \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d999a66-977c-469b-939c-c3934210972e",
   "metadata": {},
   "source": [
    "> 4) For each sentence in the document, remove punctuations and other special characters so that each sentence only contains English words and numbers. To make your life easier, you can make all words as lower cases. For each class, print out the first article's name and the processed first 40 words. (for both train and test dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "819a28ef-0821-4ace-be70-fb9b249d355b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result of the train data list:\n",
      "\n",
      "The first article's name of class       Film is         Citizen_Kane\n",
      "The cleaned text is: [Citizen Kane is a 1941 American drama fi] ==> [citizen kane is a 1941 american drama fi]\n",
      "\n",
      "The first article's name of class       Book is The_Spirit_of_the_Age\n",
      "The cleaned text is: [The Spirit of the Age (full title \"The S] ==> [the spirit of the age full title the spi]\n",
      "\n",
      "The first article's name of class Politician is    Charles_de_Gaulle\n",
      "The cleaned text is: [Charles André Joseph Marie de Gaulle (; ] ==> [charles andr joseph marie de gaulle 22 n]\n",
      "\n",
      "The first article's name of class     Writer is        Mircea_Eliade\n",
      "The cleaned text is: [Mircea Eliade (; – April 22, 1986) was a] ==> [mircea eliade april 22 1986 was a romani]\n",
      "\n",
      "The first article's name of class       Food is       Korean_cuisine\n",
      "The cleaned text is: [ \n",
      "Korean cuisine has evolved through cen] ==> [korean cuisine has evolved through centu]\n",
      "\n",
      "The first article's name of class      Actor is       Roman_Polanski\n",
      "The cleaned text is: [Roman Polanski ( ; ; born Raymond Thierr] ==> [roman polanski born raymond thierry lieb]\n",
      "\n",
      "The first article's name of class     Animal is      Oesophagostomum\n",
      "The cleaned text is: [Oesophagostomum is a genus of parasitic ] ==> [oesophagostomum is a genus of parasitic ]\n",
      "\n",
      "The first article's name of class   Software is Android_(operating_system)\n",
      "The cleaned text is: [Android is a mobile operating system bas] ==> [android is a mobile operating system bas]\n",
      "\n",
      "The first article's name of class     Artist is           Mihai_Olos\n",
      "The cleaned text is: [Mihai Olos (born 26 February 1940 in Ari] ==> [mihai olos born 26 february 1940 in arin]\n",
      "\n",
      "The first article's name of class    Disease is    Domestic_violence\n",
      "The cleaned text is: [Domestic violence (also called domestic ] ==> [domestic violence also called domestic a]\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "The result of the test data list:\n",
      "\n",
      "The first article's name of class       Film is Monty_Python's_Life_of_Brian\n",
      "The cleaned text is: [Monty Python's Life of Brian, also known] ==> [monty pythons life of brian also known a]\n",
      "\n",
      "The first article's name of class       Book is         Cousin_Bette\n",
      "The cleaned text is: [La Cousine Bette (, \"Cousin Bette\") is a] ==> [la cousine bette cousin bette is an 1846]\n",
      "\n",
      "The first article's name of class Politician is    Olusegun_Obasanjo\n",
      "The cleaned text is: [Chief Olusegun Matthew Okikiola Aremu Ob] ==> [chief olusegun matthew okikiola aremu ob]\n",
      "\n",
      "The first article's name of class     Writer is         Horia_Gârbea\n",
      "The cleaned text is: [Horia-Răzvan Gârbea or Gîrbea (; born Au] ==> [horiarzvan grbea or grbea born august 10]\n",
      "\n",
      "The first article's name of class       Food is          Sponge_cake\n",
      "The cleaned text is: [Sponge cake is a light cake made with eg] ==> [sponge cake is a light cake made with eg]\n",
      "\n",
      "The first article's name of class      Actor is       Kom_Chuanchuen\n",
      "The cleaned text is: [Akom Preedakul (, , ; 5 January 1958 – 3] ==> [akom preedakul 5 january 1958 30 april 2]\n",
      "\n",
      "The first article's name of class     Animal is Articulata_hypothesis\n",
      "The cleaned text is: [The Articulata hypothesis is the groupin] ==> [the articulata hypothesis is the groupin]\n",
      "\n",
      "The first article's name of class   Software is                 Unix\n",
      "The cleaned text is: [Unix (; trademarked as UNIX) is a family] ==> [unix trademarked as unix is a family of ]\n",
      "\n",
      "The first article's name of class     Artist is     Camille_Pissarro\n",
      "The cleaned text is: [Camille Pissarro ( , ; 10 July 1830 – 13] ==> [camille pissarro 10 july 1830 13 novembe]\n",
      "\n",
      "The first article's name of class    Disease is Staphylococcus_aureus\n",
      "The cleaned text is: [Staphylococcus aureus is a Gram-positive] ==> [staphylococcus aureus is a grampositive ]\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from copy import deepcopy\n",
    "\n",
    "def clean_doc(document: str) -> str:\n",
    "    document = document.lower()\n",
    "    # remove punctuations and special characters\n",
    "    cleaned_document = re.sub(r'[^a-zA-Z0-9\\s]', '', document)\n",
    "    # remove extra whitespaces\n",
    "    cleaned_document = re.sub(r'\\s+', ' ', cleaned_document).strip()\n",
    "    return cleaned_document\n",
    "\n",
    "def process_data_list(data_list: list, type: str = \"train\") -> list:\n",
    "    explored = []\n",
    "    print(\"The result of the \" + type + \" data list:\")\n",
    "    # process the data_list\n",
    "    for line in data_list:\n",
    "        class_label = line[\"label\"]\n",
    "        former_line_text = line[\"text\"]            # former text\n",
    "        line[\"text\"] = clean_doc(line[\"text\"])     # cleaned text\n",
    "        if class_label not in explored:\n",
    "            explored.append(class_label)\n",
    "            # print the result\n",
    "            print()\n",
    "            print(\"The first article's name of class {:>10} is {:>20}\".format(class_label, line[\"title\"]))\n",
    "            print(\"The cleaned text is: [{}] ==> [{}]\".format(former_line_text[:40], line[\"text\"][:40]))\n",
    "    print(\"-\"*120)\n",
    "    return data_list\n",
    "\n",
    "# make a deepcopy of the origin data list to avoid over-write\n",
    "cleaned_train_data_list = deepcopy(train_data_list)\n",
    "cleaned_test_data_list = deepcopy(test_data_list)\n",
    "\n",
    "# process the copyed data list in place by `process_data_list`\n",
    "cleaned_train_data_list = process_data_list(cleaned_train_data_list)\n",
    "cleaned_test_data_list = process_data_list(cleaned_test_data_list, \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6c7ea9-18fa-43f1-bded-625c09a947b1",
   "metadata": {},
   "source": [
    "## Task2 - Build language models\n",
    "\n",
    "> 1) Based on the training dataset, build unigram, bigram, and trigram language models using Add-one smoothing technique. It is encouraged to implement models by yourself. If you use public code, please cite it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e979b1-235f-4663-8acc-00257735e209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes to here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb6edc5-78c5-4de0-8855-2a455d625c97",
   "metadata": {},
   "source": [
    "> 2) Report the perplexity of these 3 trained models on the testing dataset and explain your findings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acda9a7-606c-4fa6-918a-9ebe7745ad42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes to here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd75b35-134d-4629-ac87-ed84fcc9d8e4",
   "metadata": {},
   "source": [
    "> 3) Use each built model to generate five sentences and explain these generated patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6e1244-2b15-4845-a83b-5708797569b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes to here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1b4282-d67c-4543-88cf-1bd0518173c2",
   "metadata": {},
   "source": [
    "## Task3 - Build NB/LR classifiers\n",
    "\n",
    "> 1) Build a Naive Bayes classifier (with Laplace smoothing) and test your model on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9cdf1d-8147-4033-bbb0-9147f3647c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes to here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca80a4a-9db5-426f-a45a-b1c2c1e34423",
   "metadata": {},
   "source": [
    "> 2) Build a LR classifier. This question seems to be challenging. We did not directly provide features for samples. But just use your own method to build useful features. You may need to split the training dataset into train and validation so that some involved parameters can be tuned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae83301a-0529-4dc9-899b-504324f9a547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes to here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53a8b62-1ef4-4c2f-934d-f78551ad039e",
   "metadata": {},
   "source": [
    "> 3) Report Micro-F1 score and Macro-F1 score for these classifiers on testing dataset explain our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500abc8d-0e08-4a84-af6c-9d0e1e0fa035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes to here\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
